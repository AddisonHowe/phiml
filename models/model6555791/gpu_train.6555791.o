Slurm job ID: 6555791
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='kl', momentum=0.9, name='model6555791', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='adam', outdir='out/model_training/model6555791', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 3770635078
EPOCH 1:
	batch 50 loss: 6.082777309417724
	batch 100 loss: 5.305348377227784
	batch 150 loss: 3.195932722091675
	batch 200 loss: 2.458058497905731
LOSS [train: 2.458058497905731] [valid: 3.094257883230845] TIME [epoch: 498 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 2.2163892340660096
	batch 100 loss: 2.0846431827545167
	batch 150 loss: 2.0060979318618775
	batch 200 loss: 1.8179243564605714
LOSS [train: 1.8179243564605714] [valid: 2.4997440569102762] TIME [epoch: 520 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 1.8567572593688966
	batch 100 loss: 1.6728813827037812
	batch 150 loss: 1.5436227560043334
	batch 200 loss: 1.4448902201652527
LOSS [train: 1.4448902201652527] [valid: 1.8093301521303753] TIME [epoch: 560 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 1.4132270777225495
	batch 100 loss: 1.2217887771129607
	batch 150 loss: 1.1101663279533387
	batch 200 loss: 0.9461442756652833
LOSS [train: 0.9461442756652833] [valid: 1.0606847593095154] TIME [epoch: 547 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.8043931782245636
	batch 100 loss: 0.7268179285526276
	batch 150 loss: 0.5052429834008216
	batch 200 loss: 0.3804835411906242
LOSS [train: 0.3804835411906242] [valid: 0.4587620532140136] TIME [epoch: 554 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.3253358510136604
	batch 100 loss: 0.3023398180305958
	batch 150 loss: 0.2466182056069374
	batch 200 loss: 0.1953251454234123
LOSS [train: 0.1953251454234123] [valid: 0.27032771409333994] TIME [epoch: 510 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.224469031766057
	batch 100 loss: 0.18250826977193355
	batch 150 loss: 0.21470468178391455
	batch 200 loss: 0.17786190770566462
LOSS [train: 0.17786190770566462] [valid: 0.19587809056198846] TIME [epoch: 528 sec]
Saving model.
EPOCH 8:
	batch 50 loss: 0.13939684335142374
	batch 100 loss: 0.19555009178817273
	batch 150 loss: 0.1721496830880642
	batch 200 loss: 0.18927397035062313
LOSS [train: 0.18927397035062313] [valid: 0.20192008607409662] TIME [epoch: 520 sec]
EPOCH 9:
	batch 50 loss: 0.20239603482186794
	batch 100 loss: 0.1831183338165283
	batch 150 loss: 0.17362744711339473
	batch 200 loss: 0.1825215795636177
LOSS [train: 0.1825215795636177] [valid: 0.19158634178165812] TIME [epoch: 533 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.17531023982912303
	batch 100 loss: 0.1682441831380129
	batch 150 loss: 0.17663416881114244
	batch 200 loss: 0.16783026623306796
LOSS [train: 0.16783026623306796] [valid: 0.23855467023483168] TIME [epoch: 532 sec]
EPOCH 11:
	batch 50 loss: 0.18403809174895286
	batch 100 loss: 0.16833889484405518
	batch 150 loss: 0.17547062084078788
	batch 200 loss: 0.18981805391609669
LOSS [train: 0.18981805391609669] [valid: 0.21782928755371056] TIME [epoch: 554 sec]
EPOCH 12:
	batch 50 loss: 0.16740968227386474
	batch 100 loss: 0.1685951130092144
	batch 150 loss: 0.1729760353267193
	batch 200 loss: 0.18493658415973185
LOSS [train: 0.18493658415973185] [valid: 0.22407484350260348] TIME [epoch: 542 sec]
EPOCH 13:
	batch 50 loss: 0.1424174892157316
	batch 100 loss: 0.16359955817461014
	batch 150 loss: 0.15706134401261806
	batch 200 loss: 0.18665147008374333
LOSS [train: 0.18665147008374333] [valid: 0.19293089014293704] TIME [epoch: 539 sec]
EPOCH 14:
	batch 50 loss: 0.16770290903747082
	batch 100 loss: 0.17183672688901425
	batch 150 loss: 0.1580689664185047
	batch 200 loss: 0.30657148897647857
LOSS [train: 0.30657148897647857] [valid: 0.21399024911903933] TIME [epoch: 531 sec]
EPOCH 15:
	batch 50 loss: 0.17480634182691573
	batch 100 loss: 0.18414410896599293
	batch 150 loss: 0.1641383757814765
	batch 200 loss: 0.15941068179905415
LOSS [train: 0.15941068179905415] [valid: 0.18997536233315865] TIME [epoch: 559 sec]
Saving model.
EPOCH 16:
	batch 50 loss: 0.1318589262664318
	batch 100 loss: 0.1709180749952793
	batch 150 loss: 0.16716136321425437
	batch 200 loss: 0.14269297109916806
LOSS [train: 0.14269297109916806] [valid: 0.15654684769785188] TIME [epoch: 564 sec]
Saving model.
EPOCH 17:
	batch 50 loss: 0.1311807375587523
	batch 100 loss: 0.14786569088697432
	batch 150 loss: 0.13548077512532472
	batch 200 loss: 0.1432763059437275
LOSS [train: 0.1432763059437275] [valid: 0.20997316984770198] TIME [epoch: 545 sec]
EPOCH 18:
	batch 50 loss: 0.1354865412041545
	batch 100 loss: 0.09093662180006504
	batch 150 loss: 0.11540333733893932
	batch 200 loss: 0.07500350187532603
LOSS [train: 0.07500350187532603] [valid: 0.07911394344992004] TIME [epoch: 532 sec]
Saving model.
EPOCH 19:
	batch 50 loss: 0.10654090418480337
	batch 100 loss: 0.09996055779978633
	batch 150 loss: 0.11812879463657737
	batch 200 loss: 0.11799802467226982
LOSS [train: 0.11799802467226982] [valid: 0.07999049480761945] TIME [epoch: 542 sec]
EPOCH 20:
	batch 50 loss: 0.09162578374613077
	batch 100 loss: 0.09889769466128201
	batch 150 loss: 0.08006930466741323
	batch 200 loss: 0.09318215415347368
LOSS [train: 0.09318215415347368] [valid: 0.06699815640846889] TIME [epoch: 510 sec]
Saving model.
EPOCH 21:
	batch 50 loss: 0.08944976608501748
	batch 100 loss: 0.10020303581375628
	batch 150 loss: 0.10014272548258304
	batch 200 loss: 0.09354046380147338
LOSS [train: 0.09354046380147338] [valid: 0.36336711442563685] TIME [epoch: 500 sec]
EPOCH 22:
	batch 50 loss: 0.1160499863140285
	batch 100 loss: 0.08663809211924672
	batch 150 loss: 0.09969655305147171
	batch 200 loss: 0.1166829177364707
LOSS [train: 0.1166829177364707] [valid: 0.1207647723989794] TIME [epoch: 508 sec]
EPOCH 23:
	batch 50 loss: 0.07710676966235042
	batch 100 loss: 0.10699465572834015
	batch 150 loss: 0.10212477655149996
	batch 200 loss: 0.09373466905206441
LOSS [train: 0.09373466905206441] [valid: 0.07660181970180323] TIME [epoch: 500 sec]
EPOCH 24:
	batch 50 loss: 0.06901160651352256
	batch 100 loss: 0.09745531475171447
	batch 150 loss: 0.10455347621813417
	batch 200 loss: 0.07935383930802345
LOSS [train: 0.07935383930802345] [valid: 0.08737065407913178] TIME [epoch: 506 sec]
EPOCH 25:
	batch 50 loss: 0.09260599920526147
	batch 100 loss: 0.08099151398986577
	batch 150 loss: 0.09409023098647594
	batch 200 loss: 0.09645116682164372
LOSS [train: 0.09645116682164372] [valid: 0.08521692837348382] TIME [epoch: 513 sec]
EPOCH 26:
	batch 50 loss: 0.08812954676337541
	batch 100 loss: 0.09916184291243553
	batch 150 loss: 0.10905469151679427
	batch 200 loss: 0.08996252570184879
LOSS [train: 0.08996252570184879] [valid: 0.06378966270567617] TIME [epoch: 483 sec]
Saving model.
EPOCH 27:
	batch 50 loss: 0.3486140273325145
	batch 100 loss: 0.12337644007056951
	batch 150 loss: 0.07217870019376278
	batch 200 loss: 0.07692646522191353
LOSS [train: 0.07692646522191353] [valid: 0.07970448111882433] TIME [epoch: 484 sec]
EPOCH 28:
	batch 50 loss: 0.0739092014823109
	batch 100 loss: 0.08611051021143794
	batch 150 loss: 0.13158178351819516
	batch 200 loss: 0.11715336047112941
LOSS [train: 0.11715336047112941] [valid: 0.057792238452627015] TIME [epoch: 492 sec]
Saving model.
EPOCH 29:
	batch 50 loss: 0.07426549768075347
	batch 100 loss: 0.09400027509778738
	batch 150 loss: 0.07650285098701716
	batch 200 loss: 0.0792833890975453
LOSS [train: 0.0792833890975453] [valid: 0.07309691915094542] TIME [epoch: 496 sec]
EPOCH 30:
	batch 50 loss: 0.061888038096949455
	batch 100 loss: 0.09348609512671828
	batch 150 loss: 0.08513963421806693
	batch 200 loss: 0.05641156270001375
LOSS [train: 0.05641156270001375] [valid: 0.059571165065669146] TIME [epoch: 503 sec]
EPOCH 31:
	batch 50 loss: 0.07276288648368791
	batch 100 loss: 0.07478975097532384
	batch 150 loss: 0.08511593074537814
	batch 200 loss: 0.08311854406259954
LOSS [train: 0.08311854406259954] [valid: 0.06312043802269424] TIME [epoch: 488 sec]
EPOCH 32:
	batch 50 loss: 0.07917569184675813
	batch 100 loss: 0.08800880355760456
	batch 150 loss: 0.09906406442634762
	batch 200 loss: 0.07609676180873066
LOSS [train: 0.07609676180873066] [valid: 0.07437648512956609] TIME [epoch: 490 sec]
EPOCH 33:
	batch 50 loss: 0.15141265315003694
	batch 100 loss: 0.0884813822992146
	batch 150 loss: 0.0722793580405414
	batch 200 loss: 0.16953114200383423
LOSS [train: 0.16953114200383423] [valid: 0.05818267414094104] TIME [epoch: 492 sec]
EPOCH 34:
	batch 50 loss: 0.07759397006593645
	batch 100 loss: 0.09089822791516781
	batch 150 loss: 0.11619637901429086
	batch 200 loss: 0.07991454745642841
LOSS [train: 0.07991454745642841] [valid: 0.051415993313033446] TIME [epoch: 516 sec]
Saving model.
EPOCH 35:
	batch 50 loss: 0.07599963324377314
	batch 100 loss: 0.0879626358859241
	batch 150 loss: 0.12306073075160384
	batch 200 loss: 0.09901309096254408
LOSS [train: 0.09901309096254408] [valid: 0.058420085654749224] TIME [epoch: 518 sec]
EPOCH 36:
	batch 50 loss: 0.07157047907589004
	batch 100 loss: 0.07816814315039665
	batch 150 loss: 0.07592969263438136
	batch 200 loss: 0.05938696305966005
LOSS [train: 0.05938696305966005] [valid: 0.05978374994495728] TIME [epoch: 504 sec]
EPOCH 37:
	batch 50 loss: 0.08280205637216569
	batch 100 loss: 0.08123011489864439
	batch 150 loss: 0.06136821023304947
	batch 200 loss: 0.08492856139317155
LOSS [train: 0.08492856139317155] [valid: 0.052745996209948014] TIME [epoch: 500 sec]
EPOCH 38:
	batch 50 loss: 0.07355692275450565
	batch 100 loss: 0.08053349511697888
	batch 150 loss: 0.07703368647955358
	batch 200 loss: 0.09751502535305917
LOSS [train: 0.09751502535305917] [valid: 0.0751245074692027] TIME [epoch: 516 sec]
EPOCH 39:
	batch 50 loss: 0.1286080065742135
	batch 100 loss: 0.05958186311705504
	batch 150 loss: 0.07865847595967353
	batch 200 loss: 0.06876271843386349
LOSS [train: 0.06876271843386349] [valid: 0.0701597333032017] TIME [epoch: 505 sec]
EPOCH 40:
	batch 50 loss: 0.07001919980393723
	batch 100 loss: 0.09768339417874813
	batch 150 loss: 0.07661887684836984
	batch 200 loss: 0.11348103872354841
LOSS [train: 0.11348103872354841] [valid: 0.064983781784152] TIME [epoch: 502 sec]
EPOCH 41:
	batch 50 loss: 0.055474198473384605
	batch 100 loss: 0.07763648819644003
	batch 150 loss: 0.07271677006268874
	batch 200 loss: 0.06800785765051842
LOSS [train: 0.06800785765051842] [valid: 0.05269252909056377] TIME [epoch: 519 sec]
EPOCH 42:
	batch 50 loss: 0.07629747738596052
	batch 100 loss: 0.06376563731115312
	batch 150 loss: 0.08081040751188993
	batch 200 loss: 0.06843837604857982
LOSS [train: 0.06843837604857982] [valid: 0.06803326080359208] TIME [epoch: 517 sec]
EPOCH 43:
	batch 50 loss: 0.06338139621540904
	batch 100 loss: 0.10047316025942564
	batch 150 loss: 0.10241713622584939
	batch 200 loss: 0.06762946217320859
LOSS [train: 0.06762946217320859] [valid: 0.07457335876921813] TIME [epoch: 520 sec]
EPOCH 44:
	batch 50 loss: 0.06778314444585704
	batch 100 loss: 0.06822711529610387
	batch 150 loss: 0.0768454858683981
	batch 200 loss: 0.0957805793080479
LOSS [train: 0.0957805793080479] [valid: 0.11456763462532156] TIME [epoch: 532 sec]
EPOCH 45:
	batch 50 loss: 0.08040742536075413
	batch 100 loss: 0.06949311864649645
	batch 150 loss: 0.0814992764312774
	batch 200 loss: 0.05486853209324181
LOSS [train: 0.05486853209324181] [valid: 0.08353396551441013] TIME [epoch: 544 sec]
EPOCH 46:
	batch 50 loss: 0.0679579957749229
	batch 100 loss: 0.07221125839510932
	batch 150 loss: 0.07214428189356113
	batch 200 loss: 0.0648138911742717
LOSS [train: 0.0648138911742717] [valid: 0.07145267436729531] TIME [epoch: 504 sec]
EPOCH 47:
	batch 50 loss: 0.07381171884946525
	batch 100 loss: 0.09177364330738783
	batch 150 loss: 0.05641452925745398
	batch 200 loss: 0.07036852752324194
LOSS [train: 0.07036852752324194] [valid: 0.08149933680979302] TIME [epoch: 474 sec]
EPOCH 48:
	batch 50 loss: 0.08699147792533041
	batch 100 loss: 0.060241761701181534
	batch 150 loss: 0.057771045674453486
	batch 200 loss: 0.0812074964120984
LOSS [train: 0.0812074964120984] [valid: 0.06182468951640961] TIME [epoch: 457 sec]
EPOCH 49:
	batch 50 loss: 0.06603985569439828
	batch 100 loss: 0.06492886607302353
	batch 150 loss: 0.23782607244327664
	batch 200 loss: 0.0760344038088806
LOSS [train: 0.0760344038088806] [valid: 0.06091261334562053] TIME [epoch: 407 sec]
EPOCH 50:
	batch 50 loss: 0.0831947401445359
	batch 100 loss: 0.08390139413066208
	batch 150 loss: 0.053792947670444846
	batch 200 loss: 0.08172726416261866
LOSS [train: 0.08172726416261866] [valid: 0.063001832657028] TIME [epoch: 428 sec]
Finished training in 25829.018 seconds.
