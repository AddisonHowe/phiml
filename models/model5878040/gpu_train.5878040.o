Slurm job ID: 5878040
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', infer_noise=False, learning_rate=0.001, momentum=0.9, name='model5878040', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=75, optimizer='rms', outdir='out/model_training/model5878040', seed=0, sigma=0.01, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 1560552621
EPOCH 1:
	batch 50 loss: 0.09455751011148095
	batch 100 loss: 0.0775909805612173
	batch 150 loss: 0.07959941927911132
	batch 200 loss: 0.046779881690163165
LOSS [train: 0.046779881690163165] [valid: 0.05746482481639153] TIME [epoch: 251 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.04783037949178834
	batch 100 loss: 0.08186305871116929
	batch 150 loss: 0.06834035040519666
	batch 200 loss: 0.06778296054049861
LOSS [train: 0.06778296054049861] [valid: 0.05875956818150977] TIME [epoch: 251 sec]
EPOCH 3:
	batch 50 loss: 0.0690948022250086
	batch 100 loss: 0.059376816197159316
	batch 150 loss: 0.07093061106279493
	batch 200 loss: 0.059962735383305696
LOSS [train: 0.059962735383305696] [valid: 0.05923174976778682] TIME [epoch: 251 sec]
EPOCH 4:
	batch 50 loss: 0.06352242772467434
	batch 100 loss: 0.07464115907612723
	batch 150 loss: 0.05583427604404278
	batch 200 loss: 0.0673487676406512
LOSS [train: 0.0673487676406512] [valid: 0.13548068391003956] TIME [epoch: 250 sec]
EPOCH 5:
	batch 50 loss: 0.0651110932813026
	batch 100 loss: 0.07690405839064623
	batch 150 loss: 0.05396035675192252
	batch 200 loss: 0.07016046411823482
LOSS [train: 0.07016046411823482] [valid: 0.07426720777099642] TIME [epoch: 251 sec]
EPOCH 6:
	batch 50 loss: 0.05650260768597946
	batch 100 loss: 0.05940113497199491
	batch 150 loss: 0.0593536028591916
	batch 200 loss: 0.07072319102007896
LOSS [train: 0.07072319102007896] [valid: 0.050343095828429794] TIME [epoch: 252 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.05772420681081712
	batch 100 loss: 0.05630225884844549
	batch 150 loss: 0.07110605506051798
	batch 200 loss: 0.0732525005727075
LOSS [train: 0.0732525005727075] [valid: 0.05217976866600414] TIME [epoch: 242 sec]
EPOCH 8:
	batch 50 loss: 0.06783046450465918
	batch 100 loss: 0.07778662406723015
	batch 150 loss: 0.0632572613703087
	batch 200 loss: 0.06284472431987524
LOSS [train: 0.06284472431987524] [valid: 0.057021993736270815] TIME [epoch: 241 sec]
EPOCH 9:
	batch 50 loss: 0.05446166503243148
	batch 100 loss: 0.07279964515008032
	batch 150 loss: 0.059083763306771286
	batch 200 loss: 0.07191089352359996
LOSS [train: 0.07191089352359996] [valid: 0.056259588582906875] TIME [epoch: 242 sec]
EPOCH 10:
	batch 50 loss: 0.05611348134945729
	batch 100 loss: 0.0658078239459428
	batch 150 loss: 0.06425977918093849
	batch 200 loss: 0.06922130921389907
LOSS [train: 0.06922130921389907] [valid: 0.0535080414925081] TIME [epoch: 241 sec]
EPOCH 11:
	batch 50 loss: 0.07200773665681481
	batch 100 loss: 0.06571718172170221
	batch 150 loss: 0.0655674429051578
	batch 200 loss: 0.07559746796978288
LOSS [train: 0.07559746796978288] [valid: 0.059675552645542966] TIME [epoch: 239 sec]
EPOCH 12:
	batch 50 loss: 0.072706793572288
	batch 100 loss: 0.07274707405362278
	batch 150 loss: 0.05124818606767803
	batch 200 loss: 0.06218339599203318
LOSS [train: 0.06218339599203318] [valid: 0.06613619390845997] TIME [epoch: 238 sec]
EPOCH 13:
	batch 50 loss: 0.06519043559543207
	batch 100 loss: 0.07315691505093128
	batch 150 loss: 0.06084898118278943
	batch 200 loss: 0.06538723576057237
LOSS [train: 0.06538723576057237] [valid: 0.08381872172079359] TIME [epoch: 239 sec]
EPOCH 14:
	batch 50 loss: 0.052972310453187677
	batch 100 loss: 0.07638534992816857
	batch 150 loss: 0.05585435526911169
	batch 200 loss: 0.0727478563832119
LOSS [train: 0.0727478563832119] [valid: 0.06038208483175064] TIME [epoch: 238 sec]
EPOCH 15:
	batch 50 loss: 0.06237587108975276
	batch 100 loss: 0.08182733449852093
	batch 150 loss: 0.06965513952076435
	batch 200 loss: 0.07402852238272317
LOSS [train: 0.07402852238272317] [valid: 0.08116096324908237] TIME [epoch: 238 sec]
EPOCH 16:
	batch 50 loss: 0.06376318492228165
	batch 100 loss: 0.06716094916919246
	batch 150 loss: 0.07477357162162662
	batch 200 loss: 0.06079266229644418
LOSS [train: 0.06079266229644418] [valid: 0.054138891332938026] TIME [epoch: 238 sec]
EPOCH 17:
	batch 50 loss: 0.06548104360961587
	batch 100 loss: 0.06778416409513738
	batch 150 loss: 0.06254707004409284
	batch 200 loss: 0.06793221112340689
LOSS [train: 0.06793221112340689] [valid: 0.06615293357948152] TIME [epoch: 238 sec]
EPOCH 18:
	batch 50 loss: 0.06479892260686029
	batch 100 loss: 0.07834166448563337
	batch 150 loss: 0.06912046704092063
	batch 200 loss: 0.0714964525110554
LOSS [train: 0.0714964525110554] [valid: 0.06018287583895775] TIME [epoch: 238 sec]
EPOCH 19:
	batch 50 loss: 0.08122761128935962
	batch 100 loss: 0.03359783657244406
	batch 150 loss: 0.06403887043008581
	batch 200 loss: 0.07508899793261663
LOSS [train: 0.07508899793261663] [valid: 0.09687262029619888] TIME [epoch: 238 sec]
EPOCH 20:
	batch 50 loss: 0.06374207913642749
	batch 100 loss: 0.0708160779485479
	batch 150 loss: 0.06681242977036163
	batch 200 loss: 0.056298405479174106
LOSS [train: 0.056298405479174106] [valid: 0.07474207812435149] TIME [epoch: 239 sec]
EPOCH 21:
	batch 50 loss: 0.06351333764265292
	batch 100 loss: 0.06884757521562278
	batch 150 loss: 0.05048316747997887
	batch 200 loss: 0.06978155830758624
LOSS [train: 0.06978155830758624] [valid: 0.06383268827812572] TIME [epoch: 238 sec]
EPOCH 22:
	batch 50 loss: 0.05430301172775216
	batch 100 loss: 0.41598577917553486
	batch 150 loss: 0.10761125526158138
	batch 200 loss: 0.05817575847948319
LOSS [train: 0.05817575847948319] [valid: 0.05465373529974992] TIME [epoch: 239 sec]
EPOCH 23:
	batch 50 loss: 0.05671329954173416
	batch 100 loss: 0.05098506029229611
	batch 150 loss: 0.06628883884288371
	batch 200 loss: 0.06777822761214339
LOSS [train: 0.06777822761214339] [valid: 0.056320848935865794] TIME [epoch: 239 sec]
EPOCH 24:
	batch 50 loss: 0.06492034152848646
	batch 100 loss: 0.10267544495407492
	batch 150 loss: 0.039485418752301486
	batch 200 loss: 0.0747229181707371
LOSS [train: 0.0747229181707371] [valid: 0.08682731034156556] TIME [epoch: 239 sec]
EPOCH 25:
	batch 50 loss: 0.052925829293672
	batch 100 loss: 0.0619775325700175
	batch 150 loss: 0.0674090660829097
	batch 200 loss: 0.06297717300476506
LOSS [train: 0.06297717300476506] [valid: 0.059014772123191506] TIME [epoch: 239 sec]
EPOCH 26:
	batch 50 loss: 0.06288022956345231
	batch 100 loss: 0.06905193664788385
	batch 150 loss: 0.07030789482523687
	batch 200 loss: 0.04803752253530547
LOSS [train: 0.04803752253530547] [valid: 0.07111332133645192] TIME [epoch: 238 sec]
EPOCH 27:
	batch 50 loss: 0.07333893127040937
	batch 100 loss: 0.07204453375656157
	batch 150 loss: 0.05127261871588416
	batch 200 loss: 0.060911093801259995
LOSS [train: 0.060911093801259995] [valid: 0.0712861177395098] TIME [epoch: 239 sec]
EPOCH 28:
	batch 50 loss: 0.07487070736475289
	batch 100 loss: 0.0659014780985308
	batch 150 loss: 0.05112862529509585
	batch 200 loss: 0.07848955304158153
LOSS [train: 0.07848955304158153] [valid: 0.0552112911012955] TIME [epoch: 239 sec]
EPOCH 29:
	batch 50 loss: 0.06891726427245885
	batch 100 loss: 0.054458110315317755
	batch 150 loss: 0.0695962070004316
	batch 200 loss: 0.07976465581450612
LOSS [train: 0.07976465581450612] [valid: 0.05463780585972321] TIME [epoch: 238 sec]
EPOCH 30:
	batch 50 loss: 0.06327278125099838
	batch 100 loss: 0.06911433158442377
	batch 150 loss: 0.06280683693883475
	batch 200 loss: 0.07207451909431256
LOSS [train: 0.07207451909431256] [valid: 0.1352034967898362] TIME [epoch: 238 sec]
EPOCH 31:
	batch 50 loss: 0.05923754967749119
	batch 100 loss: 0.06571010697400198
	batch 150 loss: 0.06966204193071462
	batch 200 loss: 0.0754892885775189
LOSS [train: 0.0754892885775189] [valid: 0.06567021469430377] TIME [epoch: 238 sec]
EPOCH 32:
	batch 50 loss: 0.06663491255836561
	batch 100 loss: 0.0546681714092847
	batch 150 loss: 0.06962799196131528
	batch 200 loss: 0.057733504094212545
LOSS [train: 0.057733504094212545] [valid: 0.06010538884826625] TIME [epoch: 238 sec]
EPOCH 33:
	batch 50 loss: 0.06385355049045756
	batch 100 loss: 0.0851716974179726
	batch 150 loss: 0.06710017582925502
	batch 200 loss: 0.052286373396636915
LOSS [train: 0.052286373396636915] [valid: 0.049007527499149245] TIME [epoch: 239 sec]
Saving model.
EPOCH 34:
	batch 50 loss: 0.05067386394017376
	batch 100 loss: 0.08811123511055484
	batch 150 loss: 0.0746155656175688
	batch 200 loss: 0.06118600549641997
LOSS [train: 0.06118600549641997] [valid: 0.06656966747214028] TIME [epoch: 239 sec]
EPOCH 35:
	batch 50 loss: 0.06039167816285044
	batch 100 loss: 0.0742655646847561
	batch 150 loss: 0.05107445023721084
	batch 200 loss: 0.06517608331050724
LOSS [train: 0.06517608331050724] [valid: 0.06398520460740352] TIME [epoch: 238 sec]
EPOCH 36:
	batch 50 loss: 0.06218963137944229
	batch 100 loss: 0.06410705275135115
	batch 150 loss: 0.051581137443426996
	batch 200 loss: 0.07974176277406514
LOSS [train: 0.07974176277406514] [valid: 0.055242380653119955] TIME [epoch: 239 sec]
EPOCH 37:
	batch 50 loss: 0.05010479404838406
	batch 100 loss: 0.06252337396144866
	batch 150 loss: 0.06137796147260815
	batch 200 loss: 0.06297224337235093
LOSS [train: 0.06297224337235093] [valid: 0.06433052636760597] TIME [epoch: 239 sec]
EPOCH 38:
	batch 50 loss: 0.05565835709800013
	batch 100 loss: 0.0808312373473018
	batch 150 loss: 0.06456147533957847
	batch 200 loss: 0.054105983171612027
LOSS [train: 0.054105983171612027] [valid: 0.05019085974878787] TIME [epoch: 239 sec]
EPOCH 39:
	batch 50 loss: 0.061778112493921074
	batch 100 loss: 0.056892229369841514
	batch 150 loss: 0.05860852607234847
	batch 200 loss: 0.06876598522998392
LOSS [train: 0.06876598522998392] [valid: 0.07831438862873862] TIME [epoch: 238 sec]
EPOCH 40:
	batch 50 loss: 0.0674297801638022
	batch 100 loss: 0.05302602356066927
	batch 150 loss: 0.05413489918340929
	batch 200 loss: 0.07323726933740544
LOSS [train: 0.07323726933740544] [valid: 0.05272554513940122] TIME [epoch: 239 sec]
EPOCH 41:
	batch 50 loss: 0.06382482103537768
	batch 100 loss: 0.0705431402893737
	batch 150 loss: 0.07303375497693196
	batch 200 loss: 0.0391280150890816
LOSS [train: 0.0391280150890816] [valid: 0.046940401109168306] TIME [epoch: 238 sec]
Saving model.
EPOCH 42:
	batch 50 loss: 0.05210993643384427
	batch 100 loss: 0.07485891497075499
	batch 150 loss: 0.047707357843755745
	batch 200 loss: 0.06265983557561412
LOSS [train: 0.06265983557561412] [valid: 0.06075312188671281] TIME [epoch: 239 sec]
EPOCH 43:
	batch 50 loss: 0.04913478272384964
	batch 100 loss: 0.061011451319791375
	batch 150 loss: 0.05288248603435932
	batch 200 loss: 0.07497658642940223
LOSS [train: 0.07497658642940223] [valid: 0.04728545708326237] TIME [epoch: 239 sec]
EPOCH 44:
	batch 50 loss: 0.06300453893200028
	batch 100 loss: 0.060085866656154396
	batch 150 loss: 0.07299418854760006
	batch 200 loss: 0.05625407205778174
LOSS [train: 0.05625407205778174] [valid: 0.049353300078413063] TIME [epoch: 239 sec]
EPOCH 45:
	batch 50 loss: 0.05873763511088328
	batch 100 loss: 0.05737355998717249
	batch 150 loss: 0.058233706497121604
	batch 200 loss: 0.07474892059806734
LOSS [train: 0.07474892059806734] [valid: 0.04921455344786712] TIME [epoch: 239 sec]
EPOCH 46:
	batch 50 loss: 0.07570045702159404
	batch 100 loss: 0.05697790937032551
	batch 150 loss: 0.06121284935143194
	batch 200 loss: 0.0616194405220449
LOSS [train: 0.0616194405220449] [valid: 0.048818869978034245] TIME [epoch: 239 sec]
EPOCH 47:
	batch 50 loss: 0.061678900560364125
	batch 100 loss: 0.054942754944786426
	batch 150 loss: 0.0550397649104707
	batch 200 loss: 0.07246744990348816
LOSS [train: 0.07246744990348816] [valid: 0.07082492659731845] TIME [epoch: 239 sec]
EPOCH 48:
	batch 50 loss: 0.05001756386307534
	batch 100 loss: 0.07511420372238717
	batch 150 loss: 0.06423358317581006
	batch 200 loss: 0.04902496329974383
LOSS [train: 0.04902496329974383] [valid: 0.05373538940912113] TIME [epoch: 239 sec]
EPOCH 49:
	batch 50 loss: 0.08648461760138161
	batch 100 loss: 0.043114803648204546
	batch 150 loss: 0.06491521915420889
	batch 200 loss: 0.05529678238322958
LOSS [train: 0.05529678238322958] [valid: 0.07973882924537369] TIME [epoch: 238 sec]
EPOCH 50:
	batch 50 loss: 0.06314301631646231
	batch 100 loss: 0.05094132338184863
	batch 150 loss: 0.0568847036693478
	batch 200 loss: 0.06083736149594188
LOSS [train: 0.06083736149594188] [valid: 0.05803908124313845] TIME [epoch: 239 sec]
EPOCH 51:
	batch 50 loss: 0.050309026108589026
	batch 100 loss: 0.07178150111110881
	batch 150 loss: 0.04444833642570302
	batch 200 loss: 0.06400288751843618
LOSS [train: 0.06400288751843618] [valid: 0.050514196880006544] TIME [epoch: 238 sec]
EPOCH 52:
	batch 50 loss: 0.04298149522626773
	batch 100 loss: 0.05141103903297335
	batch 150 loss: 0.07217306686798111
	batch 200 loss: 0.08569283742457628
LOSS [train: 0.08569283742457628] [valid: 0.06992030636562656] TIME [epoch: 240 sec]
EPOCH 53:
	batch 50 loss: 0.061884654501918705
	batch 100 loss: 0.06702404395211488
	batch 150 loss: 0.06742280584061518
	batch 200 loss: 0.0482184209022671
LOSS [train: 0.0482184209022671] [valid: 0.055577468318127404] TIME [epoch: 239 sec]
EPOCH 54:
	batch 50 loss: 0.07620745176565834
	batch 100 loss: 0.07351472731446847
	batch 150 loss: 0.048399417776963674
	batch 200 loss: 0.04292540762704448
LOSS [train: 0.04292540762704448] [valid: 0.08651333552164336] TIME [epoch: 239 sec]
EPOCH 55:
	batch 50 loss: 0.07216734079876914
	batch 100 loss: 0.060498438353097296
	batch 150 loss: 0.05018889086088166
	batch 200 loss: 0.054156575146480465
LOSS [train: 0.054156575146480465] [valid: 0.049941104689302546] TIME [epoch: 239 sec]
EPOCH 56:
	batch 50 loss: 0.05965184793807566
	batch 100 loss: 0.0348953057511244
	batch 150 loss: 0.070306711550802
	batch 200 loss: 0.06164115168619901
LOSS [train: 0.06164115168619901] [valid: 0.047762659206152115] TIME [epoch: 239 sec]
EPOCH 57:
	batch 50 loss: 0.059183739526197314
	batch 100 loss: 0.06610719850403257
	batch 150 loss: 0.05055478457827121
	batch 200 loss: 0.05291775268386118
LOSS [train: 0.05291775268386118] [valid: 0.04064166867659272] TIME [epoch: 239 sec]
Saving model.
EPOCH 58:
	batch 50 loss: 0.060313922993373126
	batch 100 loss: 0.06021594669815386
	batch 150 loss: 0.05795555307995528
	batch 200 loss: 0.049603391089476645
LOSS [train: 0.049603391089476645] [valid: 0.05238256793600158] TIME [epoch: 239 sec]
EPOCH 59:
	batch 50 loss: 0.05499889462022111
	batch 100 loss: 0.05461701867170632
	batch 150 loss: 0.06970738392788917
	batch 200 loss: 0.047449427848914635
LOSS [train: 0.047449427848914635] [valid: 0.050545232440830055] TIME [epoch: 238 sec]
EPOCH 60:
	batch 50 loss: 0.05212129474617541
	batch 100 loss: 0.06433764912420883
	batch 150 loss: 0.04669149619265227
	batch 200 loss: 0.08680312016047537
LOSS [train: 0.08680312016047537] [valid: 0.06287421623904568] TIME [epoch: 239 sec]
EPOCH 61:
	batch 50 loss: 0.08018621391616762
	batch 100 loss: 0.04218425652012229
	batch 150 loss: 0.06575085445307195
	batch 200 loss: 0.058659901726059616
LOSS [train: 0.058659901726059616] [valid: 0.04937680541794786] TIME [epoch: 239 sec]
EPOCH 62:
	batch 50 loss: 0.06120103754801676
	batch 100 loss: 0.04348175136139616
	batch 150 loss: 0.05424343845341355
	batch 200 loss: 0.06491439299585182
LOSS [train: 0.06491439299585182] [valid: 0.057875847996911034] TIME [epoch: 238 sec]
EPOCH 63:
	batch 50 loss: 0.055709111775213385
	batch 100 loss: 0.06393776517561492
	batch 150 loss: 0.05719865408958867
	batch 200 loss: 0.06344768478535116
LOSS [train: 0.06344768478535116] [valid: 0.050005931693400875] TIME [epoch: 239 sec]
EPOCH 64:
	batch 50 loss: 0.06721773328725249
	batch 100 loss: 0.049467327237362045
	batch 150 loss: 0.053221165760187435
	batch 200 loss: 0.0603017718414776
LOSS [train: 0.0603017718414776] [valid: 0.06131036001024768] TIME [epoch: 238 sec]
EPOCH 65:
	batch 50 loss: 0.06374999665422365
	batch 100 loss: 0.048950124667026106
	batch 150 loss: 0.06362037907354534
	batch 200 loss: 0.050458803488872946
LOSS [train: 0.050458803488872946] [valid: 0.05780688159817752] TIME [epoch: 239 sec]
EPOCH 66:
	batch 50 loss: 0.06348900084383785
	batch 100 loss: 0.060403155160602184
	batch 150 loss: 0.05506132985698059
	batch 200 loss: 0.049783871807157994
LOSS [train: 0.049783871807157994] [valid: 0.05068778514881463] TIME [epoch: 239 sec]
EPOCH 67:
	batch 50 loss: 0.06435451031429693
	batch 100 loss: 0.0470659352466464
	batch 150 loss: 0.04647924799472094
	batch 200 loss: 0.05672238453582395
LOSS [train: 0.05672238453582395] [valid: 0.05555686912266537] TIME [epoch: 239 sec]
EPOCH 68:
	batch 50 loss: 0.05127100644167513
	batch 100 loss: 0.06501172004267573
	batch 150 loss: 0.06895856512011961
	batch 200 loss: 0.05028861883969512
LOSS [train: 0.05028861883969512] [valid: 0.16617131998840098] TIME [epoch: 238 sec]
EPOCH 69:
	batch 50 loss: 0.05964734782697633
	batch 100 loss: 0.04587879891972989
	batch 150 loss: 0.050056544370199844
	batch 200 loss: 0.07615409790538251
LOSS [train: 0.07615409790538251] [valid: 0.05484980881640998] TIME [epoch: 239 sec]
EPOCH 70:
	batch 50 loss: 0.051374388304539026
	batch 100 loss: 0.06301746975281276
	batch 150 loss: 0.05791021555036423
	batch 200 loss: 0.0584833555672958
LOSS [train: 0.0584833555672958] [valid: 0.04461228007955166] TIME [epoch: 238 sec]
EPOCH 71:
	batch 50 loss: 0.04194527702245978
	batch 100 loss: 0.0442382516246289
	batch 150 loss: 0.06340153375727824
	batch 200 loss: 0.06349515681155025
LOSS [train: 0.06349515681155025] [valid: 0.04640925440095695] TIME [epoch: 238 sec]
EPOCH 72:
	batch 50 loss: 0.04176264849491417
	batch 100 loss: 0.05128461522981524
	batch 150 loss: 0.06332100666710176
	batch 200 loss: 0.05216102277510799
LOSS [train: 0.05216102277510799] [valid: 0.042464835984234624] TIME [epoch: 238 sec]
EPOCH 73:
	batch 50 loss: 0.06648478882852941
	batch 100 loss: 0.06505139994900673
	batch 150 loss: 0.04280984210112365
	batch 200 loss: 0.05963521168940133
LOSS [train: 0.05963521168940133] [valid: 0.15269675343685474] TIME [epoch: 238 sec]
EPOCH 74:
	batch 50 loss: 0.04197313363314606
	batch 100 loss: 0.06931877392169554
	batch 150 loss: 0.05880866413470358
	batch 200 loss: 0.04973516099504195
LOSS [train: 0.04973516099504195] [valid: 0.05064678046231468] TIME [epoch: 239 sec]
EPOCH 75:
	batch 50 loss: 0.06890726072248071
	batch 100 loss: 0.05010824027813214
	batch 150 loss: 0.04136319442652166
	batch 200 loss: 0.05163561060326174
LOSS [train: 0.05163561060326174] [valid: 0.05002628604900868] TIME [epoch: 238 sec]
Finished training in 17979.103 seconds.
