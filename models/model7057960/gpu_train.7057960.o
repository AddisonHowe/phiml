Slurm job ID: 7057960
args: Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['softplus'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='kl', momentum=0.9, name='model7057960', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='adam', outdir='out/model_training/model7057960', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2', weight_decay=0.0001)
Using device: cuda
Using seed: 4112088747
EPOCH 1:
	batch 50 loss: 4.349786987304688
	batch 100 loss: 3.9915701389312743
	batch 150 loss: 3.572597723007202
	batch 200 loss: 3.2675081300735473
LOSS [train: 3.2675081300735473] [valid: 3.5666452953591943] TIME [epoch: 308 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 2.82335307598114
	batch 100 loss: 2.3055846118927
	batch 150 loss: 1.6522876012325287
	batch 200 loss: 1.106781336069107
LOSS [train: 1.106781336069107] [valid: 0.983646195443968] TIME [epoch: 310 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.5269389793276786
	batch 100 loss: 0.17494888477027415
	batch 150 loss: 0.1187369361706078
	batch 200 loss: 0.06137473555310862
LOSS [train: 0.06137473555310862] [valid: 0.06364051172276959] TIME [epoch: 310 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.061622694067191335
	batch 100 loss: 0.062039180139545354
	batch 150 loss: 0.07548718720907345
	batch 200 loss: 0.0381104491854785
LOSS [train: 0.0381104491854785] [valid: 0.05652631477714749] TIME [epoch: 310 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.0662638686504215
	batch 100 loss: 0.05045860302139772
	batch 150 loss: 0.06175097915227525
	batch 200 loss: 0.054917069074581375
LOSS [train: 0.054917069074581375] [valid: 0.05276708821426534] TIME [epoch: 312 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.05187136020511389
	batch 100 loss: 0.039907629473600534
	batch 150 loss: 0.06474528918624856
	batch 200 loss: 0.0660592820495367
LOSS [train: 0.0660592820495367] [valid: 0.051406697419588455] TIME [epoch: 311 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.07041292569076177
	batch 100 loss: 0.051060654665343465
	batch 150 loss: 0.04428925712534692
	batch 200 loss: 0.056644288434181365
LOSS [train: 0.056644288434181365] [valid: 0.05262847336319586] TIME [epoch: 312 sec]
EPOCH 8:
	batch 50 loss: 0.06336670177057385
	batch 100 loss: 0.05741813166459906
	batch 150 loss: 0.04478955688886344
	batch 200 loss: 0.05687395364046097
LOSS [train: 0.05687395364046097] [valid: 0.054738089326322854] TIME [epoch: 311 sec]
EPOCH 9:
	batch 50 loss: 0.05847628677263856
	batch 100 loss: 0.05581344348298444
	batch 150 loss: 0.06178421166725457
	batch 200 loss: 0.044735809760168194
LOSS [train: 0.044735809760168194] [valid: 0.05351438980578678] TIME [epoch: 311 sec]
EPOCH 10:
	batch 50 loss: 0.05281892370432615
	batch 100 loss: 0.054080894625221844
	batch 150 loss: 0.05298885188531131
	batch 200 loss: 0.05463045525830239
LOSS [train: 0.05463045525830239] [valid: 0.05408493176122041] TIME [epoch: 311 sec]
EPOCH 11:
	batch 50 loss: 0.06586982433102094
	batch 100 loss: 0.0949791206791997
	batch 150 loss: 0.05518177371763158
	batch 200 loss: 0.05205924247624352
LOSS [train: 0.05205924247624352] [valid: 0.05005833227187395] TIME [epoch: 312 sec]
Saving model.
EPOCH 12:
	batch 50 loss: 0.04629983729217201
	batch 100 loss: 0.07393123793357517
	batch 150 loss: 0.03647407454642235
	batch 200 loss: 0.056962625258602205
LOSS [train: 0.056962625258602205] [valid: 0.04831352323332491] TIME [epoch: 311 sec]
Saving model.
EPOCH 13:
	batch 50 loss: 0.05626212332514115
	batch 100 loss: 0.0638478445285
	batch 150 loss: 0.0512148158880882
	batch 200 loss: 0.04685754962731153
LOSS [train: 0.04685754962731153] [valid: 0.04997293590568006] TIME [epoch: 311 sec]
EPOCH 14:
	batch 50 loss: 0.04745042367372662
	batch 100 loss: 0.04973616195085924
	batch 150 loss: 0.06202819020021707
	batch 200 loss: 0.049850064909551295
LOSS [train: 0.049850064909551295] [valid: 0.05098160329483411] TIME [epoch: 312 sec]
EPOCH 15:
	batch 50 loss: 0.051252600657753646
	batch 100 loss: 0.04297554331365973
	batch 150 loss: 0.053137729181908074
	batch 200 loss: 0.05940935279475525
LOSS [train: 0.05940935279475525] [valid: 0.04972586955700535] TIME [epoch: 299 sec]
EPOCH 16:
	batch 50 loss: 0.05422294995514676
	batch 100 loss: 0.058841190834064035
	batch 150 loss: 0.05250720513286069
	batch 200 loss: 0.04801573271193774
LOSS [train: 0.04801573271193774] [valid: 0.05098355734129048] TIME [epoch: 297 sec]
EPOCH 17:
	batch 50 loss: 0.045801013594027606
	batch 100 loss: 0.06406831133877859
	batch 150 loss: 0.03759431741869776
	batch 200 loss: 0.061911049713380635
LOSS [train: 0.061911049713380635] [valid: 0.05073004941029164] TIME [epoch: 296 sec]
EPOCH 18:
	batch 50 loss: 0.024883090815274046
	batch 100 loss: 0.06655981136485935
	batch 150 loss: 0.05493013636019896
	batch 200 loss: 0.061299648125132083
LOSS [train: 0.061299648125132083] [valid: 0.04862821445082469] TIME [epoch: 296 sec]
EPOCH 19:
	batch 50 loss: 0.05639983449713327
	batch 100 loss: 0.04711062493734062
	batch 150 loss: 0.03663466186961159
	batch 200 loss: 0.06884553071286063
LOSS [train: 0.06884553071286063] [valid: 0.04748994731732334] TIME [epoch: 295 sec]
Saving model.
EPOCH 20:
	batch 50 loss: 0.051143665618728844
	batch 100 loss: 0.0421856117893185
	batch 150 loss: 0.06948955030267825
	batch 200 loss: 0.04309181864839047
LOSS [train: 0.04309181864839047] [valid: 0.04800610742725742] TIME [epoch: 295 sec]
EPOCH 21:
	batch 50 loss: 0.05966351207927801
	batch 100 loss: 0.034619171599042604
	batch 150 loss: 0.05533935133833438
	batch 200 loss: 0.052825613929890096
LOSS [train: 0.052825613929890096] [valid: 0.048820332137014096] TIME [epoch: 296 sec]
EPOCH 22:
	batch 50 loss: 0.03440955387792201
	batch 100 loss: 0.06612853564962279
	batch 150 loss: 0.06308076019398869
	batch 200 loss: 0.05176930827088654
LOSS [train: 0.05176930827088654] [valid: 0.04929216902370778] TIME [epoch: 296 sec]
EPOCH 23:
	batch 50 loss: 0.0673191573820077
	batch 100 loss: 0.042766802915139124
	batch 150 loss: 0.03964338205132663
	batch 200 loss: 0.06014280692208558
LOSS [train: 0.06014280692208558] [valid: 0.04834262288495665] TIME [epoch: 295 sec]
EPOCH 24:
	batch 50 loss: 0.04235591837941684
	batch 100 loss: 0.058266044860938566
	batch 150 loss: 0.05298223927500658
	batch 200 loss: 0.04500914138508961
LOSS [train: 0.04500914138508961] [valid: 0.04832374368949483] TIME [epoch: 295 sec]
EPOCH 25:
	batch 50 loss: 0.04180190265295096
	batch 100 loss: 0.045839802395203154
	batch 150 loss: 0.055001552257454026
	batch 200 loss: 0.05185679034795612
LOSS [train: 0.05185679034795612] [valid: 0.04849747932215299] TIME [epoch: 295 sec]
EPOCH 26:
	batch 50 loss: 0.053872732422314586
	batch 100 loss: 0.06205001383787021
	batch 150 loss: 0.04699221344431862
	batch 200 loss: 0.04411082595121116
