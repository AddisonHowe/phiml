Slurm job ID: 5690831
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', learning_rate=0.001, momentum=0.9, name='model5690831', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model5690831', seed=0, sigma=0.01, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 3366949746
EPOCH 1:
	batch 50 loss: 0.02269433356821537
	batch 100 loss: 0.022457442190498115
	batch 150 loss: 0.022400621296837925
	batch 200 loss: 0.020218858579173684
LOSS [train: 0.020218858579173684] [valid: 0.016380563581818327] TIME [epoch: 570 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.01975322650745511
	batch 100 loss: 0.019335681945085524
	batch 150 loss: 0.018963454738259316
	batch 200 loss: 0.019577356474474072
LOSS [train: 0.019577356474474072] [valid: 0.015290096470077211] TIME [epoch: 575 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.020211559031158687
	batch 100 loss: 0.01731649596244097
	batch 150 loss: 0.01938449179753661
	batch 200 loss: 0.018959891190752386
LOSS [train: 0.018959891190752386] [valid: 0.015867513341800078] TIME [epoch: 573 sec]
EPOCH 4:
	batch 50 loss: 0.021356099843978883
	batch 100 loss: 0.02053333505988121
	batch 150 loss: 0.02102228915318847
	batch 200 loss: 0.01791548595763743
LOSS [train: 0.01791548595763743] [valid: 0.016034823399968444] TIME [epoch: 547 sec]
EPOCH 5:
	batch 50 loss: 0.021122899558395147
	batch 100 loss: 0.01905575948767364
	batch 150 loss: 0.02078949898481369
	batch 200 loss: 0.01984190762042999
LOSS [train: 0.01984190762042999] [valid: 0.01855786348605761] TIME [epoch: 547 sec]
EPOCH 6:
	batch 50 loss: 0.02086532324552536
	batch 100 loss: 0.01847404115833342
	batch 150 loss: 0.02092898041009903
	batch 200 loss: 0.01835020124912262
LOSS [train: 0.01835020124912262] [valid: 0.016595872826292178] TIME [epoch: 542 sec]
EPOCH 7:
	batch 50 loss: 0.020969355832785367
	batch 100 loss: 0.022855900255963207
	batch 150 loss: 0.018596380287781357
	batch 200 loss: 0.018139092829078437
LOSS [train: 0.018139092829078437] [valid: 0.016212788465782068] TIME [epoch: 540 sec]
EPOCH 8:
	batch 50 loss: 0.017484379233792424
	batch 100 loss: 0.01736429089680314
	batch 150 loss: 0.017261968944221735
	batch 200 loss: 0.022434957195073366
LOSS [train: 0.022434957195073366] [valid: 0.015737891888784362] TIME [epoch: 541 sec]
EPOCH 9:
	batch 50 loss: 0.017817964069545267
	batch 100 loss: 0.016049835113808513
	batch 150 loss: 0.01860335220582783
	batch 200 loss: 0.020024171089753507
LOSS [train: 0.020024171089753507] [valid: 0.015266437849398547] TIME [epoch: 545 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.01868981909006834
	batch 100 loss: 0.018172474391758443
	batch 150 loss: 0.019135943967849015
	batch 200 loss: 0.01774298621341586
LOSS [train: 0.01774298621341586] [valid: 0.016432242623947484] TIME [epoch: 544 sec]
EPOCH 11:
	batch 50 loss: 0.016671879654750227
	batch 100 loss: 0.01874584429897368
	batch 150 loss: 0.01826741429977119
	batch 200 loss: 0.020665860176086424
LOSS [train: 0.020665860176086424] [valid: 0.015348247436365151] TIME [epoch: 544 sec]
EPOCH 12:
	batch 50 loss: 0.018617031862959266
	batch 100 loss: 0.018259867988526822
	batch 150 loss: 0.01867325340397656
	batch 200 loss: 0.016956301312893628
LOSS [train: 0.016956301312893628] [valid: 0.01563407354357575] TIME [epoch: 542 sec]
EPOCH 13:
	batch 50 loss: 0.018754000253975393
	batch 100 loss: 0.022138390354812146
	batch 150 loss: 0.015849986569955943
	batch 200 loss: 0.01565406191162765
LOSS [train: 0.01565406191162765] [valid: 0.015638826893700754] TIME [epoch: 541 sec]
EPOCH 14:
	batch 50 loss: 0.017377023184672
	batch 100 loss: 0.018956317771226167
	batch 150 loss: 0.020146631486713886
	batch 200 loss: 0.016358603583648802
LOSS [train: 0.016358603583648802] [valid: 0.0157357167241571] TIME [epoch: 537 sec]
EPOCH 15:
	batch 50 loss: 0.01722861509770155
	batch 100 loss: 0.015970122618600726
	batch 150 loss: 0.018366882475093008
	batch 200 loss: 0.019734428310766817
LOSS [train: 0.019734428310766817] [valid: 0.015922101261579277] TIME [epoch: 538 sec]
EPOCH 16:
	batch 50 loss: 0.017612792570143938
	batch 100 loss: 0.017486906498670577
	batch 150 loss: 0.02004706342704594
	batch 200 loss: 0.020306750386953353
LOSS [train: 0.020306750386953353] [valid: 0.015869821769350288] TIME [epoch: 538 sec]
EPOCH 17:
	batch 50 loss: 0.019068367537111042
	batch 100 loss: 0.019330844171345233
	batch 150 loss: 0.017529654214158655
	batch 200 loss: 0.017892596619203685
LOSS [train: 0.017892596619203685] [valid: 0.0159889120356335] TIME [epoch: 539 sec]
EPOCH 18:
	batch 50 loss: 0.01763536817394197
	batch 100 loss: 0.019253119425848127
	batch 150 loss: 0.018350482657551764
	batch 200 loss: 0.017228962434455753
LOSS [train: 0.017228962434455753] [valid: 0.016238859307486565] TIME [epoch: 537 sec]
EPOCH 19:
	batch 50 loss: 0.01667465438134968
	batch 100 loss: 0.018792509259656073
	batch 150 loss: 0.017666168957948685
	batch 200 loss: 0.022615130208432675
LOSS [train: 0.022615130208432675] [valid: 0.01797758548539908] TIME [epoch: 538 sec]
EPOCH 20:
	batch 50 loss: 0.017796647921204566
	batch 100 loss: 0.018718613684177397
	batch 150 loss: 0.018128808215260507
	batch 200 loss: 0.018606813540682197
LOSS [train: 0.018606813540682197] [valid: 0.017267271816187226] TIME [epoch: 539 sec]
EPOCH 21:
	batch 50 loss: 0.016911969622597098
	batch 100 loss: 0.01841819535009563
	batch 150 loss: 0.018207981837913393
	batch 200 loss: 0.017790515283122657
LOSS [train: 0.017790515283122657] [valid: 0.015530689061658146] TIME [epoch: 541 sec]
EPOCH 22:
	batch 50 loss: 0.01846916016191244
	batch 100 loss: 0.016457427442073822
	batch 150 loss: 0.016801909897476434
	batch 200 loss: 0.015428416095674038
LOSS [train: 0.015428416095674038] [valid: 0.014475853814413615] TIME [epoch: 549 sec]
Saving model.
EPOCH 23:
	batch 50 loss: 0.017875553518533708
	batch 100 loss: 0.016143827429041267
	batch 150 loss: 0.016511109303683043
	batch 200 loss: 0.018091998361051083
LOSS [train: 0.018091998361051083] [valid: 0.015418931449433633] TIME [epoch: 539 sec]
EPOCH 24:
	batch 50 loss: 0.01420997692272067
	batch 100 loss: 0.017156928461045027
	batch 150 loss: 0.015836935294792055
	batch 200 loss: 0.017416307032108308
LOSS [train: 0.017416307032108308] [valid: 0.016306285561586264] TIME [epoch: 546 sec]
EPOCH 25:
	batch 50 loss: 0.017069309037178754
	batch 100 loss: 0.015236731683835388
	batch 150 loss: 0.017591485297307374
	batch 200 loss: 0.01656799664720893
LOSS [train: 0.01656799664720893] [valid: 0.014713225926485999] TIME [epoch: 539 sec]
EPOCH 26:
	batch 50 loss: 0.017252629362046717
	batch 100 loss: 0.01753943068906665
	batch 150 loss: 0.014391272207722067
	batch 200 loss: 0.01689060932956636
LOSS [train: 0.01689060932956636] [valid: 0.015260770308668726] TIME [epoch: 539 sec]
EPOCH 27:
	batch 50 loss: 0.016178330974653363
	batch 100 loss: 0.016219263561069966
	batch 150 loss: 0.016121161244809626
	batch 200 loss: 0.01675185827538371
LOSS [train: 0.01675185827538371] [valid: 0.01414118733179445] TIME [epoch: 538 sec]
Saving model.
EPOCH 28:
	batch 50 loss: 0.01548931458964944
	batch 100 loss: 0.015288807125762105
	batch 150 loss: 0.015415283581241966
	batch 200 loss: 0.017802682472392917
LOSS [train: 0.017802682472392917] [valid: 0.014803781484564146] TIME [epoch: 538 sec]
EPOCH 29:
	batch 50 loss: 0.01722862934693694
	batch 100 loss: 0.01590926244854927
	batch 150 loss: 0.01710087539628148
	batch 200 loss: 0.014355138801038265
LOSS [train: 0.014355138801038265] [valid: 0.01507584334249259] TIME [epoch: 539 sec]
EPOCH 30:
	batch 50 loss: 0.018596537709236145
	batch 100 loss: 0.015062132589519025
	batch 150 loss: 0.016168268220499157
	batch 200 loss: 0.014718913724645972
LOSS [train: 0.014718913724645972] [valid: 0.013844983221012323] TIME [epoch: 538 sec]
Saving model.
EPOCH 31:
	batch 50 loss: 0.015191077291965484
	batch 100 loss: 0.01568337723612785
	batch 150 loss: 0.016835933644324542
	batch 200 loss: 0.016858074385672807
LOSS [train: 0.016858074385672807] [valid: 0.01564635178850343] TIME [epoch: 536 sec]
EPOCH 32:
	batch 50 loss: 0.01679496987722814
	batch 100 loss: 0.01602284986525774
	batch 150 loss: 0.017291627554222942
	batch 200 loss: 0.014014592617750168
LOSS [train: 0.014014592617750168] [valid: 0.013748278447019402] TIME [epoch: 538 sec]
Saving model.
EPOCH 33:
	batch 50 loss: 0.014554755343124271
	batch 100 loss: 0.01734531425870955
	batch 150 loss: 0.015718178460374474
	batch 200 loss: 0.016124248849228025
LOSS [train: 0.016124248849228025] [valid: 0.015380210037013361] TIME [epoch: 539 sec]
EPOCH 34:
	batch 50 loss: 0.01554831923916936
	batch 100 loss: 0.01597929496318102
	batch 150 loss: 0.015004429770633577
	batch 200 loss: 0.015172470025718213
LOSS [train: 0.015172470025718213] [valid: 0.013596389303468943] TIME [epoch: 537 sec]
Saving model.
EPOCH 35:
	batch 50 loss: 0.015328255444765092
	batch 100 loss: 0.014134308965876698
	batch 150 loss: 0.015485658347606658
	batch 200 loss: 0.01452150423079729
LOSS [train: 0.01452150423079729] [valid: 0.01356087152986826] TIME [epoch: 538 sec]
Saving model.
EPOCH 36:
	batch 50 loss: 0.016249633803963662
	batch 100 loss: 0.015274340268224478
	batch 150 loss: 0.014925608541816474
	batch 200 loss: 0.01515328305773437
LOSS [train: 0.01515328305773437] [valid: 0.013241459993757114] TIME [epoch: 538 sec]
Saving model.
EPOCH 37:
	batch 50 loss: 0.01477641730569303
	batch 100 loss: 0.014511529169976712
	batch 150 loss: 0.01593253530561924
	batch 200 loss: 0.016263202987611292
LOSS [train: 0.016263202987611292] [valid: 0.014248499369326357] TIME [epoch: 538 sec]
EPOCH 38:
	batch 50 loss: 0.015271282326430081
	batch 100 loss: 0.013541699890047311
	batch 150 loss: 0.013549976628273725
	batch 200 loss: 0.016747200386598705
LOSS [train: 0.016747200386598705] [valid: 0.013142320110152166] TIME [epoch: 539 sec]
Saving model.
EPOCH 39:
	batch 50 loss: 0.013557436019182205
	batch 100 loss: 0.01414247952401638
	batch 150 loss: 0.014853424318134785
	batch 200 loss: 0.014114846782758832
LOSS [train: 0.014114846782758832] [valid: 0.012151180508347655] TIME [epoch: 537 sec]
Saving model.
EPOCH 40:
	batch 50 loss: 0.015492326188832522
	batch 100 loss: 0.015104492828249931
	batch 150 loss: 0.015541428783908486
	batch 200 loss: 0.014306199569255114
LOSS [train: 0.014306199569255114] [valid: 0.013599510291048016] TIME [epoch: 537 sec]
EPOCH 41:
	batch 50 loss: 0.016304182605817914
	batch 100 loss: 0.013300711801275612
	batch 150 loss: 0.013515188442543149
	batch 200 loss: 0.014783173808827996
LOSS [train: 0.014783173808827996] [valid: 0.01190871269548855] TIME [epoch: 537 sec]
Saving model.
EPOCH 42:
	batch 50 loss: 0.014251615582033993
	batch 100 loss: 0.013922598911449313
	batch 150 loss: 0.014032592959702016
	batch 200 loss: 0.015013040574267507
LOSS [train: 0.015013040574267507] [valid: 0.01287738306831064] TIME [epoch: 537 sec]
EPOCH 43:
	batch 50 loss: 0.01397885087877512
	batch 100 loss: 0.015070375595241785
	batch 150 loss: 0.014800188168883323
	batch 200 loss: 0.013647760823369026
LOSS [train: 0.013647760823369026] [valid: 0.01280778367217863] TIME [epoch: 537 sec]
EPOCH 44:
	batch 50 loss: 0.015540672317147255
	batch 100 loss: 0.014682706901803613
	batch 150 loss: 0.014053218020126223
	batch 200 loss: 0.01431981579400599
LOSS [train: 0.01431981579400599] [valid: 0.012419728926033713] TIME [epoch: 537 sec]
EPOCH 45:
	batch 50 loss: 0.014583397172391415
	batch 100 loss: 0.015361542385071515
	batch 150 loss: 0.014994494048878551
	batch 200 loss: 0.013821920249611139
LOSS [train: 0.013821920249611139] [valid: 0.012552372135542101] TIME [epoch: 538 sec]
EPOCH 46:
	batch 50 loss: 0.013473817044869066
	batch 100 loss: 0.013816889720037579
	batch 150 loss: 0.014281680565327406
	batch 200 loss: 0.01381605957634747
LOSS [train: 0.01381605957634747] [valid: 0.012262080148987782] TIME [epoch: 537 sec]
EPOCH 47:
	batch 50 loss: 0.014193608118221164
	batch 100 loss: 0.014790772637352347
	batch 150 loss: 0.013699242854490876
	batch 200 loss: 0.013527043405920267
LOSS [train: 0.013527043405920267] [valid: 0.012281769200732621] TIME [epoch: 537 sec]
EPOCH 48:
	batch 50 loss: 0.013333806106820702
	batch 100 loss: 0.015830645272508263
	batch 150 loss: 0.014124090475961566
	batch 200 loss: 0.014202322922646999
LOSS [train: 0.014202322922646999] [valid: 0.01496333000686718] TIME [epoch: 538 sec]
EPOCH 49:
	batch 50 loss: 0.01374734403565526
	batch 100 loss: 0.013516739048063754
	batch 150 loss: 0.014430222241207958
	batch 200 loss: 0.0135392814129591
LOSS [train: 0.0135392814129591] [valid: 0.012132644335846028] TIME [epoch: 538 sec]
EPOCH 50:
	batch 50 loss: 0.012133473437279463
	batch 100 loss: 0.013352976851165295
	batch 150 loss: 0.014202732220292091
	batch 200 loss: 0.013077661599963904
LOSS [train: 0.013077661599963904] [valid: 0.011665313038732469] TIME [epoch: 539 sec]
Saving model.
Finished training in 27074.528 seconds.
