Slurm job ID: 6219994
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', infer_noise=True, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6219994', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model6219994', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 245578670
EPOCH 1:
	batch 50 loss: 0.27422045335173606
	batch 100 loss: 0.03181009275838733
	batch 150 loss: 0.02585785599425435
	batch 200 loss: 0.02046108099631965
LOSS [train: 0.02046108099631965] [valid: 0.019930881775993232] TIME [epoch: 257 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.02358837398700416
	batch 100 loss: 0.022792082503437994
	batch 150 loss: 0.02341668764129281
	batch 200 loss: 0.02448309935629368
LOSS [train: 0.02448309935629368] [valid: 0.019171635836877008] TIME [epoch: 258 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.024071293296292425
	batch 100 loss: 0.02017814852297306
	batch 150 loss: 0.024142942056059838
	batch 200 loss: 0.022801216784864665
LOSS [train: 0.022801216784864665] [valid: 0.02018123567686416] TIME [epoch: 259 sec]
EPOCH 4:
	batch 50 loss: 0.024698055069893598
	batch 100 loss: 0.019808371663093568
	batch 150 loss: 0.02524114053696394
	batch 200 loss: 0.02469470728188753
LOSS [train: 0.02469470728188753] [valid: 0.01976015168329468] TIME [epoch: 259 sec]
EPOCH 5:
	batch 50 loss: 0.023322151815518735
	batch 100 loss: 0.02489150027744472
	batch 150 loss: 0.022668584315106273
	batch 200 loss: 0.02238902629353106
LOSS [train: 0.02238902629353106] [valid: 0.020331637975080714] TIME [epoch: 248 sec]
EPOCH 6:
	batch 50 loss: 0.02065581353381276
	batch 100 loss: 0.026384872542694213
	batch 150 loss: 0.025842664940282704
	batch 200 loss: 0.021688123093917967
LOSS [train: 0.021688123093917967] [valid: 0.020524293073200774] TIME [epoch: 246 sec]
EPOCH 7:
	batch 50 loss: 0.024293239265680312
	batch 100 loss: 0.021517421333119274
	batch 150 loss: 0.023258010968565942
	batch 200 loss: 0.02583009430207312
LOSS [train: 0.02583009430207312] [valid: 0.019243813114856796] TIME [epoch: 246 sec]
EPOCH 8:
	batch 50 loss: 0.024024423258379102
	batch 100 loss: 0.025879228226840495
	batch 150 loss: 0.02097406942397356
	batch 200 loss: 0.022635851707309484
LOSS [train: 0.022635851707309484] [valid: 0.02001779544564973] TIME [epoch: 247 sec]
EPOCH 9:
	batch 50 loss: 0.020509440274909137
	batch 100 loss: 0.02702402007766068
	batch 150 loss: 0.023723806589841842
	batch 200 loss: 0.022481894437223673
LOSS [train: 0.022481894437223673] [valid: 0.020688870059530018] TIME [epoch: 245 sec]
EPOCH 10:
	batch 50 loss: 0.022226168066263197
	batch 100 loss: 0.02100934988819063
	batch 150 loss: 0.024459728989750146
	batch 200 loss: 0.025286414958536625
LOSS [train: 0.025286414958536625] [valid: 0.02012860309429622] TIME [epoch: 252 sec]
EPOCH 11:
	batch 50 loss: 0.022881072284653782
	batch 100 loss: 0.02488660070113838
	batch 150 loss: 0.023369437740184365
	batch 200 loss: 0.023683235067874193
LOSS [train: 0.023683235067874193] [valid: 0.020164792146291195] TIME [epoch: 264 sec]
EPOCH 12:
	batch 50 loss: 0.02383084856905043
	batch 100 loss: 0.024394837329164146
	batch 150 loss: 0.021871987171471118
	batch 200 loss: 0.02299358997493982
LOSS [train: 0.02299358997493982] [valid: 0.020836610110321393] TIME [epoch: 263 sec]
EPOCH 13:
	batch 50 loss: 0.025059263240545988
	batch 100 loss: 0.026043769251555205
	batch 150 loss: 0.02166357864625752
	batch 200 loss: 0.020884730657562613
LOSS [train: 0.020884730657562613] [valid: 0.019841459753903716] TIME [epoch: 264 sec]
EPOCH 14:
	batch 50 loss: 0.025046529127284885
	batch 100 loss: 0.02213987414725125
	batch 150 loss: 0.02377259951084852
	batch 200 loss: 0.022440339904278517
LOSS [train: 0.022440339904278517] [valid: 0.020040674992681792] TIME [epoch: 261 sec]
EPOCH 15:
	batch 50 loss: 0.021228001173585653
	batch 100 loss: 0.023639141684398055
	batch 150 loss: 0.02457691775634885
	batch 200 loss: 0.0236819988489151
LOSS [train: 0.0236819988489151] [valid: 0.01914504078425428] TIME [epoch: 263 sec]
Saving model.
EPOCH 16:
	batch 50 loss: 0.024418105995282532
	batch 100 loss: 0.02517485111951828
	batch 150 loss: 0.021283271005377174
	batch 200 loss: 0.022490687323734163
LOSS [train: 0.022490687323734163] [valid: 0.02022100868149816] TIME [epoch: 264 sec]
EPOCH 17:
	batch 50 loss: 0.02291697300039232
	batch 100 loss: 0.023941493164747953
	batch 150 loss: 0.02368949661962688
	batch 200 loss: 0.024129574364051224
LOSS [train: 0.024129574364051224] [valid: 0.01974443936002596] TIME [epoch: 263 sec]
EPOCH 18:
	batch 50 loss: 0.024167538760229944
	batch 100 loss: 0.023337442567571996
	batch 150 loss: 0.02222032964229584
	batch 200 loss: 0.025156217981129884
LOSS [train: 0.025156217981129884] [valid: 0.020678498752143545] TIME [epoch: 261 sec]
EPOCH 19:
	batch 50 loss: 0.022319667153060437
	batch 100 loss: 0.023517182236537336
	batch 150 loss: 0.020852462342008948
	batch 200 loss: 0.027599191386252643
LOSS [train: 0.027599191386252643] [valid: 0.020447602161827186] TIME [epoch: 264 sec]
EPOCH 20:
	batch 50 loss: 0.024618217712268232
	batch 100 loss: 0.02284585832618177
	batch 150 loss: 0.02430761678144336
	batch 200 loss: 0.022942952970042826
LOSS [train: 0.022942952970042826] [valid: 0.01994649942680553] TIME [epoch: 268 sec]
EPOCH 21:
	batch 50 loss: 0.023235471127554773
	batch 100 loss: 0.021597125474363564
	batch 150 loss: 0.026251476351171733
	batch 200 loss: 0.023084877319633962
LOSS [train: 0.023084877319633962] [valid: 0.019733560235908953] TIME [epoch: 267 sec]
EPOCH 22:
	batch 50 loss: 0.024786243122071026
	batch 100 loss: 0.02246968208812177
	batch 150 loss: 0.023687578151002527
	batch 200 loss: 0.02314622052013874
LOSS [train: 0.02314622052013874] [valid: 0.019903537070543584] TIME [epoch: 266 sec]
EPOCH 23:
	batch 50 loss: 0.02504399798810482
	batch 100 loss: 0.024518697503954173
	batch 150 loss: 0.024190852222964167
	batch 200 loss: 0.02209002402611077
LOSS [train: 0.02209002402611077] [valid: 0.020484551791741978] TIME [epoch: 264 sec]
EPOCH 24:
	batch 50 loss: 0.025206193802878262
	batch 100 loss: 0.02564857926219702
	batch 150 loss: 0.022908305767923593
	batch 200 loss: 0.021149347806349396
LOSS [train: 0.021149347806349396] [valid: 0.020167509885747375] TIME [epoch: 263 sec]
EPOCH 25:
	batch 50 loss: 0.021953435791656376
	batch 100 loss: 0.020347295589745046
	batch 150 loss: 0.027414282727986575
	batch 200 loss: 0.023747171284630896
LOSS [train: 0.023747171284630896] [valid: 0.019915825057735977] TIME [epoch: 262 sec]
EPOCH 26:
	batch 50 loss: 0.02367763448506594
	batch 100 loss: 0.026027551274746655
	batch 150 loss: 0.019921953855082393
	batch 200 loss: 0.026849790690466762
LOSS [train: 0.026849790690466762] [valid: 0.01972999195762289] TIME [epoch: 262 sec]
EPOCH 27:
	batch 50 loss: 0.0219524254091084
	batch 100 loss: 0.023507062718272208
	batch 150 loss: 0.02354993173852563
	batch 200 loss: 0.024455127036198974
LOSS [train: 0.024455127036198974] [valid: 0.02000819273501596] TIME [epoch: 263 sec]
EPOCH 28:
	batch 50 loss: 0.025287013892084362
	batch 100 loss: 0.023532021921128035
	batch 150 loss: 0.022859372850507497
	batch 200 loss: 0.022929376419633628
LOSS [train: 0.022929376419633628] [valid: 0.020429160033139247] TIME [epoch: 263 sec]
EPOCH 29:
	batch 50 loss: 0.020824295440688728
	batch 100 loss: 0.02568700224161148
	batch 150 loss: 0.020489297788590193
