Slurm job ID: 6344510
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='softplus', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6344510', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='sgd', outdir='out/model_training/model6344510', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 3417832460
EPOCH 1:
	batch 50 loss: 0.5989053922891617
	batch 100 loss: 0.33491535067558287
	batch 150 loss: 0.18853143066167832
	batch 200 loss: 0.12360571667551995
LOSS [train: 0.12360571667551995] [valid: 0.10469300107409557] TIME [epoch: 246 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.08286232501268387
	batch 100 loss: 0.05858772747218609
	batch 150 loss: 0.039030066691339016
	batch 200 loss: 0.033235561884939674
LOSS [train: 0.033235561884939674] [valid: 0.033730091891872385] TIME [epoch: 249 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.02999698145315051
	batch 100 loss: 0.03171977769583464
	batch 150 loss: 0.024041794147342442
	batch 200 loss: 0.02541502930223942
LOSS [train: 0.02541502930223942] [valid: 0.022440551954787225] TIME [epoch: 248 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.02436404500156641
	batch 100 loss: 0.025481623150408268
	batch 150 loss: 0.021660389080643654
	batch 200 loss: 0.0244532766379416
LOSS [train: 0.0244532766379416] [valid: 0.020363954552643313] TIME [epoch: 248 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.02648789582774043
	batch 100 loss: 0.023503238931298256
	batch 150 loss: 0.02032172227278352
	batch 200 loss: 0.021294823689386247
LOSS [train: 0.021294823689386247] [valid: 0.019341609214704174] TIME [epoch: 248 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.019763320982456208
	batch 100 loss: 0.023638468701392412
	batch 150 loss: 0.02355060354806483
	batch 200 loss: 0.02269560594111681
LOSS [train: 0.02269560594111681] [valid: 0.019543129437079188] TIME [epoch: 249 sec]
EPOCH 7:
	batch 50 loss: 0.021756624430418016
	batch 100 loss: 0.021119536655023693
	batch 150 loss: 0.025339241521432996
	batch 200 loss: 0.02133504065684974
LOSS [train: 0.02133504065684974] [valid: 0.01871575004479382] TIME [epoch: 241 sec]
Saving model.
EPOCH 8:
	batch 50 loss: 0.020997187262400983
	batch 100 loss: 0.02277684026397765
	batch 150 loss: 0.023376632872968914
	batch 200 loss: 0.022427557352930307
LOSS [train: 0.022427557352930307] [valid: 0.0189421108996612] TIME [epoch: 237 sec]
EPOCH 9:
	batch 50 loss: 0.01989174643531442
	batch 100 loss: 0.02316099913790822
	batch 150 loss: 0.023777540028095245
	batch 200 loss: 0.02010989548638463
LOSS [train: 0.02010989548638463] [valid: 0.018552860578347465] TIME [epoch: 236 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.020608573453500867
	batch 100 loss: 0.023912808876484632
	batch 150 loss: 0.021953290328383446
	batch 200 loss: 0.019646571343764664
LOSS [train: 0.019646571343764664] [valid: 0.01835494576128743] TIME [epoch: 236 sec]
Saving model.
EPOCH 11:
	batch 50 loss: 0.022150440104305745
	batch 100 loss: 0.021577905416488647
	batch 150 loss: 0.019534026347100733
	batch 200 loss: 0.023032358102500438
LOSS [train: 0.023032358102500438] [valid: 0.018770952171568447] TIME [epoch: 235 sec]
EPOCH 12:
	batch 50 loss: 0.01974978230893612
	batch 100 loss: 0.025850224923342466
	batch 150 loss: 0.022358930092304944
	batch 200 loss: 0.021015789825469255
LOSS [train: 0.021015789825469255] [valid: 0.018362105351358574] TIME [epoch: 235 sec]
EPOCH 13:
	batch 50 loss: 0.021014614049345254
	batch 100 loss: 0.020634313011541963
	batch 150 loss: 0.021671058759093285
	batch 200 loss: 0.023618099074810744
LOSS [train: 0.023618099074810744] [valid: 0.018417273267550625] TIME [epoch: 235 sec]
EPOCH 14:
	batch 50 loss: 0.02215470962226391
	batch 100 loss: 0.021844751369208097
	batch 150 loss: 0.02177943829447031
	batch 200 loss: 0.021197075378149747
LOSS [train: 0.021197075378149747] [valid: 0.01849338110453876] TIME [epoch: 235 sec]
EPOCH 15:
	batch 50 loss: 0.021737926499918105
	batch 100 loss: 0.020778610808774828
	batch 150 loss: 0.023612820953130723
	batch 200 loss: 0.02068530355580151
LOSS [train: 0.02068530355580151] [valid: 0.018698626882057093] TIME [epoch: 236 sec]
EPOCH 16:
	batch 50 loss: 0.02251308159902692
	batch 100 loss: 0.02181657592765987
	batch 150 loss: 0.019460821682587268
	batch 200 loss: 0.02337248175404966
LOSS [train: 0.02337248175404966] [valid: 0.017944668842634806] TIME [epoch: 235 sec]
Saving model.
EPOCH 17:
	batch 50 loss: 0.022039573471993208
	batch 100 loss: 0.02090742520056665
	batch 150 loss: 0.021729941088706253
	batch 200 loss: 0.020857710829004644
LOSS [train: 0.020857710829004644] [valid: 0.018310489333089208] TIME [epoch: 235 sec]
EPOCH 18:
	batch 50 loss: 0.016884439820423723
	batch 100 loss: 0.023259752858430148
	batch 150 loss: 0.02468873955309391
	batch 200 loss: 0.020384967094287276
LOSS [train: 0.020384967094287276] [valid: 0.018969907530360312] TIME [epoch: 236 sec]
EPOCH 19:
	batch 50 loss: 0.021568471426144244
	batch 100 loss: 0.01742847836576402
	batch 150 loss: 0.021511404877528548
	batch 200 loss: 0.024495383016765116
LOSS [train: 0.024495383016765116] [valid: 0.018505505679543906] TIME [epoch: 236 sec]
EPOCH 20:
	batch 50 loss: 0.01944563097320497
	batch 100 loss: 0.020855522975325586
	batch 150 loss: 0.02101739317178726
	batch 200 loss: 0.024593057557940484
LOSS [train: 0.024593057557940484] [valid: 0.018186078756843926] TIME [epoch: 235 sec]
EPOCH 21:
	batch 50 loss: 0.021315961396321654
	batch 100 loss: 0.023770610885694623
	batch 150 loss: 0.021980978529900313
	batch 200 loss: 0.018375432454049587
LOSS [train: 0.018375432454049587] [valid: 0.01854185049790734] TIME [epoch: 225 sec]
EPOCH 22:
	batch 50 loss: 0.020135400518774986
	batch 100 loss: 0.021671784557402134
	batch 150 loss: 0.022064313953742386
	batch 200 loss: 0.021453643469139933
LOSS [train: 0.021453643469139933] [valid: 0.017553474512533286] TIME [epoch: 234 sec]
Saving model.
EPOCH 23:
	batch 50 loss: 0.021916938498616218
	batch 100 loss: 0.02214959863573313
	batch 150 loss: 0.021598012279719116
	batch 200 loss: 0.01962964894250035
LOSS [train: 0.01962964894250035] [valid: 0.018366817827579023] TIME [epoch: 237 sec]
EPOCH 24:
	batch 50 loss: 0.021829912131652238
	batch 100 loss: 0.023402874059975147
	batch 150 loss: 0.02032008582726121
	batch 200 loss: 0.01914187797345221
LOSS [train: 0.01914187797345221] [valid: 0.017566234736780945] TIME [epoch: 237 sec]
EPOCH 25:
	batch 50 loss: 0.01814509654417634
	batch 100 loss: 0.020335147827863692
	batch 150 loss: 0.021481782803311943
	batch 200 loss: 0.022759789656847714
LOSS [train: 0.022759789656847714] [valid: 0.017794947583509687] TIME [epoch: 237 sec]
EPOCH 26:
	batch 50 loss: 0.021800262220203877
	batch 100 loss: 0.020719017749652267
	batch 150 loss: 0.021079051829874514
	batch 200 loss: 0.020563041996210812
LOSS [train: 0.020563041996210812] [valid: 0.017508015980032116] TIME [epoch: 236 sec]
Saving model.
EPOCH 27:
	batch 50 loss: 0.018741534873843193
	batch 100 loss: 0.02360100045800209
	batch 150 loss: 0.02183620532974601
	batch 200 loss: 0.020650193197652697
LOSS [train: 0.020650193197652697] [valid: 0.017321496617417626] TIME [epoch: 236 sec]
Saving model.
EPOCH 28:
	batch 50 loss: 0.021082278843969106
	batch 100 loss: 0.020244571585208176
	batch 150 loss: 0.019747030884027482
	batch 200 loss: 0.022833693930879237
LOSS [train: 0.022833693930879237] [valid: 0.01820781624580074] TIME [epoch: 236 sec]
EPOCH 29:
	batch 50 loss: 0.020242200111970306
	batch 100 loss: 0.02287820987403393
	batch 150 loss: 0.02034583072178066
	batch 200 loss: 0.021438662726432085
LOSS [train: 0.021438662726432085] [valid: 0.017574010574268564] TIME [epoch: 236 sec]
EPOCH 30:
	batch 50 loss: 0.020088860215619207
	batch 100 loss: 0.022093420950695872
	batch 150 loss: 0.019967608312144877
	batch 200 loss: 0.02024465479888022
LOSS [train: 0.02024465479888022] [valid: 0.017590937154454877] TIME [epoch: 237 sec]
EPOCH 31:
	batch 50 loss: 0.019269042043015362
	batch 100 loss: 0.022614539526402952
	batch 150 loss: 0.019118016390129923
	batch 200 loss: 0.02098366274498403
LOSS [train: 0.02098366274498403] [valid: 0.01729382195577879] TIME [epoch: 236 sec]
Saving model.
EPOCH 32:
	batch 50 loss: 0.0227132217399776
	batch 100 loss: 0.02220900818705559
	batch 150 loss: 0.02072521031834185
	batch 200 loss: 0.019420891348272563
LOSS [train: 0.019420891348272563] [valid: 0.017167325859433428] TIME [epoch: 235 sec]
Saving model.
EPOCH 33:
	batch 50 loss: 0.020694342367351057
	batch 100 loss: 0.022564142607152463
	batch 150 loss: 0.01983422216027975
	batch 200 loss: 0.01841697867959738
LOSS [train: 0.01841697867959738] [valid: 0.0174003414150017] TIME [epoch: 235 sec]
EPOCH 34:
	batch 50 loss: 0.02081143982708454
	batch 100 loss: 0.021493593491613866
	batch 150 loss: 0.02209001235663891
	batch 200 loss: 0.019409352671355008
LOSS [train: 0.019409352671355008] [valid: 0.017240197516124076] TIME [epoch: 236 sec]
EPOCH 35:
	batch 50 loss: 0.02139878075569868
	batch 100 loss: 0.021611998779699205
	batch 150 loss: 0.020230912864208223
	batch 200 loss: 0.0202195699326694
LOSS [train: 0.0202195699326694] [valid: 0.017466584901315703] TIME [epoch: 235 sec]
EPOCH 36:
	batch 50 loss: 0.02304757671430707
	batch 100 loss: 0.01947417765855789
	batch 150 loss: 0.019329487131908536
	batch 200 loss: 0.0199492443818599
LOSS [train: 0.0199492443818599] [valid: 0.017423114912526218] TIME [epoch: 235 sec]
EPOCH 37:
	batch 50 loss: 0.020543473400175573
	batch 100 loss: 0.02137034717015922
	batch 150 loss: 0.020498276241123676
	batch 200 loss: 0.01946962161920965
LOSS [train: 0.01946962161920965] [valid: 0.017751163425661312] TIME [epoch: 235 sec]
EPOCH 38:
	batch 50 loss: 0.02237937679514289
	batch 100 loss: 0.02017066454514861
	batch 150 loss: 0.020091962646692992
	batch 200 loss: 0.018989277323707938
LOSS [train: 0.018989277323707938] [valid: 0.017253774896623022] TIME [epoch: 235 sec]
EPOCH 39:
	batch 50 loss: 0.02085290502756834
	batch 100 loss: 0.019840150447562335
	batch 150 loss: 0.022852220693603157
	batch 200 loss: 0.018235072661191225
LOSS [train: 0.018235072661191225] [valid: 0.017577641394988557] TIME [epoch: 235 sec]
EPOCH 40:
	batch 50 loss: 0.01977968686260283
	batch 100 loss: 0.02160273177549243
	batch 150 loss: 0.0203892781957984
	batch 200 loss: 0.019651039205491543
LOSS [train: 0.019651039205491543] [valid: 0.017444966782447106] TIME [epoch: 235 sec]
EPOCH 41:
	batch 50 loss: 0.017770193829201163
	batch 100 loss: 0.02025555184110999
	batch 150 loss: 0.02087132403627038
	batch 200 loss: 0.022973063737154006
LOSS [train: 0.022973063737154006] [valid: 0.017467239570639018] TIME [epoch: 235 sec]
EPOCH 42:
	batch 50 loss: 0.01930916341021657
	batch 100 loss: 0.01966031049378216
	batch 150 loss: 0.020354888429865242
	batch 200 loss: 0.02199767589569092
LOSS [train: 0.02199767589569092] [valid: 0.017019558788160794] TIME [epoch: 235 sec]
Saving model.
EPOCH 43:
	batch 50 loss: 0.020409719264134766
	batch 100 loss: 0.021598695311695337
	batch 150 loss: 0.01763441687449813
	batch 200 loss: 0.02244690355844796
LOSS [train: 0.02244690355844796] [valid: 0.016825608996199056] TIME [epoch: 235 sec]
Saving model.
EPOCH 44:
	batch 50 loss: 0.017666225116699935
	batch 100 loss: 0.023461176976561547
	batch 150 loss: 0.020718092508614065
	batch 200 loss: 0.01815263694152236
LOSS [train: 0.01815263694152236] [valid: 0.017571000108243123] TIME [epoch: 235 sec]
EPOCH 45:
	batch 50 loss: 0.020162580888718366
	batch 100 loss: 0.018834508266299965
	batch 150 loss: 0.022193356556817888
	batch 200 loss: 0.01897464849986136
LOSS [train: 0.01897464849986136] [valid: 0.017692219671638063] TIME [epoch: 235 sec]
EPOCH 46:
	batch 50 loss: 0.019088997915387155
	batch 100 loss: 0.017656712336465718
	batch 150 loss: 0.020712111443281174
	batch 200 loss: 0.023832561168819665
LOSS [train: 0.023832561168819665] [valid: 0.017576535145781235] TIME [epoch: 235 sec]
EPOCH 47:
	batch 50 loss: 0.021867369720712303
	batch 100 loss: 0.019665538603439926
	batch 150 loss: 0.020240567931905387
	batch 200 loss: 0.01906902953982353
LOSS [train: 0.01906902953982353] [valid: 0.01669521301543379] TIME [epoch: 235 sec]
Saving model.
EPOCH 48:
	batch 50 loss: 0.018310598274692894
	batch 100 loss: 0.018222966315224767
	batch 150 loss: 0.01885804277844727
	batch 200 loss: 0.023789979182183743
LOSS [train: 0.023789979182183743] [valid: 0.017242019169498236] TIME [epoch: 235 sec]
EPOCH 49:
	batch 50 loss: 0.022400871329009534
	batch 100 loss: 0.02192413975484669
	batch 150 loss: 0.019097578823566438
	batch 200 loss: 0.0164363431930542
LOSS [train: 0.0164363431930542] [valid: 0.01757902992903837] TIME [epoch: 235 sec]
EPOCH 50:
	batch 50 loss: 0.021460939180105925
	batch 100 loss: 0.018182827178388834
	batch 150 loss: 0.021064745523035527
	batch 200 loss: 0.02001300308853388
LOSS [train: 0.02001300308853388] [valid: 0.01694737206053105] TIME [epoch: 236 sec]
Finished training in 11950.247 seconds.
