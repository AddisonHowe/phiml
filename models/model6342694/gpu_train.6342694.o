Slurm job ID: 6342694
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='softplus', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 16], infer_noise=False, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6342694', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model6342694', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 774457832
EPOCH 1:
	batch 50 loss: 0.10114038188010455
	batch 100 loss: 0.02156099885702133
	batch 150 loss: 0.01886656160466373
	batch 200 loss: 0.023962470926344393
LOSS [train: 0.023962470926344393] [valid: 0.01843326165835606] TIME [epoch: 237 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.023033688124269248
	batch 100 loss: 0.020046186912804843
	batch 150 loss: 0.021933791972696782
	batch 200 loss: 0.020097205154597758
LOSS [train: 0.020097205154597758] [valid: 0.01753184143647862] TIME [epoch: 237 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.019560042321681976
	batch 100 loss: 0.019929661694914103
	batch 150 loss: 0.018782481253147125
	batch 200 loss: 0.017897067172452808
LOSS [train: 0.017897067172452808] [valid: 0.015651470184093342] TIME [epoch: 237 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.019296266734600068
	batch 100 loss: 0.019919718112796546
	batch 150 loss: 0.019997755764052272
	batch 200 loss: 0.039465163741260766
LOSS [train: 0.039465163741260766] [valid: 0.024365868205980707] TIME [epoch: 237 sec]
EPOCH 5:
	batch 50 loss: 0.027699040500447154
	batch 100 loss: 0.025329652456566692
	batch 150 loss: 0.020526325558312238
	batch 200 loss: 0.022690625600516795
LOSS [train: 0.022690625600516795] [valid: 0.01980019521191328] TIME [epoch: 237 sec]
EPOCH 6:
	batch 50 loss: 0.02524063950404525
	batch 100 loss: 0.02206530869938433
	batch 150 loss: 0.02001181901432574
	batch 200 loss: 0.021261997036635876
LOSS [train: 0.021261997036635876] [valid: 0.017409319777410323] TIME [epoch: 238 sec]
EPOCH 7:
	batch 50 loss: 0.02273912058211863
	batch 100 loss: 0.022253665197640656
	batch 150 loss: 0.018147552805021405
	batch 200 loss: 0.021798527957871558
LOSS [train: 0.021798527957871558] [valid: 0.01730747067219151] TIME [epoch: 238 sec]
EPOCH 8:
	batch 50 loss: 0.022536182664334773
	batch 100 loss: 0.02006254229694605
	batch 150 loss: 0.023192649260163307
	batch 200 loss: 0.020815484151244165
LOSS [train: 0.020815484151244165] [valid: 0.016614925716809618] TIME [epoch: 238 sec]
EPOCH 9:
	batch 50 loss: 0.02229737814515829
	batch 100 loss: 0.05635937571525574
	batch 150 loss: 0.046363629437983035
	batch 200 loss: 0.03235277039930225
LOSS [train: 0.03235277039930225] [valid: 0.025281469628680497] TIME [epoch: 238 sec]
EPOCH 10:
	batch 50 loss: 0.02776395403780043
	batch 100 loss: 0.022740509612485766
	batch 150 loss: 0.023809682661667466
	batch 200 loss: 0.021959599498659374
LOSS [train: 0.021959599498659374] [valid: 0.02008273824127779] TIME [epoch: 238 sec]
EPOCH 11:
	batch 50 loss: 0.021146446699276567
	batch 100 loss: 0.02548451131209731
	batch 150 loss: 0.021072534266859292
	batch 200 loss: 0.024551016744226217
LOSS [train: 0.024551016744226217] [valid: 0.019212474412051962] TIME [epoch: 238 sec]
EPOCH 12:
	batch 50 loss: 0.023098479378968476
	batch 100 loss: 0.022946690646931528
	batch 150 loss: 0.019404297191649676
	batch 200 loss: 0.024121592473238706
LOSS [train: 0.024121592473238706] [valid: 0.019207017027656547] TIME [epoch: 232 sec]
EPOCH 13:
	batch 50 loss: 0.023319227090105414
	batch 100 loss: 0.020903635285794735
	batch 150 loss: 0.020749330017715693
	batch 200 loss: 0.02474732855334878
LOSS [train: 0.02474732855334878] [valid: 0.019730175860847035] TIME [epoch: 227 sec]
EPOCH 14:
	batch 50 loss: 0.025839983616024256
	batch 100 loss: 0.020420507211238146
	batch 150 loss: 0.017755239214748145
	batch 200 loss: 0.0213589949067682
LOSS [train: 0.0213589949067682] [valid: 0.017653172474577636] TIME [epoch: 227 sec]
EPOCH 15:
	batch 50 loss: 0.01878391483798623
	batch 100 loss: 0.018231705371290444
	batch 150 loss: 0.021279742708429694
	batch 200 loss: 0.022495540715754034
LOSS [train: 0.022495540715754034] [valid: 0.01596200448596695] TIME [epoch: 226 sec]
EPOCH 16:
	batch 50 loss: 0.02070851804688573
	batch 100 loss: 0.017134921280667185
	batch 150 loss: 0.017151010436937213
	batch 200 loss: 0.022215629890561105
LOSS [train: 0.022215629890561105] [valid: 0.015801230011857115] TIME [epoch: 226 sec]
EPOCH 17:
	batch 50 loss: 0.019640743769705295
	batch 100 loss: 0.020036952523514628
	batch 150 loss: 0.021580307949334382
	batch 200 loss: 0.023315270235762
LOSS [train: 0.023315270235762] [valid: 0.01917693484914101] TIME [epoch: 225 sec]
EPOCH 18:
	batch 50 loss: 0.020680517693981527
	batch 100 loss: 0.023018800104036927
	batch 150 loss: 0.019448360204696657
	batch 200 loss: 0.020069969352334738
LOSS [train: 0.020069969352334738] [valid: 0.017394171846293223] TIME [epoch: 225 sec]
EPOCH 19:
	batch 50 loss: 1.533277605427429
	batch 100 loss: 0.2046181544661522
	batch 150 loss: 0.21642869621515273
	batch 200 loss: 0.20490266799926757
LOSS [train: 0.20490266799926757] [valid: 0.18251781635917724] TIME [epoch: 225 sec]
EPOCH 20:
	batch 50 loss: 0.2184622736275196
	batch 100 loss: 0.2121335968375206
	batch 150 loss: 0.22317718088626862
	batch 200 loss: 0.21704834580421448
LOSS [train: 0.21704834580421448] [valid: 0.1892070327885449] TIME [epoch: 225 sec]
EPOCH 21:
	batch 50 loss: 0.21772421211004256
	batch 100 loss: 0.21771265149116517
	batch 150 loss: 0.22362612009048463
	batch 200 loss: 0.21490929052233695
LOSS [train: 0.21490929052233695] [valid: 0.1890525308282425] TIME [epoch: 227 sec]
EPOCH 22:
	batch 50 loss: 0.2298870649933815
	batch 100 loss: 0.3101137751340866
	batch 150 loss: 0.5806648242473602
	batch 200 loss: 0.5663370040059089
LOSS [train: 0.5663370040059089] [valid: 0.5165055411557357] TIME [epoch: 239 sec]
EPOCH 23:
	batch 50 loss: 0.5613548624515533
	batch 100 loss: 0.4314976650476456
	batch 150 loss: 0.44071579098701474
	batch 200 loss: 0.48685323297977445
LOSS [train: 0.48685323297977445] [valid: 0.4537561223221322] TIME [epoch: 237 sec]
EPOCH 24:
	batch 50 loss: 0.4697631502151489
	batch 100 loss: 0.49374240279197695
	batch 150 loss: 0.577576933503151
	batch 200 loss: 0.5964108201861381
LOSS [train: 0.5964108201861381] [valid: 0.3571248804839949] TIME [epoch: 237 sec]
EPOCH 25:
	batch 50 loss: 0.3786772876977921
	batch 100 loss: 0.3315442979335785
	batch 150 loss: 0.34265850692987443
	batch 200 loss: 3.5113177260756494
LOSS [train: 3.5113177260756494] [valid: 4.4112271616545815] TIME [epoch: 236 sec]
EPOCH 26:
	batch 50 loss: 20.631633021235466
	batch 100 loss: 11.78764202952385
	batch 150 loss: 15.848921796679496
	batch 200 loss: 5.421729276180267
LOSS [train: 5.421729276180267] [valid: 5.5453804912045594] TIME [epoch: 237 sec]
EPOCH 27:
	batch 50 loss: 6.298983237743378
	batch 100 loss: 4.470035965442658
	batch 150 loss: 6.859058268070221
	batch 200 loss: 7.522086016535759
LOSS [train: 7.522086016535759] [valid: 12.588055324399223] TIME [epoch: 237 sec]
EPOCH 28:
	batch 50 loss: 6.039252880215645
	batch 100 loss: 19.57288340985775
	batch 150 loss: 8.63772631496191
	batch 200 loss: 6.133393805623054
LOSS [train: 6.133393805623054] [valid: 4.675943748482193] TIME [epoch: 237 sec]
EPOCH 29:
	batch 50 loss: 3.5738021740317345
	batch 100 loss: 3.51190573990345
	batch 150 loss: 4.991791023313999
	batch 200 loss: 4.50597874224186
LOSS [train: 4.50597874224186] [valid: 3.0889917007957894] TIME [epoch: 237 sec]
EPOCH 30:
	batch 50 loss: 3.8924191284179686
	batch 100 loss: 3.4556092804670335
	batch 150 loss: 2.321200760602951
	batch 200 loss: 2.8142750346660614
LOSS [train: 2.8142750346660614] [valid: 2.5691229696851225] TIME [epoch: 237 sec]
EPOCH 31:
	batch 50 loss: 2.880728905797005
	batch 100 loss: 2.5931022500991823
	batch 150 loss: 2.7924079394340513
	batch 200 loss: 1.8877179169654845
LOSS [train: 1.8877179169654845] [valid: 2.077496049987773] TIME [epoch: 236 sec]
EPOCH 32:
	batch 50 loss: 0.8158981108665466
	batch 100 loss: 0.412482348382473
	batch 150 loss: 1.5327330142259599
	batch 200 loss: 2.5475778329372405
LOSS [train: 2.5475778329372405] [valid: 2.835262371149535] TIME [epoch: 237 sec]
EPOCH 33:
	batch 50 loss: 2.209671733379364
	batch 100 loss: 1.017444261610508
	batch 150 loss: 1.5861356863379479
	batch 200 loss: 1.5943237313628196
LOSS [train: 1.5943237313628196] [valid: 1.055606360392024] TIME [epoch: 237 sec]
EPOCH 34:
	batch 50 loss: 12.932783504724503
	batch 100 loss: 38.285055881738664
	batch 150 loss: 6.021468023657799
	batch 200 loss: 127.54086374402046
LOSS [train: 127.54086374402046] [valid: 517.6562325133322] TIME [epoch: 237 sec]
EPOCH 35:
	batch 50 loss: 9.960163502693176
	batch 100 loss: 5.622808924913406
	batch 150 loss: 7.004561893939972
	batch 200 loss: 5.968799840807915
LOSS [train: 5.968799840807915] [valid: 3.3402074961612622] TIME [epoch: 237 sec]
EPOCH 36:
	batch 50 loss: 6.343026064038277
	batch 100 loss: 3.554030392765999
	batch 150 loss: 3.4619465658068656
	batch 200 loss: 4.51579020678997
LOSS [train: 4.51579020678997] [valid: 2.5722656761296094] TIME [epoch: 237 sec]
EPOCH 37:
	batch 50 loss: 4.283839671015739
	batch 100 loss: 3.147787176966667
	batch 150 loss: 3.9754230704903604
	batch 200 loss: 3.5172740402817726
LOSS [train: 3.5172740402817726] [valid: 1.826943915678809] TIME [epoch: 237 sec]
EPOCH 38:
	batch 50 loss: 2.590818471908569
	batch 100 loss: 2.760550325214863
	batch 150 loss: 1.772850894331932
	batch 200 loss: 1.9534666401147842
LOSS [train: 1.9534666401147842] [valid: 1.5716302257341643] TIME [epoch: 237 sec]
EPOCH 39:
	batch 50 loss: 1.9268951404094696
	batch 100 loss: 1.664682416319847
	batch 150 loss: 1.4036280086636543
	batch 200 loss: 0.4483883813023567
LOSS [train: 0.4483883813023567] [valid: 0.3479098624860247] TIME [epoch: 237 sec]
EPOCH 40:
	batch 50 loss: 0.3360347113013267
	batch 100 loss: 0.44348706036806107
	batch 150 loss: 0.5589842528104783
	batch 200 loss: 0.5761382538080215
LOSS [train: 0.5761382538080215] [valid: 0.4239435140353938] TIME [epoch: 237 sec]
EPOCH 41:
	batch 50 loss: 0.39660493850708006
	batch 100 loss: 0.3780540588498116
	batch 150 loss: 0.42880025148391726
	batch 200 loss: 0.5191159218549728
LOSS [train: 0.5191159218549728] [valid: 0.48519058575232826] TIME [epoch: 237 sec]
EPOCH 42:
	batch 50 loss: 0.4359272265434265
	batch 100 loss: 0.4451606947183609
	batch 150 loss: 0.44098795115947725
	batch 200 loss: 0.4367982339859009
LOSS [train: 0.4367982339859009] [valid: 0.48421534166360897] TIME [epoch: 236 sec]
EPOCH 43:
	batch 50 loss: 0.43460042476654054
	batch 100 loss: 0.44758961856365204
	batch 150 loss: 0.4491436052322388
	batch 200 loss: 0.4223345100879669
LOSS [train: 0.4223345100879669] [valid: 0.48234964059665797] TIME [epoch: 237 sec]
EPOCH 44:
	batch 50 loss: 0.430558967590332
	batch 100 loss: 0.4256416541337967
	batch 150 loss: 0.4577092462778091
	batch 200 loss: 0.43518120646476743
LOSS [train: 0.43518120646476743] [valid: 0.48397382550562423] TIME [epoch: 237 sec]
EPOCH 45:
	batch 50 loss: 0.4389479792118072
	batch 100 loss: 0.44370426595211027
	batch 150 loss: 0.43236555278301236
	batch 200 loss: 0.44211005806922915
LOSS [train: 0.44211005806922915] [valid: 0.48602953997130194] TIME [epoch: 237 sec]
EPOCH 46:
	batch 50 loss: 0.4265067964792252
	batch 100 loss: 0.41192974269390104
	batch 150 loss: 0.39866773664951327
	batch 200 loss: 0.40115202486515045
LOSS [train: 0.40115202486515045] [valid: 0.4426749876389901] TIME [epoch: 237 sec]
EPOCH 47:
	batch 50 loss: 0.41073681056499484
	batch 100 loss: 0.38351876407861707
	batch 150 loss: 0.39549096673727036
	batch 200 loss: 0.3806728595495224
LOSS [train: 0.3806728595495224] [valid: 0.43308440037071705] TIME [epoch: 237 sec]
EPOCH 48:
	batch 50 loss: 0.3889999097585678
	batch 100 loss: 0.3902239602804184
	batch 150 loss: 0.3711504369974136
	batch 200 loss: 0.36082532674074175
LOSS [train: 0.36082532674074175] [valid: 0.396887634601444] TIME [epoch: 237 sec]
EPOCH 49:
	batch 50 loss: 0.34319244205951693
	batch 100 loss: 0.34755828469991684
	batch 150 loss: 0.3361680179834366
	batch 200 loss: 0.3249774187803268
LOSS [train: 0.3249774187803268] [valid: 0.3391228749727209] TIME [epoch: 237 sec]
EPOCH 50:
	batch 50 loss: 0.30384899049997327
	batch 100 loss: 0.2874430775642395
	batch 150 loss: 0.26280145525932314
	batch 200 loss: 0.22839839845895768
LOSS [train: 0.22839839845895768] [valid: 0.2283594978818049] TIME [epoch: 237 sec]
Finished training in 11857.113 seconds.
