Slurm job ID: 7300168
args: Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['softplus'], hidden_dims=[16, 32, 32, 16], infer_noise=True, init_phi_bias_args=[0.0], init_phi_bias_method='constant', init_phi_weights_args=[0.0, 0.01], init_phi_weights_method='normal', init_tilt_bias_args=None, init_tilt_bias_method=None, init_tilt_weights_args=[0.0, 0.01], init_tilt_weights_method='normal', layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model7300168', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model7300168', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2', weight_decay=0.0)
Using device: cuda
Using seed: 2132013317
EPOCH 1:
	batch 50 loss: 0.027638169769197703
	batch 100 loss: 0.02342229817993939
	batch 150 loss: 0.024153708163648844
	batch 200 loss: 0.02037440063431859
LOSS [train: 0.02037440063431859] [valid: 0.019795546652070093] TIME [epoch: 260 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.025758954416960477
	batch 100 loss: 0.024383098017424346
	batch 150 loss: 0.02234090873040259
	batch 200 loss: 0.019420091276988388
LOSS [train: 0.019420091276988388] [valid: 0.019584542809752747] TIME [epoch: 260 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.024893763083964585
	batch 100 loss: 0.021680016070604324
	batch 150 loss: 0.02222368821501732
	batch 200 loss: 0.02193620391190052
LOSS [train: 0.02193620391190052] [valid: 0.01866462025185077] TIME [epoch: 259 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.021262272298336028
	batch 100 loss: 0.024675124175846577
	batch 150 loss: 0.020029207784682514
	batch 200 loss: 0.020740554397925735
LOSS [train: 0.020740554397925735] [valid: 0.018434609925558714] TIME [epoch: 251 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.02033528544008732
	batch 100 loss: 0.021993288286030293
	batch 150 loss: 0.02227973470464349
	batch 200 loss: 0.020218768920749426
LOSS [train: 0.020218768920749426] [valid: 0.01782591520701923] TIME [epoch: 252 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.020229869838804006
	batch 100 loss: 0.022043550089001655
	batch 150 loss: 0.020767277367413045
	batch 200 loss: 0.020540717840194702
LOSS [train: 0.020540717840194702] [valid: 0.018819731813467418] TIME [epoch: 252 sec]
EPOCH 7:
	batch 50 loss: 0.021289866492152212
	batch 100 loss: 0.020747599340975285
	batch 150 loss: 0.020562153439968825
	batch 200 loss: 0.018843169212341308
LOSS [train: 0.018843169212341308] [valid: 0.017861651132504146] TIME [epoch: 251 sec]
EPOCH 8:
	batch 50 loss: 0.019466529544442893
	batch 100 loss: 0.019623967204242945
	batch 150 loss: 0.02095865026116371
	batch 200 loss: 0.020992399603128434
LOSS [train: 0.020992399603128434] [valid: 0.0181137973961692] TIME [epoch: 251 sec]
EPOCH 9:
	batch 50 loss: 0.021329715307801964
	batch 100 loss: 0.02000281826592982
	batch 150 loss: 0.02204412842169404
	batch 200 loss: 0.020554653499275446
LOSS [train: 0.020554653499275446] [valid: 0.018468232245747156] TIME [epoch: 251 sec]
EPOCH 10:
	batch 50 loss: 0.02025423038750887
	batch 100 loss: 0.02045638920739293
	batch 150 loss: 0.020809868536889552
	batch 200 loss: 0.020332638155668974
LOSS [train: 0.020332638155668974] [valid: 0.018238553905393928] TIME [epoch: 251 sec]
EPOCH 11:
	batch 50 loss: 0.02179671570658684
	batch 100 loss: 0.01953683264553547
	batch 150 loss: 0.02132405837997794
	batch 200 loss: 0.019761022021993993
LOSS [train: 0.019761022021993993] [valid: 0.017066468048869864] TIME [epoch: 251 sec]
Saving model.
EPOCH 12:
	batch 50 loss: 0.02320986779406667
	batch 100 loss: 0.019494289383292198
	batch 150 loss: 0.01960795549675822
	batch 200 loss: 0.01875950027257204
LOSS [train: 0.01875950027257204] [valid: 0.01813946973416023] TIME [epoch: 251 sec]
EPOCH 13:
	batch 50 loss: 0.021615391056984665
	batch 100 loss: 0.021817810721695422
	batch 150 loss: 0.019172531059011816
	batch 200 loss: 0.020979655748233197
LOSS [train: 0.020979655748233197] [valid: 0.017823538645461667] TIME [epoch: 251 sec]
EPOCH 14:
	batch 50 loss: 0.020084362123161555
	batch 100 loss: 0.020182331260293722
	batch 150 loss: 0.020411165226250888
	batch 200 loss: 0.01994858095422387
LOSS [train: 0.01994858095422387] [valid: 0.017148138944079013] TIME [epoch: 251 sec]
EPOCH 15:
	batch 50 loss: 0.019353371262550354
	batch 100 loss: 0.018606880083680152
	batch 150 loss: 0.02019339693710208
	batch 200 loss: 0.020946913082152605
LOSS [train: 0.020946913082152605] [valid: 0.017914636171190067] TIME [epoch: 251 sec]
EPOCH 16:
	batch 50 loss: 0.01961086515337229
	batch 100 loss: 0.020149179995059968
	batch 150 loss: 0.020107244588434697
	batch 200 loss: 0.019322935808449983
LOSS [train: 0.019322935808449983] [valid: 0.017941534707400328] TIME [epoch: 251 sec]
EPOCH 17:
	batch 50 loss: 0.01871494695544243
	batch 100 loss: 0.021245379224419592
	batch 150 loss: 0.020375250140205026
	batch 200 loss: 0.019822311624884605
LOSS [train: 0.019822311624884605] [valid: 0.01763573464025588] TIME [epoch: 251 sec]
EPOCH 18:
	batch 50 loss: 0.021257262360304594
	batch 100 loss: 0.020669820345938207
	batch 150 loss: 0.01779057487845421
	batch 200 loss: 0.019833624381572008
LOSS [train: 0.019833624381572008] [valid: 0.016981953002202014] TIME [epoch: 251 sec]
Saving model.
EPOCH 19:
	batch 50 loss: 0.019534374633803964
	batch 100 loss: 0.020872726114466788
	batch 150 loss: 0.018732307329773903
	batch 200 loss: 0.020806136382743715
LOSS [train: 0.020806136382743715] [valid: 0.017537890422924345] TIME [epoch: 251 sec]
EPOCH 20:
	batch 50 loss: 0.018528431849554182
	batch 100 loss: 0.020552843008190394
	batch 150 loss: 0.021976096080616115
	batch 200 loss: 0.019255479825660587
LOSS [train: 0.019255479825660587] [valid: 0.017040882791237284] TIME [epoch: 251 sec]
EPOCH 21:
	batch 50 loss: 0.020763126984238624
	batch 100 loss: 0.018093348108232023
	batch 150 loss: 0.019377657771110536
	batch 200 loss: 0.02033484852872789
LOSS [train: 0.02033484852872789] [valid: 0.017376616651987812] TIME [epoch: 251 sec]
EPOCH 22:
	batch 50 loss: 0.019375606458634138
	batch 100 loss: 0.01896171475760639
	batch 150 loss: 0.02003254637122154
	batch 200 loss: 0.019916077237576248
LOSS [train: 0.019916077237576248] [valid: 0.016782465791766298] TIME [epoch: 251 sec]
Saving model.
EPOCH 23:
	batch 50 loss: 0.02139268945902586
	batch 100 loss: 0.018159808488562704
	batch 150 loss: 0.01884552063420415
	batch 200 loss: 0.02105781150981784
LOSS [train: 0.02105781150981784] [valid: 0.017460267302036907] TIME [epoch: 251 sec]
EPOCH 24:
	batch 50 loss: 0.02138077462092042
	batch 100 loss: 541.8906966675818
	batch 150 loss: 10158.138994140625
	batch 200 loss: 2811.5025927734373
LOSS [train: 2811.5025927734373] [valid: 2597.7651312679054] TIME [epoch: 251 sec]
EPOCH 25:
	batch 50 loss: 1775.759041748047
	batch 100 loss: 1785.0478759765624
	batch 150 loss: 1328.4179821777343
	batch 200 loss: 1326.1716809082031
LOSS [train: 1326.1716809082031] [valid: 1662.3382055933278] TIME [epoch: 251 sec]
EPOCH 26:
	batch 50 loss: 761.2173891830445
	batch 100 loss: 70.486705493927
	batch 150 loss: 68.10031063079833
	batch 200 loss: 74.72269485473633
LOSS [train: 74.72269485473633] [valid: 59.529153029620645] TIME [epoch: 251 sec]
EPOCH 27:
	batch 50 loss: 81.38812109947204
	batch 100 loss: 56.09043643951416
	batch 150 loss: 54.465890703201296
	batch 200 loss: 31.672564249038697
LOSS [train: 31.672564249038697] [valid: 33.567599387994655] TIME [epoch: 251 sec]
EPOCH 28:
	batch 50 loss: 33.09451171398163
	batch 100 loss: 30.851617736816408
	batch 150 loss: 26.453172175884248
	batch 200 loss: 17.52211687147617
LOSS [train: 17.52211687147617] [valid: 16.262986984321227] TIME [epoch: 251 sec]
EPOCH 29:
	batch 50 loss: 16.787722989320756
	batch 100 loss: 9.519443290233612
	batch 150 loss: 13.929939941763879
	batch 200 loss: 13.850755805969237
LOSS [train: 13.850755805969237] [valid: 16.497866738711796] TIME [epoch: 251 sec]
EPOCH 30:
	batch 50 loss: 13.277575588226318
	batch 100 loss: 13.372109079360962
	batch 150 loss: 13.250597295761109
	batch 200 loss: 12.529849271774292
LOSS [train: 12.529849271774292] [valid: 14.425415146909653] TIME [epoch: 250 sec]
EPOCH 31:
	batch 50 loss: 12.28619900226593
	batch 100 loss: 11.344387965202332
	batch 150 loss: 10.257444525659084
	batch 200 loss: 11.666714808940888
LOSS [train: 11.666714808940888] [valid: 12.721748462924733] TIME [epoch: 251 sec]
EPOCH 32:
	batch 50 loss: 9.382654518485069
	batch 100 loss: 11.233664803504944
	batch 150 loss: 11.871905341148377
	batch 200 loss: 11.14425464719534
LOSS [train: 11.14425464719534] [valid: 13.416618661954999] TIME [epoch: 251 sec]
EPOCH 33:
	batch 50 loss: 11.605425996780395
	batch 100 loss: 10.5474139213562
	batch 150 loss: 11.164382257461547
	batch 200 loss: 11.698292937278747
LOSS [train: 11.698292937278747] [valid: 13.493015740175421] TIME [epoch: 251 sec]
EPOCH 34:
	batch 50 loss: 11.201213293075561
	batch 100 loss: 11.691844754219055
	batch 150 loss: 10.493762485980987
	batch 200 loss: 11.890808992385864
LOSS [train: 11.890808992385864] [valid: 13.247731088505438] TIME [epoch: 251 sec]
EPOCH 35:
	batch 50 loss: 10.851233973503113
	batch 100 loss: 8.974024488925934
	batch 150 loss: 10.697563190460205
	batch 200 loss: 10.06582287788391
LOSS [train: 10.06582287788391] [valid: 11.830117191032818] TIME [epoch: 252 sec]
EPOCH 36:
	batch 50 loss: 10.510556588172912
	batch 100 loss: 9.112482695579528
	batch 150 loss: 8.337363678216935
	batch 200 loss: 5.258925445079804
LOSS [train: 5.258925445079804] [valid: 4.144519119802863] TIME [epoch: 251 sec]
EPOCH 37:
	batch 50 loss: 3.2389751935005187
	batch 100 loss: 2.653030169606209
	batch 150 loss: 1.5734275352954865
	batch 200 loss: 1.276215551495552
LOSS [train: 1.276215551495552] [valid: 1.3451901152729988] TIME [epoch: 251 sec]
EPOCH 38:
	batch 50 loss: 0.8403353148698807
	batch 100 loss: 0.6265212953090668
	batch 150 loss: 0.5541111773252487
	batch 200 loss: 0.5421625283360482
LOSS [train: 0.5421625283360482] [valid: 0.46149320824382206] TIME [epoch: 251 sec]
EPOCH 39:
	batch 50 loss: 0.4347725787758827
	batch 100 loss: 0.43895605266094206
	batch 150 loss: 0.3556755352020264
	batch 200 loss: 0.3669405949115753
LOSS [train: 0.3669405949115753] [valid: 0.3162939169444144] TIME [epoch: 251 sec]
EPOCH 40:
	batch 50 loss: 0.3061907604336739
	batch 100 loss: 0.30587134063243865
	batch 150 loss: 0.32310298264026643
	batch 200 loss: 0.31058366268873216
LOSS [train: 0.31058366268873216] [valid: 0.2723894929358115] TIME [epoch: 251 sec]
EPOCH 41:
	batch 50 loss: 0.2789986905455589
	batch 100 loss: 0.28636411249637606
	batch 150 loss: 0.2685351148247719
	batch 200 loss: 0.24330918610095978
LOSS [train: 0.24330918610095978] [valid: 0.2573215554933995] TIME [epoch: 251 sec]
EPOCH 42:
	batch 50 loss: 0.35429796785116197
	batch 100 loss: 0.34036003053188324
	batch 150 loss: 0.24934634268283845
	batch 200 loss: 0.21144035905599595
LOSS [train: 0.21144035905599595] [valid: 0.17786123418870073] TIME [epoch: 251 sec]
EPOCH 43:
	batch 50 loss: 0.1850853107869625
	batch 100 loss: 0.15365743160247802
	batch 150 loss: 0.1342712078243494
	batch 200 loss: 0.09453857429325581
LOSS [train: 0.09453857429325581] [valid: 0.07858467961971959] TIME [epoch: 251 sec]
EPOCH 44:
	batch 50 loss: 0.0713852022588253
	batch 100 loss: 0.05920939587056637
	batch 150 loss: 0.046275454387068746
	batch 200 loss: 0.04097895435988903
LOSS [train: 0.04097895435988903] [valid: 0.033416706455561024] TIME [epoch: 251 sec]
EPOCH 45:
	batch 50 loss: 0.038507307097315785
	batch 100 loss: 0.034194101300090554
	batch 150 loss: 0.029550997391343117
	batch 200 loss: 0.027024284210056065
LOSS [train: 0.027024284210056065] [valid: 0.02455810869093208] TIME [epoch: 251 sec]
EPOCH 46:
	batch 50 loss: 0.02682014910504222
	batch 100 loss: 0.028525577075779438
	batch 150 loss: 0.02303418573923409
	batch 200 loss: 0.025933549124747515
LOSS [train: 0.025933549124747515] [valid: 0.02232786604872672] TIME [epoch: 251 sec]
EPOCH 47:
	batch 50 loss: 0.02359086772426963
	batch 100 loss: 0.024329722514376043
	batch 150 loss: 0.024020623778924347
	batch 200 loss: 0.026642297897487878
LOSS [train: 0.026642297897487878] [valid: 0.021735068810812663] TIME [epoch: 252 sec]
EPOCH 48:
	batch 50 loss: 0.02409299555234611
	batch 100 loss: 0.021385522680357098
	batch 150 loss: 0.025742010790854693
	batch 200 loss: 0.024785739090293645
LOSS [train: 0.024785739090293645] [valid: 0.021028666781785433] TIME [epoch: 252 sec]
EPOCH 49:
	batch 50 loss: 0.02189736024476588
	batch 100 loss: 0.024142090789973737
	batch 150 loss: 0.023605164140462875
	batch 200 loss: 0.02230050278827548
LOSS [train: 0.02230050278827548] [valid: 0.0199824402021477] TIME [epoch: 251 sec]
EPOCH 50:
	batch 50 loss: 0.023667096998542546
	batch 100 loss: 0.022571946289390327
	batch 150 loss: 0.02366845866665244
	batch 200 loss: 0.02190797487273812
LOSS [train: 0.02190797487273812] [valid: 0.019314363630110166] TIME [epoch: 251 sec]
Finished training in 14146.069 seconds.
