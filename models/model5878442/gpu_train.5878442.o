Slurm job ID: 5878442
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', infer_noise=True, learning_rate=0.001, momentum=0.9, name='model5878442', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=75, optimizer='rms', outdir='out/model_training/model5878442', seed=0, sigma=0.01, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 3863204527
EPOCH 1:
	batch 50 loss: 0.21255790641531347
	batch 100 loss: 0.07098372307315003
	batch 150 loss: 0.046551629514433444
	batch 200 loss: 0.06843423414975405
LOSS [train: 0.06843423414975405] [valid: 0.0501466073008487] TIME [epoch: 249 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.056946916435845193
	batch 100 loss: 0.057805537494132295
	batch 150 loss: 0.0692604554160789
	batch 200 loss: 0.06538480453658849
LOSS [train: 0.06538480453658849] [valid: 0.05549116406279306] TIME [epoch: 241 sec]
EPOCH 3:
	batch 50 loss: 0.06819265600293875
	batch 100 loss: 0.077988120362279
	batch 150 loss: 0.05931681431015022
	batch 200 loss: 0.06927967392839492
LOSS [train: 0.06927967392839492] [valid: 0.09204526400038351] TIME [epoch: 239 sec]
EPOCH 4:
	batch 50 loss: 0.05492889936547726
	batch 100 loss: 0.05910606413614005
	batch 150 loss: 0.06900752163492144
	batch 200 loss: 0.05868256359011866
LOSS [train: 0.05868256359011866] [valid: 0.05854722843893493] TIME [epoch: 246 sec]
EPOCH 5:
	batch 50 loss: 0.0470198822603561
	batch 100 loss: 0.08120358191838023
	batch 150 loss: 0.06619118002359756
	batch 200 loss: 0.04202755784150213
LOSS [train: 0.04202755784150213] [valid: 0.058058357261082466] TIME [epoch: 250 sec]
EPOCH 6:
	batch 50 loss: 0.07640150397201069
	batch 100 loss: 0.057362424668390304
	batch 150 loss: 0.050163262278438196
	batch 200 loss: 0.05579861828286084
LOSS [train: 0.05579861828286084] [valid: 0.058652421422690774] TIME [epoch: 248 sec]
EPOCH 7:
	batch 50 loss: 0.06774166836868972
	batch 100 loss: 0.05059081361250719
	batch 150 loss: 0.07677345258183778
	batch 200 loss: 0.08498983970377594
LOSS [train: 0.08498983970377594] [valid: 0.06830069122928156] TIME [epoch: 248 sec]
EPOCH 8:
	batch 50 loss: 0.053851717952638864
	batch 100 loss: 0.060335840334882956
	batch 150 loss: 0.052461265586316586
	batch 200 loss: 0.06271198077360168
LOSS [train: 0.06271198077360168] [valid: 0.056428415926833016] TIME [epoch: 249 sec]
EPOCH 9:
	batch 50 loss: 0.05201979525736533
	batch 100 loss: 0.059319629003293815
	batch 150 loss: 0.0722596951903688
	batch 200 loss: 0.059958881619386374
LOSS [train: 0.059958881619386374] [valid: 0.049866284849122165] TIME [epoch: 248 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.08914781484927517
	batch 100 loss: 0.053856899694073944
	batch 150 loss: 0.05845876473002136
	batch 200 loss: 0.05760719826910645
LOSS [train: 0.05760719826910645] [valid: 0.07442172918235883] TIME [epoch: 248 sec]
EPOCH 11:
	batch 50 loss: 0.05945013591554016
	batch 100 loss: 0.0760321426903829
	batch 150 loss: 0.05857286070808186
	batch 200 loss: 0.0773327006585896
LOSS [train: 0.0773327006585896] [valid: 0.053660522975648446] TIME [epoch: 248 sec]
EPOCH 12:
	batch 50 loss: 0.07326891483739019
	batch 100 loss: 0.06362494866363705
	batch 150 loss: 0.05831017369811889
	batch 200 loss: 0.0433274048101157
LOSS [train: 0.0433274048101157] [valid: 0.07903496012149844] TIME [epoch: 248 sec]
EPOCH 13:
	batch 50 loss: 0.055988854303359405
	batch 100 loss: 0.05051869926508516
	batch 150 loss: 0.07658514635870234
	batch 200 loss: 0.07458194082137197
LOSS [train: 0.07458194082137197] [valid: 0.05416895700424599] TIME [epoch: 248 sec]
EPOCH 14:
	batch 50 loss: 0.05132311438210309
	batch 100 loss: 0.058855455693556
	batch 150 loss: 0.09036087034270167
	batch 200 loss: 0.06734858322422951
LOSS [train: 0.06734858322422951] [valid: 0.061556004910138046] TIME [epoch: 248 sec]
EPOCH 15:
	batch 50 loss: 0.06557595554739237
	batch 100 loss: 0.0570029654330574
	batch 150 loss: 0.05278008284512907
	batch 200 loss: 0.05672654165420681
LOSS [train: 0.05672654165420681] [valid: 0.05508663227859264] TIME [epoch: 248 sec]
EPOCH 16:
	batch 50 loss: 0.05381641815416515
	batch 100 loss: 0.05021930322516709
	batch 150 loss: 0.07504157072049565
	batch 200 loss: 0.06251398902386426
LOSS [train: 0.06251398902386426] [valid: 0.07356268826918798] TIME [epoch: 247 sec]
EPOCH 17:
	batch 50 loss: 0.06337115228641778
	batch 100 loss: 0.04073407412739471
	batch 150 loss: 0.07944574245950208
	batch 200 loss: 0.07922278190031648
LOSS [train: 0.07922278190031648] [valid: 0.08738123424894487] TIME [epoch: 248 sec]
EPOCH 18:
	batch 50 loss: 0.07529301207912795
	batch 100 loss: 0.0496447235532105
	batch 150 loss: 0.0366218982252758
	batch 200 loss: 0.10494839074613992
LOSS [train: 0.10494839074613992] [valid: 0.05124849291799668] TIME [epoch: 247 sec]
EPOCH 19:
	batch 50 loss: 0.04769928981550038
	batch 100 loss: 0.06587507994612679
	batch 150 loss: 0.056399809964932504
	batch 200 loss: 0.06197101511526853
LOSS [train: 0.06197101511526853] [valid: 0.05141146065822492] TIME [epoch: 248 sec]
EPOCH 20:
	batch 50 loss: 0.07042831299360841
	batch 100 loss: 0.06177611694438383
	batch 150 loss: 0.05607597909416654
	batch 200 loss: 0.059527066567679865
LOSS [train: 0.059527066567679865] [valid: 0.0622928473923821] TIME [epoch: 247 sec]
EPOCH 21:
	batch 50 loss: 0.052692778133496175
	batch 100 loss: 0.06918570025125519
	batch 150 loss: 0.05368835360277444
	batch 200 loss: 0.052784097471740093
LOSS [train: 0.052784097471740093] [valid: 0.04982453934111011] TIME [epoch: 248 sec]
Saving model.
EPOCH 22:
	batch 50 loss: 0.05521767358994111
	batch 100 loss: 0.0713428421466233
	batch 150 loss: 0.04786022522370331
	batch 200 loss: 0.07722294488921762
LOSS [train: 0.07722294488921762] [valid: 0.05651017761459419] TIME [epoch: 248 sec]
EPOCH 23:
	batch 50 loss: 0.043807113389484585
	batch 100 loss: 0.07046364337438718
	batch 150 loss: 0.0519296457618475
	batch 200 loss: 0.07470828815654386
LOSS [train: 0.07470828815654386] [valid: 0.047081308132813623] TIME [epoch: 248 sec]
Saving model.
EPOCH 24:
	batch 50 loss: 0.06031854964210652
	batch 100 loss: 0.050822677183896305
	batch 150 loss: 0.06595447139116004
	batch 200 loss: 0.06955480186501518
LOSS [train: 0.06955480186501518] [valid: 0.07624022084302169] TIME [epoch: 248 sec]
EPOCH 25:
	batch 50 loss: 0.06718941674334929
	batch 100 loss: 0.061739932256750764
	batch 150 loss: 0.05883976392447948
	batch 200 loss: 0.06237867787247524
LOSS [train: 0.06237867787247524] [valid: 0.07101650616774956] TIME [epoch: 248 sec]
EPOCH 26:
	batch 50 loss: 0.06613388691330328
	batch 100 loss: 0.058018206886481495
	batch 150 loss: 0.05119823484390509
	batch 200 loss: 0.06452177235041745
LOSS [train: 0.06452177235041745] [valid: 0.07090887043401987] TIME [epoch: 248 sec]
EPOCH 27:
	batch 50 loss: 0.06639966632705181
	batch 100 loss: 0.057625487972982226
	batch 150 loss: 0.062065787692554296
	batch 200 loss: 0.056809811107814315
LOSS [train: 0.056809811107814315] [valid: 0.05403392611187883] TIME [epoch: 249 sec]
EPOCH 28:
	batch 50 loss: 0.0531201323523419
	batch 100 loss: 0.07220955790951848
	batch 150 loss: 0.05584464591229334
	batch 200 loss: 0.0527339298890729
LOSS [train: 0.0527339298890729] [valid: 0.05568618224933743] TIME [epoch: 248 sec]
EPOCH 29:
	batch 50 loss: 0.045218290611519475
	batch 100 loss: 0.06279431134928018
	batch 150 loss: 0.06434555387895671
	batch 200 loss: 0.07862935813725926
LOSS [train: 0.07862935813725926] [valid: 0.11423852314862112] TIME [epoch: 248 sec]
EPOCH 30:
	batch 50 loss: 0.04446980676613748
	batch 100 loss: 0.05932167531340383
	batch 150 loss: 0.063674833397381
	batch 200 loss: 0.07477296666475013
LOSS [train: 0.07477296666475013] [valid: 0.04906277246579217] TIME [epoch: 248 sec]
EPOCH 31:
	batch 50 loss: 0.07167650976218283
	batch 100 loss: 0.06536508818855509
	batch 150 loss: 0.06435344921424985
	batch 200 loss: 0.05020069635240361
LOSS [train: 0.05020069635240361] [valid: 0.0519821336910051] TIME [epoch: 248 sec]
EPOCH 32:
	batch 50 loss: 0.07961619273759425
	batch 100 loss: 0.06402087779948488
	batch 150 loss: 0.08172527128393994
	batch 200 loss: 0.02925071924182703
LOSS [train: 0.02925071924182703] [valid: 0.052355339043424466] TIME [epoch: 249 sec]
EPOCH 33:
	batch 50 loss: 0.06689815983176231
	batch 100 loss: 0.06964177941030357
	batch 150 loss: 0.05734553851885721
	batch 200 loss: 0.07372215157112805
LOSS [train: 0.07372215157112805] [valid: 0.07522627987588446] TIME [epoch: 249 sec]
EPOCH 34:
	batch 50 loss: 0.05207356566737872
	batch 100 loss: 0.0725716008641757
	batch 150 loss: 0.05607057211920619
	batch 200 loss: 0.06353330199141055
LOSS [train: 0.06353330199141055] [valid: 0.04673492012564869] TIME [epoch: 248 sec]
Saving model.
EPOCH 35:
	batch 50 loss: 0.06880347307771445
	batch 100 loss: 0.04962414195528254
	batch 150 loss: 0.07363997397711501
	batch 200 loss: 0.0458044701628387
LOSS [train: 0.0458044701628387] [valid: 0.06387128042212377] TIME [epoch: 248 sec]
EPOCH 36:
	batch 50 loss: 0.05157227129500825
	batch 100 loss: 0.05514806363302341
	batch 150 loss: 0.07897949456004426
	batch 200 loss: 0.051570262522436676
LOSS [train: 0.051570262522436676] [valid: 0.05191980193097455] TIME [epoch: 248 sec]
EPOCH 37:
	batch 50 loss: 0.05698270965600386
	batch 100 loss: 0.06433837071526796
	batch 150 loss: 0.06176117717695888
	batch 200 loss: 0.07045052271801978
LOSS [train: 0.07045052271801978] [valid: 0.06651137257625427] TIME [epoch: 249 sec]
EPOCH 38:
	batch 50 loss: 0.06056488096306566
	batch 100 loss: 0.06010539955459535
	batch 150 loss: 0.08805219193454832
	batch 200 loss: 0.04385500331933145
LOSS [train: 0.04385500331933145] [valid: 0.06213605811135494] TIME [epoch: 248 sec]
EPOCH 39:
	batch 50 loss: 0.07143558056417533
	batch 100 loss: 0.06330040651140735
	batch 150 loss: 0.06175968173891306
	batch 200 loss: 0.044763092185603454
LOSS [train: 0.044763092185603454] [valid: 0.05789237067995903] TIME [epoch: 250 sec]
EPOCH 40:
	batch 50 loss: 0.08426747425270151
	batch 100 loss: 0.054323031800449825
	batch 150 loss: 0.04675134842982516
	batch 200 loss: 0.0459840539819561
LOSS [train: 0.0459840539819561] [valid: 0.04757051067620826] TIME [epoch: 249 sec]
EPOCH 41:
	batch 50 loss: 0.06231853713747114
	batch 100 loss: 0.054255589841632174
	batch 150 loss: 0.05371970325708389
	batch 200 loss: 0.06618861254770309
LOSS [train: 0.06618861254770309] [valid: 0.05740210360090714] TIME [epoch: 249 sec]
EPOCH 42:
	batch 50 loss: 0.062382977407542055
	batch 100 loss: 0.05498328974761534
	batch 150 loss: 0.05393322877585888
	batch 200 loss: 0.05345750820328249
LOSS [train: 0.05345750820328249] [valid: 0.05430412294420724] TIME [epoch: 248 sec]
EPOCH 43:
	batch 50 loss: 0.06995510391425341
	batch 100 loss: 0.061124661152716725
	batch 150 loss: 0.05555462285410613
	batch 200 loss: 0.06432578062522225
LOSS [train: 0.06432578062522225] [valid: 0.050485441308895436] TIME [epoch: 248 sec]
EPOCH 44:
	batch 50 loss: 0.06332192374044097
	batch 100 loss: 0.06242359454976395
	batch 150 loss: 0.06200945923570544
	batch 200 loss: 0.05844961975701153
LOSS [train: 0.05844961975701153] [valid: 0.05023542077112021] TIME [epoch: 249 sec]
EPOCH 45:
	batch 50 loss: 0.06119182422757149
	batch 100 loss: 0.05828214379027486
	batch 150 loss: 0.06634148944460322
	batch 200 loss: 0.06302852698601782
LOSS [train: 0.06302852698601782] [valid: 0.04543881828722078] TIME [epoch: 237 sec]
Saving model.
EPOCH 46:
	batch 50 loss: 0.0661625104117411
	batch 100 loss: 0.06817327578319236
	batch 150 loss: 0.06586549533996731
	batch 200 loss: 0.04305061642080545
LOSS [train: 0.04305061642080545] [valid: 0.05720995262575646] TIME [epoch: 238 sec]
EPOCH 47:
	batch 50 loss: 0.05827464596834034
	batch 100 loss: 0.06528265703178476
	batch 150 loss: 0.0482433070850675
	batch 200 loss: 0.0751072632567957
LOSS [train: 0.0751072632567957] [valid: 0.05270476012544047] TIME [epoch: 237 sec]
EPOCH 48:
	batch 50 loss: 0.050931456993566826
	batch 100 loss: 0.06803385305218398
	batch 150 loss: 0.05956656784634106
	batch 200 loss: 0.06539460903964937
LOSS [train: 0.06539460903964937] [valid: 0.055043557825653504] TIME [epoch: 238 sec]
EPOCH 49:
	batch 50 loss: 0.0723799763264833
	batch 100 loss: 0.05302562038123142
	batch 150 loss: 0.06479622022307012
	batch 200 loss: 0.061524642892181874
LOSS [train: 0.061524642892181874] [valid: 0.055988283884168294] TIME [epoch: 238 sec]
EPOCH 50:
	batch 50 loss: 0.07460696196299978
	batch 100 loss: 0.0620073199889157
	batch 150 loss: 0.043861314016394315
	batch 200 loss: 0.05701399158453569
LOSS [train: 0.05701399158453569] [valid: 0.06641068749639961] TIME [epoch: 238 sec]
EPOCH 51:
	batch 50 loss: 0.06674789184704423
	batch 100 loss: 0.08082675088488031
	batch 150 loss: 0.04926817933563143
	batch 200 loss: 0.04926855064695701
LOSS [train: 0.04926855064695701] [valid: 0.05899364182453913] TIME [epoch: 238 sec]
EPOCH 52:
	batch 50 loss: 0.06551316261989996
	batch 100 loss: 0.06047370292712003
	batch 150 loss: 0.05079786126036197
	batch 200 loss: 0.06375124269980006
LOSS [train: 0.06375124269980006] [valid: 0.05165245998408257] TIME [epoch: 238 sec]
EPOCH 53:
	batch 50 loss: 0.06594142930582166
	batch 100 loss: 0.07064976111752913
	batch 150 loss: 0.0538080896215979
	batch 200 loss: 0.06600661965596373
LOSS [train: 0.06600661965596373] [valid: 0.07218285195024994] TIME [epoch: 238 sec]
EPOCH 54:
	batch 50 loss: 0.06229581482242793
	batch 100 loss: 0.06198591676307842
	batch 150 loss: 0.05222002695663832
	batch 200 loss: 0.06252760460680293
LOSS [train: 0.06252760460680293] [valid: 0.058593820898871246] TIME [epoch: 238 sec]
EPOCH 55:
	batch 50 loss: 0.05562275187810883
	batch 100 loss: 0.06542038744315505
	batch 150 loss: 0.07221198513405397
	batch 200 loss: 0.07539272750960663
LOSS [train: 0.07539272750960663] [valid: 0.10008054448214049] TIME [epoch: 238 sec]
EPOCH 56:
	batch 50 loss: 0.05623503004666418
	batch 100 loss: 0.05776921925600618
	batch 150 loss: 0.046672718331683426
	batch 200 loss: 0.07632490531832445
LOSS [train: 0.07632490531832445] [valid: 0.05472180306290587] TIME [epoch: 238 sec]
EPOCH 57:
	batch 50 loss: 0.06864936152007431
	batch 100 loss: 0.06461864987912122
	batch 150 loss: 0.06208332071080804
	batch 200 loss: 0.051513984854100275
LOSS [train: 0.051513984854100275] [valid: 0.05096836271501767] TIME [epoch: 238 sec]
EPOCH 58:
	batch 50 loss: 0.06706556423567235
	batch 100 loss: 0.04439567530818749
	batch 150 loss: 0.07024790426716208
	batch 200 loss: 0.060334499126765875
LOSS [train: 0.060334499126765875] [valid: 0.056706170778003676] TIME [epoch: 238 sec]
EPOCH 59:
	batch 50 loss: 0.07347346813883632
	batch 100 loss: 0.06089098759228364
	batch 150 loss: 0.07199721797602252
	batch 200 loss: 0.05190613075275905
LOSS [train: 0.05190613075275905] [valid: 0.06140533502621111] TIME [epoch: 238 sec]
EPOCH 60:
	batch 50 loss: 0.08377160769654438
	batch 100 loss: 0.05459368009818718
	batch 150 loss: 0.0595717613704619
	batch 200 loss: 0.07725720368907787
LOSS [train: 0.07725720368907787] [valid: 0.059166555646030856] TIME [epoch: 238 sec]
EPOCH 61:
	batch 50 loss: 0.057443707131315025
	batch 100 loss: 0.051523205990088174
	batch 150 loss: 0.07904566443525254
	batch 200 loss: 0.060669865957861474
LOSS [train: 0.060669865957861474] [valid: 0.06178572495021702] TIME [epoch: 238 sec]
EPOCH 62:
	batch 50 loss: 0.046128675264772025
	batch 100 loss: 0.0447027914179489
	batch 150 loss: 0.06008900079905288
	batch 200 loss: 0.09145482262043515
LOSS [train: 0.09145482262043515] [valid: 0.08177033566462341] TIME [epoch: 238 sec]
EPOCH 63:
	batch 50 loss: 0.061071280929027125
	batch 100 loss: 0.06424378684721888
	batch 150 loss: 0.053931436161510644
	batch 200 loss: 0.06439076311362442
LOSS [train: 0.06439076311362442] [valid: 0.05339082520464823] TIME [epoch: 238 sec]
EPOCH 64:
	batch 50 loss: 0.0592408078443259
	batch 100 loss: 0.040826073289790654
	batch 150 loss: 0.07381395480129868
	batch 200 loss: 0.066441763912444
LOSS [train: 0.066441763912444] [valid: 0.06435756514353368] TIME [epoch: 238 sec]
EPOCH 65:
	batch 50 loss: 0.06130878262105398
	batch 100 loss: 0.05340303234988823
	batch 150 loss: 0.05912653624080121
	batch 200 loss: 0.0713839444273617
LOSS [train: 0.0713839444273617] [valid: 0.07542831721172358] TIME [epoch: 246 sec]
EPOCH 66:
	batch 50 loss: 0.056646349828224626
	batch 100 loss: 0.06505961135262624
	batch 150 loss: 0.06606903304462321
	batch 200 loss: 0.04968927692840225
LOSS [train: 0.04968927692840225] [valid: 0.06586978580162395] TIME [epoch: 247 sec]
EPOCH 67:
	batch 50 loss: 0.06377532761427573
	batch 100 loss: 0.06634525778703391
	batch 150 loss: 0.06689179284498095
	batch 200 loss: 0.049866187265142796
LOSS [train: 0.049866187265142796] [valid: 0.056037563799570006] TIME [epoch: 247 sec]
EPOCH 68:
	batch 50 loss: 0.061713450285606083
	batch 100 loss: 0.06855897057888796
	batch 150 loss: 0.046496208240278065
	batch 200 loss: 0.06446705469395965
LOSS [train: 0.06446705469395965] [valid: 0.06570392873060579] TIME [epoch: 247 sec]
EPOCH 69:
	batch 50 loss: 0.050047893309092616
	batch 100 loss: 0.06320292666438036
	batch 150 loss: 0.06897712577134371
	batch 200 loss: 0.06132494760211557
LOSS [train: 0.06132494760211557] [valid: 0.054283204457412165] TIME [epoch: 247 sec]
EPOCH 70:
	batch 50 loss: 0.04947642209008336
	batch 100 loss: 0.0546322776492525
	batch 150 loss: 0.08712271127675195
	batch 200 loss: 0.06959164334111848
LOSS [train: 0.06959164334111848] [valid: 0.054170050215907393] TIME [epoch: 247 sec]
EPOCH 71:
	batch 50 loss: 0.07297618478070944
	batch 100 loss: 0.04924341320293024
	batch 150 loss: 0.051504127068037635
	batch 200 loss: 0.05294672484276816
LOSS [train: 0.05294672484276816] [valid: 0.05377600117499241] TIME [epoch: 247 sec]
EPOCH 72:
	batch 50 loss: 0.04992782419547439
	batch 100 loss: 0.06251230075955391
	batch 150 loss: 0.05982380189117976
	batch 200 loss: 0.07754663128056563
LOSS [train: 0.07754663128056563] [valid: 0.10200421994086355] TIME [epoch: 247 sec]
EPOCH 73:
	batch 50 loss: 0.07069281888834666
	batch 100 loss: 0.06301706715952606
	batch 150 loss: 0.04559550828300416
	batch 200 loss: 0.058694689811673015
LOSS [train: 0.058694689811673015] [valid: 0.05498149180784821] TIME [epoch: 247 sec]
EPOCH 74:
	batch 50 loss: 0.05952682558447123
	batch 100 loss: 0.05566266009118408
	batch 150 loss: 0.09807189244776965
	batch 200 loss: 0.05997840718540828
LOSS [train: 0.05997840718540828] [valid: 0.050722051444851486] TIME [epoch: 247 sec]
EPOCH 75:
	batch 50 loss: 0.0678919749497436
	batch 100 loss: 0.057667164872400464
	batch 150 loss: 0.06683827915840083
	batch 200 loss: 0.05609101588372141
LOSS [train: 0.05609101588372141] [valid: 0.06056565192411654] TIME [epoch: 247 sec]
Finished training in 18377.487 seconds.
