Slurm job ID: 6582778
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6582778', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='adam', outdir='out/model_training/model6582778', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 4123454183
EPOCH 1:
	batch 50 loss: 0.27862668424844744
	batch 100 loss: 0.14262288451194763
	batch 150 loss: 0.08853547416627407
	batch 200 loss: 0.07539251521229744
LOSS [train: 0.07539251521229744] [valid: 0.08116530299303122] TIME [epoch: 467 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.05913649156689644
	batch 100 loss: 0.044446872733533385
	batch 150 loss: 0.03677535083144903
	batch 200 loss: 0.03171307120472193
LOSS [train: 0.03171307120472193] [valid: 0.033787519593412675] TIME [epoch: 469 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.028306162897497416
	batch 100 loss: 0.0293604195676744
	batch 150 loss: 0.026164701264351605
	batch 200 loss: 0.022982786856591702
LOSS [train: 0.022982786856591702] [valid: 0.029049469563081704] TIME [epoch: 468 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.024469243939965964
	batch 100 loss: 0.02465408381074667
	batch 150 loss: 0.024646148942410948
	batch 200 loss: 0.023161534033715726
LOSS [train: 0.023161534033715726] [valid: 0.02546689719116936] TIME [epoch: 467 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.021889384444802998
	batch 100 loss: 0.021413477193564175
	batch 150 loss: 0.019433573950082064
	batch 200 loss: 0.02237155510112643
LOSS [train: 0.02237155510112643] [valid: 0.024254956363195863] TIME [epoch: 447 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.019089308343827726
	batch 100 loss: 0.02064002927392721
	batch 150 loss: 0.022053886093199252
	batch 200 loss: 0.022568155471235515
LOSS [train: 0.022568155471235515] [valid: 0.021168528489458063] TIME [epoch: 460 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.020516006760299205
	batch 100 loss: 0.01776219394057989
	batch 150 loss: 0.0197533206269145
	batch 200 loss: 0.019759374894201754
LOSS [train: 0.019759374894201754] [valid: 0.02392087137256264] TIME [epoch: 472 sec]
EPOCH 8:
	batch 50 loss: 0.019053334146738054
	batch 100 loss: 0.017782169450074433
	batch 150 loss: 0.019094778504222632
	batch 200 loss: 0.018311192113906145
LOSS [train: 0.018311192113906145] [valid: 0.019089810354247068] TIME [epoch: 441 sec]
Saving model.
EPOCH 9:
	batch 50 loss: 0.01711978789418936
	batch 100 loss: 0.01745414974167943
	batch 150 loss: 0.01605098710395396
	batch 200 loss: 0.018095217952504752
LOSS [train: 0.018095217952504752] [valid: 0.01823098103001636] TIME [epoch: 438 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.018317225947976114
	batch 100 loss: 0.016780706765130163
	batch 150 loss: 0.01642084370367229
	batch 200 loss: 0.017351078726351263
LOSS [train: 0.017351078726351263] [valid: 0.01724135451465069] TIME [epoch: 461 sec]
Saving model.
EPOCH 11:
	batch 50 loss: 0.01611317465081811
	batch 100 loss: 0.01628677579574287
	batch 150 loss: 0.015527644716203213
	batch 200 loss: 0.017950069247744976
LOSS [train: 0.017950069247744976] [valid: 0.018443964240335238] TIME [epoch: 449 sec]
EPOCH 12:
	batch 50 loss: 0.0177853976842016
	batch 100 loss: 0.018114920416846873
	batch 150 loss: 0.016312743285670877
	batch 200 loss: 0.015887380698695778
LOSS [train: 0.015887380698695778] [valid: 0.016640855510437782] TIME [epoch: 466 sec]
Saving model.
EPOCH 13:
	batch 50 loss: 0.015320490757003426
	batch 100 loss: 0.01718648680485785
	batch 150 loss: 0.01594022298231721
	batch 200 loss: 0.015942114125937223
LOSS [train: 0.015942114125937223] [valid: 0.015488885755136532] TIME [epoch: 468 sec]
Saving model.
EPOCH 14:
	batch 50 loss: 0.015925761442631484
	batch 100 loss: 0.015810523405671118
	batch 150 loss: 0.015906702503561975
	batch 200 loss: 0.01461254126392305
LOSS [train: 0.01461254126392305] [valid: 0.014072193207054322] TIME [epoch: 442 sec]
Saving model.
EPOCH 15:
	batch 50 loss: 0.015336000677198171
	batch 100 loss: 0.01388668093830347
	batch 150 loss: 0.014916207995265723
	batch 200 loss: 0.018470331262797117
LOSS [train: 0.018470331262797117] [valid: 0.022863996770077697] TIME [epoch: 417 sec]
EPOCH 16:
	batch 50 loss: 0.01941052715294063
	batch 100 loss: 0.014694701945409179
	batch 150 loss: 0.014928166698664426
	batch 200 loss: 0.015368791278451682
LOSS [train: 0.015368791278451682] [valid: 0.015255468355705185] TIME [epoch: 426 sec]
EPOCH 17:
	batch 50 loss: 0.015719450488686563
	batch 100 loss: 0.014190431754104792
	batch 150 loss: 0.015141405556350946
	batch 200 loss: 0.014576737741008402
LOSS [train: 0.014576737741008402] [valid: 0.015758200215835436] TIME [epoch: 460 sec]
EPOCH 18:
	batch 50 loss: 0.015214515179395675
	batch 100 loss: 0.014604409886524082
	batch 150 loss: 0.015921053811907768
	batch 200 loss: 0.016210958175361157
LOSS [train: 0.016210958175361157] [valid: 0.014209980598388938] TIME [epoch: 459 sec]
EPOCH 19:
	batch 50 loss: 0.015982389338314532
	batch 100 loss: 0.015194727815687656
	batch 150 loss: 0.013154822550714016
	batch 200 loss: 0.014882251275703312
LOSS [train: 0.014882251275703312] [valid: 0.01522492575556195] TIME [epoch: 473 sec]
EPOCH 20:
	batch 50 loss: 0.01484272176399827
	batch 100 loss: 0.02349688483402133
	batch 150 loss: 0.01927182702347636
	batch 200 loss: 0.02012317324988544
LOSS [train: 0.02012317324988544] [valid: 0.01651016195731548] TIME [epoch: 486 sec]
EPOCH 21:
	batch 50 loss: 0.01771310767158866
	batch 100 loss: 0.016213181745260954
	batch 150 loss: 0.01550296038389206
	batch 200 loss: 0.016417748061940074
LOSS [train: 0.016417748061940074] [valid: 0.01737611708425296] TIME [epoch: 483 sec]
EPOCH 22:
	batch 50 loss: 0.01988779367879033
	batch 100 loss: 0.01732331121340394
	batch 150 loss: 0.016261459328234196
	batch 200 loss: 0.019626926584169268
LOSS [train: 0.019626926584169268] [valid: 0.015854315677521904] TIME [epoch: 476 sec]
EPOCH 23:
	batch 50 loss: 0.017207643184810877
	batch 100 loss: 0.016623525032773614
	batch 150 loss: 0.014507189616560936
	batch 200 loss: 0.015618028026074171
LOSS [train: 0.015618028026074171] [valid: 0.012188004309912988] TIME [epoch: 467 sec]
Saving model.
EPOCH 24:
	batch 50 loss: 0.013295562034472823
	batch 100 loss: 0.014318128349259496
	batch 150 loss: 0.013870814787223936
	batch 200 loss: 0.01487521138973534
LOSS [train: 0.01487521138973534] [valid: 0.011388669199853515] TIME [epoch: 470 sec]
Saving model.
EPOCH 25:
	batch 50 loss: 0.013888770947232842
	batch 100 loss: 0.015808961326256393
	batch 150 loss: 0.014513244619593024
	batch 200 loss: 0.014242250667884947
LOSS [train: 0.014242250667884947] [valid: 0.012704447176414153] TIME [epoch: 450 sec]
EPOCH 26:
	batch 50 loss: 0.014493751907721161
	batch 100 loss: 0.013056390257552265
	batch 150 loss: 0.01373636091593653
	batch 200 loss: 0.014842695984989406
LOSS [train: 0.014842695984989406] [valid: 0.01502028935647104] TIME [epoch: 426 sec]
EPOCH 27:
	batch 50 loss: 0.0178263645991683
	batch 100 loss: 0.015467432653531432
	batch 150 loss: 0.013732487568631768
	batch 200 loss: 0.01393395426683128
LOSS [train: 0.01393395426683128] [valid: 0.01364677081728587] TIME [epoch: 433 sec]
EPOCH 28:
	batch 50 loss: 0.015162431779317557
	batch 100 loss: 0.014502484574913979
	batch 150 loss: 0.01964397944509983
	batch 200 loss: 0.023500723484903573
LOSS [train: 0.023500723484903573] [valid: 0.018220115816560186] TIME [epoch: 449 sec]
EPOCH 29:
	batch 50 loss: 0.018338516680523755
	batch 100 loss: 0.018238339629024266
	batch 150 loss: 0.017072812607511877
	batch 200 loss: 0.015277859065681696
LOSS [train: 0.015277859065681696] [valid: 0.014046513488574419] TIME [epoch: 447 sec]
EPOCH 30:
	batch 50 loss: 0.014599794093519448
	batch 100 loss: 0.013325343327596783
	batch 150 loss: 0.013014302207157016
	batch 200 loss: 0.013606616165488958
LOSS [train: 0.013606616165488958] [valid: 0.011583460114585857] TIME [epoch: 445 sec]
EPOCH 31:
	batch 50 loss: 0.01204468754120171
	batch 100 loss: 0.01215961368754506
	batch 150 loss: 0.01326091831550002
	batch 200 loss: 0.012064722534269094
LOSS [train: 0.012064722534269094] [valid: 0.011146535198107207] TIME [epoch: 441 sec]
Saving model.
EPOCH 32:
	batch 50 loss: 0.01254662954248488
	batch 100 loss: 0.011835189331322908
	batch 150 loss: 0.013199355797842144
	batch 200 loss: 0.060102608110755684
LOSS [train: 0.060102608110755684] [valid: 0.05070928976832268] TIME [epoch: 436 sec]
EPOCH 33:
	batch 50 loss: 0.03366493728011846
	batch 100 loss: 0.02654468599706888
	batch 150 loss: 0.018942513931542636
	batch 200 loss: 0.015196437370032073
LOSS [train: 0.015196437370032073] [valid: 0.013775756606870951] TIME [epoch: 452 sec]
EPOCH 34:
	batch 50 loss: 0.015146451471373439
	batch 100 loss: 0.014107709713280202
	batch 150 loss: 0.01291819422505796
	batch 200 loss: 0.013430943787097931
LOSS [train: 0.013430943787097931] [valid: 0.01178134622023208] TIME [epoch: 454 sec]
EPOCH 35:
	batch 50 loss: 0.013100464353337884
	batch 100 loss: 0.012266685804352164
	batch 150 loss: 0.011803368888795376
	batch 200 loss: 0.012458142321556807
LOSS [train: 0.012458142321556807] [valid: 0.0107658623822014] TIME [epoch: 454 sec]
Saving model.
EPOCH 36:
	batch 50 loss: 0.01129287578165531
	batch 100 loss: 0.013248890629038215
	batch 150 loss: 0.011573181254789233
	batch 200 loss: 0.012404312072321773
LOSS [train: 0.012404312072321773] [valid: 0.010471111690276303] TIME [epoch: 465 sec]
Saving model.
EPOCH 37:
	batch 50 loss: 0.011374410167336464
	batch 100 loss: 0.011984610399231314
	batch 150 loss: 0.011868690699338913
	batch 200 loss: 0.012408390641212463
LOSS [train: 0.012408390641212463] [valid: 0.009876509149520037] TIME [epoch: 423 sec]
Saving model.
EPOCH 38:
	batch 50 loss: 0.011733251214027405
	batch 100 loss: 0.010978865660727024
	batch 150 loss: 0.011702019916847349
	batch 200 loss: 0.011581850219517947
LOSS [train: 0.011581850219517947] [valid: 0.010178163417731412] TIME [epoch: 406 sec]
EPOCH 39:
	batch 50 loss: 0.011542179267853498
	batch 100 loss: 0.01202456446364522
	batch 150 loss: 0.01149253435432911
	batch 200 loss: 0.012533633541315793
LOSS [train: 0.012533633541315793] [valid: 0.010419510643259855] TIME [epoch: 405 sec]
EPOCH 40:
	batch 50 loss: 0.012104089390486478
	batch 100 loss: 0.011065436657518149
	batch 150 loss: 0.012488409178331494
	batch 200 loss: 0.012268608370795846
LOSS [train: 0.012268608370795846] [valid: 0.009316047328320564] TIME [epoch: 410 sec]
Saving model.
EPOCH 41:
	batch 50 loss: 0.011228128466755152
	batch 100 loss: 0.011658685198053718
	batch 150 loss: 0.011373317735269665
	batch 200 loss: 0.011804782799445093
LOSS [train: 0.011804782799445093] [valid: 0.009737253738179182] TIME [epoch: 395 sec]
EPOCH 42:
	batch 50 loss: 0.012049843706190586
	batch 100 loss: 0.011021162485703825
	batch 150 loss: 0.01111012076959014
	batch 200 loss: 0.010682816691696644
LOSS [train: 0.010682816691696644] [valid: 0.009537085023960874] TIME [epoch: 412 sec]
EPOCH 43:
	batch 50 loss: 0.011526949945837259
	batch 100 loss: 0.01207241641357541
	batch 150 loss: 0.011810265239328146
	batch 200 loss: 0.01116566705517471
LOSS [train: 0.01116566705517471] [valid: 0.01046802390483208] TIME [epoch: 412 sec]
EPOCH 44:
	batch 50 loss: 0.0125166632886976
	batch 100 loss: 0.011962571265175938
	batch 150 loss: 0.012101288866251707
	batch 200 loss: 0.010571689167991281
LOSS [train: 0.010571689167991281] [valid: 0.008865671043895417] TIME [epoch: 410 sec]
Saving model.
EPOCH 45:
	batch 50 loss: 0.010776040256023406
	batch 100 loss: 0.011446883836761117
	batch 150 loss: 0.011442741192877293
	batch 200 loss: 0.011319615617394448
LOSS [train: 0.011319615617394448] [valid: 0.008870881444211894] TIME [epoch: 414 sec]
EPOCH 46:
	batch 50 loss: 0.011535490481182933
	batch 100 loss: 0.012517153192311526
	batch 150 loss: 0.011226391568779946
	batch 200 loss: 0.011301711332052947
LOSS [train: 0.011301711332052947] [valid: 0.00911462027276381] TIME [epoch: 424 sec]
EPOCH 47:
	batch 50 loss: 0.011135738212615251
	batch 100 loss: 0.011138303168118
	batch 150 loss: 0.011429832046851517
	batch 200 loss: 0.012285408610478044
LOSS [train: 0.012285408610478044] [valid: 0.009257460448740555] TIME [epoch: 406 sec]
EPOCH 48:
	batch 50 loss: 0.011891754809767008
	batch 100 loss: 0.012183506684377789
	batch 150 loss: 0.011980085838586093
	batch 200 loss: 0.011936261868104339
LOSS [train: 0.011936261868104339] [valid: 0.009320952070508308] TIME [epoch: 399 sec]
EPOCH 49:
	batch 50 loss: 0.011981484210118652
	batch 100 loss: 0.011590792918577791
	batch 150 loss: 0.011889717634767294
	batch 200 loss: 0.011121730022132396
LOSS [train: 0.011121730022132396] [valid: 0.008655989652470452] TIME [epoch: 402 sec]
Saving model.
EPOCH 50:
	batch 50 loss: 0.011322023468092085
	batch 100 loss: 0.011607352662831544
	batch 150 loss: 0.01091302203014493
	batch 200 loss: 0.011500145550817251
LOSS [train: 0.011500145550817251] [valid: 0.009000124587570706] TIME [epoch: 396 sec]
Finished training in 22259.028 seconds.
