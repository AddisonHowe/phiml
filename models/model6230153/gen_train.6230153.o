Slurm job ID: 6230153
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', infer_noise=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6230153', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model6230153', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 403802542
EPOCH 1:
	batch 50 loss: 0.2498424067348242
	batch 100 loss: 0.025840204078704118
	batch 150 loss: 0.023531844429671763
	batch 200 loss: 0.023548177517950533
LOSS [train: 0.023548177517950533] [valid: 0.020121381339655877] TIME [epoch: 259 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.02172041490674019
	batch 100 loss: 0.021187793789431454
	batch 150 loss: 0.022294047232717277
	batch 200 loss: 0.02165280928835273
LOSS [train: 0.02165280928835273] [valid: 0.019246755767865883] TIME [epoch: 262 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.023901631012558936
	batch 100 loss: 0.021024991385638715
	batch 150 loss: 0.025060928780585526
	batch 200 loss: 0.023806636165827514
LOSS [train: 0.023806636165827514] [valid: 0.02061854162044862] TIME [epoch: 262 sec]
EPOCH 4:
	batch 50 loss: 0.024964703721925618
	batch 100 loss: 0.02249423840083182
	batch 150 loss: 0.023692523352801798
	batch 200 loss: 0.02246678384952247
LOSS [train: 0.02246678384952247] [valid: 0.02030068121603108] TIME [epoch: 262 sec]
EPOCH 5:
	batch 50 loss: 0.02039627875201404
	batch 100 loss: 0.021663600131869316
	batch 150 loss: 0.02380084400065243
	batch 200 loss: 0.024225253770127894
LOSS [train: 0.024225253770127894] [valid: 0.019285961153218522] TIME [epoch: 262 sec]
EPOCH 6:
	batch 50 loss: 0.020095055233687164
	batch 100 loss: 0.02188911512494087
	batch 150 loss: 0.02293702844530344
	batch 200 loss: 0.020332503858953715
LOSS [train: 0.020332503858953715] [valid: 0.016392744666033347] TIME [epoch: 262 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.01779223846271634
	batch 100 loss: 0.019544235421344636
	batch 150 loss: 0.025367339169606568
	batch 200 loss: 0.024555049370974302
LOSS [train: 0.024555049370974302] [valid: 0.018774241130934875] TIME [epoch: 261 sec]
EPOCH 8:
	batch 50 loss: 0.023391017708927393
	batch 100 loss: 0.02651957478374243
	batch 150 loss: 0.025073592960834504
	batch 200 loss: 0.018608142724260687
LOSS [train: 0.018608142724260687] [valid: 0.019851893087858723] TIME [epoch: 262 sec]
EPOCH 9:
	batch 50 loss: 0.022789659071713687
	batch 100 loss: 0.022738535506650805
	batch 150 loss: 0.023098722845315934
	batch 200 loss: 0.02271963108330965
LOSS [train: 0.02271963108330965] [valid: 0.018322301920125027] TIME [epoch: 261 sec]
EPOCH 10:
	batch 50 loss: 0.02190405948087573
	batch 100 loss: 0.023230064092203975
	batch 150 loss: 0.02121563399210572
	batch 200 loss: 0.021498070787638424
LOSS [train: 0.021498070787638424] [valid: 0.018832252855645493] TIME [epoch: 262 sec]
EPOCH 11:
	batch 50 loss: 0.02123129863291979
	batch 100 loss: 0.02174266690388322
	batch 150 loss: 0.02264672374352813
	batch 200 loss: 0.022258462412282826
LOSS [train: 0.022258462412282826] [valid: 0.018092263232877787] TIME [epoch: 263 sec]
EPOCH 12:
	batch 50 loss: 0.02180262729525566
	batch 100 loss: 0.021162429358810186
	batch 150 loss: 0.01904431545175612
	batch 200 loss: 0.024437688868492843
LOSS [train: 0.024437688868492843] [valid: 0.01876972138270503] TIME [epoch: 261 sec]
EPOCH 13:
	batch 50 loss: 0.025302536003291607
	batch 100 loss: 0.019372866488993168
	batch 150 loss: 0.02246405209414661
	batch 200 loss: 0.02210152274928987
LOSS [train: 0.02210152274928987] [valid: 0.018118341708759544] TIME [epoch: 261 sec]
EPOCH 14:
	batch 50 loss: 0.020061788037419318
	batch 100 loss: 0.022140129916369915
	batch 150 loss: 0.024408669294789433
	batch 200 loss: 0.026604646211490036
LOSS [train: 0.026604646211490036] [valid: 0.020283067143221464] TIME [epoch: 260 sec]
EPOCH 15:
	batch 50 loss: 0.022185005377978087
	batch 100 loss: 0.026812220541760325
	batch 150 loss: 0.022275777282193304
	batch 200 loss: 0.021353482734411955
LOSS [train: 0.021353482734411955] [valid: 0.0198251367432628] TIME [epoch: 262 sec]
EPOCH 16:
	batch 50 loss: 0.023220510985702277
	batch 100 loss: 0.02591006100177765
	batch 150 loss: 0.02106467152945697
	batch 200 loss: 0.024146382519975303
LOSS [train: 0.024146382519975303] [valid: 0.022180752372757222] TIME [epoch: 262 sec]
EPOCH 17:
	batch 50 loss: 0.022378892805427313
	batch 100 loss: 0.026476099090650677
	batch 150 loss: 0.021654506754130123
	batch 200 loss: 0.02232800333760679
LOSS [train: 0.02232800333760679] [valid: 0.020286383575633712] TIME [epoch: 261 sec]
EPOCH 18:
	batch 50 loss: 0.02281978351995349
	batch 100 loss: 0.022015670640394093
	batch 150 loss: 0.022493885140866043
	batch 200 loss: 0.025869552828371525
LOSS [train: 0.025869552828371525] [valid: 0.019907011607817063] TIME [epoch: 261 sec]
EPOCH 19:
	batch 50 loss: 0.02115589624270797
	batch 100 loss: 0.024371234998106957
	batch 150 loss: 0.023728780038654803
	batch 200 loss: 0.025681062955409287
LOSS [train: 0.025681062955409287] [valid: 0.02034070980056034] TIME [epoch: 261 sec]
EPOCH 20:
	batch 50 loss: 0.023412538934499027
	batch 100 loss: 0.023672503186389804
	batch 150 loss: 0.02201103065162897
	batch 200 loss: 0.024285976449027656
LOSS [train: 0.024285976449027656] [valid: 0.020401814081318057] TIME [epoch: 264 sec]
EPOCH 21:
	batch 50 loss: 0.023957619983702896
	batch 100 loss: 0.023091612961143256
	batch 150 loss: 0.02306885240599513
	batch 200 loss: 0.023816348118707538
LOSS [train: 0.023816348118707538] [valid: 0.020605588274581046] TIME [epoch: 262 sec]
EPOCH 22:
	batch 50 loss: 0.025341051770374177
	batch 100 loss: 0.02478290632367134
	batch 150 loss: 0.022255200333893298
	batch 200 loss: 0.022490931348875164
LOSS [train: 0.022490931348875164] [valid: 0.020342121464879407] TIME [epoch: 262 sec]
EPOCH 23:
	batch 50 loss: 0.026186134172603487
	batch 100 loss: 0.0196478128246963
	batch 150 loss: 0.023852373184636236
	batch 200 loss: 0.022643844550475477
LOSS [train: 0.022643844550475477] [valid: 0.0200084409424259] TIME [epoch: 261 sec]
EPOCH 24:
	batch 50 loss: 0.022246305560693146
	batch 100 loss: 0.020962477549910544
	batch 150 loss: 0.024240978742018342
	batch 200 loss: 0.025637269858270882
LOSS [train: 0.025637269858270882] [valid: 0.02034953518596012] TIME [epoch: 262 sec]
EPOCH 25:
	batch 50 loss: 0.020278908908367158
	batch 100 loss: 0.02381083835847676
	batch 150 loss: 0.025749336695298553
	batch 200 loss: 0.022652141619473695
LOSS [train: 0.022652141619473695] [valid: 0.020924810367675187] TIME [epoch: 261 sec]
EPOCH 26:
	batch 50 loss: 0.025660961885005235
	batch 100 loss: 0.022819259371608495
	batch 150 loss: 0.021945585906505586
	batch 200 loss: 0.023635382754728197
LOSS [train: 0.023635382754728197] [valid: 0.020003894367982868] TIME [epoch: 261 sec]
EPOCH 27:
	batch 50 loss: 0.02506700649857521
	batch 100 loss: 0.02130951308645308
	batch 150 loss: 0.02140689691528678
	batch 200 loss: 0.02596377009525895
LOSS [train: 0.02596377009525895] [valid: 0.021642133425727177] TIME [epoch: 261 sec]
EPOCH 28:
	batch 50 loss: 0.02384442087262869
	batch 100 loss: 0.022612930061295628
	batch 150 loss: 0.020870142262429
	batch 200 loss: 0.025257557425647975
LOSS [train: 0.025257557425647975] [valid: 0.01970264164216739] TIME [epoch: 261 sec]
EPOCH 29:
	batch 50 loss: 0.018938344614580275
