Slurm job ID: 6347932
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6347932', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='sgd', outdir='out/model_training/model6347932', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 4262887713
EPOCH 1:
	batch 50 loss: 0.4234597009420395
	batch 100 loss: 0.20749402061104774
	batch 150 loss: 0.11750503078103065
	batch 200 loss: 0.07781548753380775
LOSS [train: 0.07781548753380775] [valid: 0.054601477552205326] TIME [epoch: 268 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.053678077086806296
	batch 100 loss: 0.04319706842303276
	batch 150 loss: 0.0365666090324521
	batch 200 loss: 0.03238632224500179
LOSS [train: 0.03238632224500179] [valid: 0.02530946060154141] TIME [epoch: 268 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.029475497920066117
	batch 100 loss: 0.02557735363021493
	batch 150 loss: 0.023786327932029963
	batch 200 loss: 0.02071439556777477
LOSS [train: 0.02071439556777477] [valid: 0.01943871182447765] TIME [epoch: 269 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.02237384341657162
	batch 100 loss: 0.022212798688560725
	batch 150 loss: 0.02078515965491533
	batch 200 loss: 0.0211498630233109
LOSS [train: 0.0211498630233109] [valid: 0.018025532192647613] TIME [epoch: 269 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.019069610480219126
	batch 100 loss: 0.02165406514890492
	batch 150 loss: 0.022305058436468243
	batch 200 loss: 0.019047696432098745
LOSS [train: 0.019047696432098745] [valid: 0.017065828063884205] TIME [epoch: 270 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.01983513094484806
	batch 100 loss: 0.01942357378080487
	batch 150 loss: 0.021561489794403314
	batch 200 loss: 0.020152882169932128
LOSS [train: 0.020152882169932128] [valid: 0.017712656020012218] TIME [epoch: 269 sec]
EPOCH 7:
	batch 50 loss: 0.019061426101252436
	batch 100 loss: 0.019465407095849513
	batch 150 loss: 0.02059535790234804
	batch 200 loss: 0.021010410133749248
LOSS [train: 0.021010410133749248] [valid: 0.01822697829969305] TIME [epoch: 269 sec]
EPOCH 8:
	batch 50 loss: 0.019552240874618293
	batch 100 loss: 0.020216129925101994
	batch 150 loss: 0.0191901065595448
	batch 200 loss: 0.02028852269053459
LOSS [train: 0.02028852269053459] [valid: 0.017528404945187502] TIME [epoch: 271 sec]
EPOCH 9:
	batch 50 loss: 0.018855902682989836
	batch 100 loss: 0.02023779259994626
	batch 150 loss: 0.021210395814850926
	batch 200 loss: 0.01879222205840051
LOSS [train: 0.01879222205840051] [valid: 0.017579322955619623] TIME [epoch: 270 sec]
EPOCH 10:
	batch 50 loss: 0.019436296680942176
	batch 100 loss: 0.02075468819588423
	batch 150 loss: 0.020048201493918897
	batch 200 loss: 0.019097665883600713
LOSS [train: 0.019097665883600713] [valid: 0.017478440640843473] TIME [epoch: 270 sec]
EPOCH 11:
	batch 50 loss: 0.021206915471702815
	batch 100 loss: 0.019525428237393498
	batch 150 loss: 0.020699276234954597
	batch 200 loss: 0.019369690576568246
LOSS [train: 0.019369690576568246] [valid: 0.017172590774619797] TIME [epoch: 270 sec]
EPOCH 12:
	batch 50 loss: 0.019778942177072167
	batch 100 loss: 0.018707409519702196
	batch 150 loss: 0.01928851621225476
	batch 200 loss: 0.020862115817144512
LOSS [train: 0.020862115817144512] [valid: 0.017486632583556153] TIME [epoch: 270 sec]
EPOCH 13:
	batch 50 loss: 0.01982921879738569
	batch 100 loss: 0.021051350990310313
	batch 150 loss: 0.019731005607172846
	batch 200 loss: 0.020263678915798663
LOSS [train: 0.020263678915798663] [valid: 0.0169685193922002] TIME [epoch: 270 sec]
Saving model.
EPOCH 14:
	batch 50 loss: 0.018298274651169778
	batch 100 loss: 0.020885319532826543
	batch 150 loss: 0.021428252179175614
	batch 200 loss: 0.01879705647006631
LOSS [train: 0.01879705647006631] [valid: 0.016922361016137682] TIME [epoch: 266 sec]
Saving model.
EPOCH 15:
	batch 50 loss: 0.01945492159575224
	batch 100 loss: 0.019718274492770433
	batch 150 loss: 0.019671400608494878
	batch 200 loss: 0.01946804584003985
LOSS [train: 0.01946804584003985] [valid: 0.017706788880119954] TIME [epoch: 246 sec]
EPOCH 16:
	batch 50 loss: 0.019777724966406823
	batch 100 loss: 0.019190565263852478
	batch 150 loss: 0.019124758718535303
	batch 200 loss: 0.022068599946796896
LOSS [train: 0.022068599946796896] [valid: 0.017267383539971586] TIME [epoch: 244 sec]
EPOCH 17:
	batch 50 loss: 0.02002647764980793
	batch 100 loss: 0.0192289862036705
	batch 150 loss: 0.019674831768497825
	batch 200 loss: 0.020357706174254416
LOSS [train: 0.020357706174254416] [valid: 0.01797743441250835] TIME [epoch: 246 sec]
EPOCH 18:
	batch 50 loss: 0.01863413560204208
	batch 100 loss: 0.01886138442903757
	batch 150 loss: 0.01943069606088102
	batch 200 loss: 0.0202651047706604
LOSS [train: 0.0202651047706604] [valid: 0.017015645980427506] TIME [epoch: 245 sec]
EPOCH 19:
	batch 50 loss: 0.01760153322480619
	batch 100 loss: 0.01941418316215277
	batch 150 loss: 0.02123702640645206
	batch 200 loss: 0.0186151484772563
LOSS [train: 0.0186151484772563] [valid: 0.017095804005415025] TIME [epoch: 243 sec]
EPOCH 20:
	batch 50 loss: 0.02158980255946517
	batch 100 loss: 0.01841226522810757
	batch 150 loss: 0.018892801627516748
	batch 200 loss: 0.0183698164857924
LOSS [train: 0.0183698164857924] [valid: 0.017134791952897407] TIME [epoch: 244 sec]
EPOCH 21:
	batch 50 loss: 0.018443453470245005
	batch 100 loss: 0.019931682059541343
	batch 150 loss: 0.020948709193617104
	batch 200 loss: 0.0191927935089916
LOSS [train: 0.0191927935089916] [valid: 0.017227826367404] TIME [epoch: 244 sec]
EPOCH 22:
	batch 50 loss: 0.019877064656466246
	batch 100 loss: 0.017942170957103373
	batch 150 loss: 0.019658369859680534
	batch 200 loss: 0.019839764423668384
LOSS [train: 0.019839764423668384] [valid: 0.016835374411796996] TIME [epoch: 244 sec]
Saving model.
EPOCH 23:
	batch 50 loss: 0.01899129377678037
	batch 100 loss: 0.019171588411554693
	batch 150 loss: 0.01953137757256627
	batch 200 loss: 0.020918035125359894
LOSS [train: 0.020918035125359894] [valid: 0.016636763542192057] TIME [epoch: 247 sec]
Saving model.
EPOCH 24:
	batch 50 loss: 0.018872570432722568
	batch 100 loss: 0.01977780748158693
	batch 150 loss: 0.020054793702438473
	batch 200 loss: 0.0177504775300622
LOSS [train: 0.0177504775300622] [valid: 0.01742728117533261] TIME [epoch: 246 sec]
EPOCH 25:
	batch 50 loss: 0.017476537926122547
	batch 100 loss: 0.01986518882215023
	batch 150 loss: 0.01897852811962366
	batch 200 loss: 0.023266486525535583
LOSS [train: 0.023266486525535583] [valid: 0.01724648071588793] TIME [epoch: 245 sec]
EPOCH 26:
	batch 50 loss: 0.021303390953689813
	batch 100 loss: 0.018085658755153416
	batch 150 loss: 0.020811113864183425
	batch 200 loss: 0.017999939769506455
LOSS [train: 0.017999939769506455] [valid: 0.016749465874939536] TIME [epoch: 245 sec]
EPOCH 27:
	batch 50 loss: 0.020266033466905354
	batch 100 loss: 0.019310695258900522
	batch 150 loss: 0.018420348595827817
	batch 200 loss: 0.018121137265115977
LOSS [train: 0.018121137265115977] [valid: 0.016183463930307576] TIME [epoch: 246 sec]
Saving model.
EPOCH 28:
	batch 50 loss: 0.018315164502710105
	batch 100 loss: 0.022072091633453965
	batch 150 loss: 0.018457794478163123
	batch 200 loss: 0.01881458727642894
LOSS [train: 0.01881458727642894] [valid: 0.01726725568102362] TIME [epoch: 246 sec]
EPOCH 29:
	batch 50 loss: 0.019337754268199205
	batch 100 loss: 0.020451405849307776
	batch 150 loss: 0.019124793522059917
	batch 200 loss: 0.018021940160542725
LOSS [train: 0.018021940160542725] [valid: 0.016319900573095462] TIME [epoch: 245 sec]
EPOCH 30:
	batch 50 loss: 0.020556124048307537
	batch 100 loss: 0.018899178449064493
	batch 150 loss: 0.018539329254999758
	batch 200 loss: 0.019196153013035656
LOSS [train: 0.019196153013035656] [valid: 0.016692775983392492] TIME [epoch: 248 sec]
EPOCH 31:
	batch 50 loss: 0.020822709426283836
	batch 100 loss: 0.017706267200410367
	batch 150 loss: 0.01863635314628482
	batch 200 loss: 0.01756573863327503
LOSS [train: 0.01756573863327503] [valid: 0.016633645847226337] TIME [epoch: 246 sec]
EPOCH 32:
	batch 50 loss: 0.019468704946339132
	batch 100 loss: 0.0207377190515399
	batch 150 loss: 0.018643007604405282
	batch 200 loss: 0.016657238574698568
LOSS [train: 0.016657238574698568] [valid: 0.016428691777885737] TIME [epoch: 245 sec]
EPOCH 33:
	batch 50 loss: 0.019535223906859755
	batch 100 loss: 0.02071686340495944
	batch 150 loss: 0.017773482464253903
	batch 200 loss: 0.018906690757721663
LOSS [train: 0.018906690757721663] [valid: 0.01694537492876407] TIME [epoch: 244 sec]
EPOCH 34:
	batch 50 loss: 0.018936219550669195
	batch 100 loss: 0.017694211937487124
	batch 150 loss: 0.01884160118177533
	batch 200 loss: 0.020605448745191098
LOSS [train: 0.020605448745191098] [valid: 0.016648017341261343] TIME [epoch: 244 sec]
EPOCH 35:
	batch 50 loss: 0.017160779796540737
	batch 100 loss: 0.018267381591722368
	batch 150 loss: 0.021320306099951267
	batch 200 loss: 0.019596305657178164
LOSS [train: 0.019596305657178164] [valid: 0.016067099121088783] TIME [epoch: 244 sec]
Saving model.
EPOCH 36:
	batch 50 loss: 0.018145197834819556
	batch 100 loss: 0.018604177217930555
	batch 150 loss: 0.018029398461803795
	batch 200 loss: 0.019284592801705004
LOSS [train: 0.019284592801705004] [valid: 0.01668563337176844] TIME [epoch: 250 sec]
EPOCH 37:
	batch 50 loss: 0.020647034235298633
	batch 100 loss: 0.017166479639708996
	batch 150 loss: 0.020565256047993898
	batch 200 loss: 0.017218786031007766
LOSS [train: 0.017218786031007766] [valid: 0.016691850631226166] TIME [epoch: 254 sec]
EPOCH 38:
	batch 50 loss: 0.017683597607538105
	batch 100 loss: 0.01964256638661027
	batch 150 loss: 0.020110910329967736
	batch 200 loss: 0.01788337130099535
LOSS [train: 0.01788337130099535] [valid: 0.0164356131691117] TIME [epoch: 254 sec]
EPOCH 39:
	batch 50 loss: 0.018853769432753326
	batch 100 loss: 0.017863552682101727
	batch 150 loss: 0.019769600834697486
	batch 200 loss: 0.016815933268517255
LOSS [train: 0.016815933268517255] [valid: 0.015760368811606897] TIME [epoch: 254 sec]
Saving model.
EPOCH 40:
	batch 50 loss: 0.020831551207229494
	batch 100 loss: 0.017181419217959047
	batch 150 loss: 0.018613441120833157
	batch 200 loss: 0.018683505635708572
LOSS [train: 0.018683505635708572] [valid: 0.01641794889825784] TIME [epoch: 254 sec]
EPOCH 41:
	batch 50 loss: 0.018538814410567282
	batch 100 loss: 0.017399262208491563
	batch 150 loss: 0.020270082391798495
	batch 200 loss: 0.018279188238084318
LOSS [train: 0.018279188238084318] [valid: 0.016055106666559973] TIME [epoch: 254 sec]
EPOCH 42:
	batch 50 loss: 0.016888093212619425
	batch 100 loss: 0.018431739257648586
	batch 150 loss: 0.02028248742222786
	batch 200 loss: 0.01807405037805438
LOSS [train: 0.01807405037805438] [valid: 0.015957536616770084] TIME [epoch: 253 sec]
EPOCH 43:
	batch 50 loss: 0.019636877272278072
	batch 100 loss: 0.01843171400949359
	batch 150 loss: 0.016453419495373964
	batch 200 loss: 0.0193855993822217
LOSS [train: 0.0193855993822217] [valid: 0.0160785033061984] TIME [epoch: 253 sec]
EPOCH 44:
	batch 50 loss: 0.019582628132775427
	batch 100 loss: 0.017893381379544736
	batch 150 loss: 0.018068529702723025
	batch 200 loss: 0.017915627770125866
LOSS [train: 0.017915627770125866] [valid: 0.015816424510800668] TIME [epoch: 254 sec]
EPOCH 45:
	batch 50 loss: 0.017737399023026228
	batch 100 loss: 0.018365578232333063
	batch 150 loss: 0.017817735616117717
	batch 200 loss: 0.019965539053082466
LOSS [train: 0.019965539053082466] [valid: 0.01605763232073514] TIME [epoch: 254 sec]
EPOCH 46:
	batch 50 loss: 0.017651521842926742
	batch 100 loss: 0.01832993235439062
	batch 150 loss: 0.018591886200010777
	batch 200 loss: 0.018670102413743735
LOSS [train: 0.018670102413743735] [valid: 0.016024615378895154] TIME [epoch: 254 sec]
EPOCH 47:
	batch 50 loss: 0.016922622453421354
	batch 100 loss: 0.0190607506968081
	batch 150 loss: 0.020229852041229606
	batch 200 loss: 0.016920788241550327
LOSS [train: 0.016920788241550327] [valid: 0.016141958500035494] TIME [epoch: 254 sec]
EPOCH 48:
	batch 50 loss: 0.018619650732725857
	batch 100 loss: 0.01890662593767047
	batch 150 loss: 0.019211877305060625
	batch 200 loss: 0.0176306539028883
LOSS [train: 0.0176306539028883] [valid: 0.0162745093824924] TIME [epoch: 254 sec]
EPOCH 49:
	batch 50 loss: 0.01947180699557066
	batch 100 loss: 0.019632552359253166
	batch 150 loss: 0.01696526588872075
	batch 200 loss: 0.016828673360869288
LOSS [train: 0.016828673360869288] [valid: 0.015804647621310626] TIME [epoch: 254 sec]
EPOCH 50:
	batch 50 loss: 0.018355645518749953
	batch 100 loss: 0.018564811367541553
	batch 150 loss: 0.019065333232283592
	batch 200 loss: 0.01757600758224726
LOSS [train: 0.01757600758224726] [valid: 0.01565042939352376] TIME [epoch: 254 sec]
Saving model.
Finished training in 12823.081 seconds.
