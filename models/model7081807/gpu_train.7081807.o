Slurm job ID: 7081807
args: Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='softplus', hidden_acts=['softplus', 'tanh', 'softplus', 'tanh', 'softplus'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.01, loss='kl', momentum=0.9, name='model7081807', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='adam', outdir='out/model_training/model7081807', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2', weight_decay=0.0001)
Using device: cuda
Using seed: 2583360372
EPOCH 1:
	batch 50 loss: 5.5315784740447995
	batch 100 loss: 2.6515996837615967
	batch 150 loss: 0.13455786771606654
	batch 200 loss: 0.09361002651858144
LOSS [train: 0.09361002651858144] [valid: 0.06620666247132855] TIME [epoch: 320 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.07636812867829576
	batch 100 loss: 0.12894527961267158
	batch 150 loss: 0.0871398030349519
	batch 200 loss: 0.07815561467112275
LOSS [train: 0.07815561467112275] [valid: 0.06593213812836135] TIME [epoch: 325 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.07071902829688043
	batch 100 loss: 0.07424014236079529
	batch 150 loss: 0.052133288327604534
	batch 200 loss: 0.05760029496508651
LOSS [train: 0.05760029496508651] [valid: 0.06357808148216766] TIME [epoch: 324 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.06478016113047488
	batch 100 loss: 0.06712329295231029
	batch 150 loss: 0.05917012638412416
	batch 200 loss: 0.05743382132146507
LOSS [train: 0.05743382132146507] [valid: 0.04862262745543073] TIME [epoch: 326 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.07819338390952907
	batch 100 loss: 0.05521979156299494
	batch 150 loss: 0.05416133732185699
	batch 200 loss: 0.058659197946544735
LOSS [train: 0.058659197946544735] [valid: 0.07125280355103314] TIME [epoch: 325 sec]
EPOCH 6:
	batch 50 loss: 0.07892437166068703
	batch 100 loss: 0.06192551289685071
	batch 150 loss: 0.06970111080911011
	batch 200 loss: 0.04320862136315554
LOSS [train: 0.04320862136315554] [valid: 0.04576351675592984] TIME [epoch: 325 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.055093915816978555
	batch 100 loss: 0.053934408957138656
	batch 150 loss: 0.06739772045286373
	batch 200 loss: 0.0632573541183956
LOSS [train: 0.0632573541183956] [valid: 0.0525811092108294] TIME [epoch: 327 sec]
EPOCH 8:
	batch 50 loss: 0.06125871503609233
	batch 100 loss: 0.03934124137274921
	batch 150 loss: 0.07402108798269183
	batch 200 loss: 0.059350621780322396
LOSS [train: 0.059350621780322396] [valid: 0.07308800668203427] TIME [epoch: 327 sec]
EPOCH 9:
	batch 50 loss: 0.08148976090131327
	batch 100 loss: 0.065957473651506
	batch 150 loss: 0.05844739415450022
	batch 200 loss: 0.03852944443598972
LOSS [train: 0.03852944443598972] [valid: 0.04908269303183867] TIME [epoch: 326 sec]
EPOCH 10:
	batch 50 loss: 0.050268044641707095
	batch 100 loss: 0.06935298180556856
	batch 150 loss: 0.05863216682570055
	batch 200 loss: 0.060875045955181124
LOSS [train: 0.060875045955181124] [valid: 0.08074167834953792] TIME [epoch: 327 sec]
EPOCH 11:
	batch 50 loss: 0.06280658655799926
	batch 100 loss: 0.04434516477980651
	batch 150 loss: 0.06280959503492341
	batch 200 loss: 0.08015920381993055
LOSS [train: 0.08015920381993055] [valid: 0.062082345669235414] TIME [epoch: 325 sec]
EPOCH 12:
	batch 50 loss: 0.052954326393082736
	batch 100 loss: 0.06147687425138429
	batch 150 loss: 0.0908521935925819
	batch 200 loss: 0.07314446350326761
LOSS [train: 0.07314446350326761] [valid: 0.07678539488697425] TIME [epoch: 327 sec]
EPOCH 13:
	batch 50 loss: 0.05154399011807982
	batch 100 loss: 0.05963781173340976
	batch 150 loss: 0.052415313832461834
	batch 200 loss: 0.08973047496459913
LOSS [train: 0.08973047496459913] [valid: 0.06026474150130525] TIME [epoch: 327 sec]
EPOCH 14:
	batch 50 loss: 0.04665368357382249
	batch 100 loss: 0.10203158209566027
	batch 150 loss: 0.0703462425368707
	batch 200 loss: 0.059443921350502936
LOSS [train: 0.059443921350502936] [valid: 0.05532382645033067] TIME [epoch: 327 sec]
EPOCH 15:
	batch 50 loss: 0.06445576106954831
	batch 100 loss: 0.07131729809043463
	batch 150 loss: 0.047215510220266876
	batch 200 loss: 0.05645983508322388
LOSS [train: 0.05645983508322388] [valid: 0.06369612498892821] TIME [epoch: 328 sec]
EPOCH 16:
	batch 50 loss: 0.05691907180240378
	batch 100 loss: 0.050633091509807854
	batch 150 loss: 0.0783598053501919
	batch 200 loss: 0.06352021121885627
LOSS [train: 0.06352021121885627] [valid: 0.07653927040907244] TIME [epoch: 328 sec]
EPOCH 17:
	batch 50 loss: 0.06928681381046772
	batch 100 loss: 0.05921394527191296
	batch 150 loss: 0.06008311387791764
	batch 200 loss: 0.0656176399521064
LOSS [train: 0.0656176399521064] [valid: 0.0848593931509337] TIME [epoch: 327 sec]
EPOCH 18:
	batch 50 loss: 0.0787685537757352
	batch 100 loss: 0.05226869458740111
	batch 150 loss: 0.05059877958963625
	batch 200 loss: 0.06491024494520388
LOSS [train: 0.06491024494520388] [valid: 0.059153836746312055] TIME [epoch: 328 sec]
EPOCH 19:
	batch 50 loss: 0.08464648503810167
	batch 100 loss: 0.037164925852557644
	batch 150 loss: 0.08689730347134172
	batch 200 loss: 0.046718125597108154
LOSS [train: 0.046718125597108154] [valid: 0.05790171693758263] TIME [epoch: 328 sec]
EPOCH 20:
	batch 50 loss: 0.07062170489865821
	batch 100 loss: 0.045303946319036184
	batch 150 loss: 0.04242581504629925
	batch 200 loss: 0.06696313199354335
LOSS [train: 0.06696313199354335] [valid: 0.059490619861753656] TIME [epoch: 328 sec]
EPOCH 21:
	batch 50 loss: 0.04818702709511854
	batch 100 loss: 0.05563559092930518
	batch 150 loss: 0.0662402450083755
	batch 200 loss: 0.06194854594534263
LOSS [train: 0.06194854594534263] [valid: 0.055268212720208494] TIME [epoch: 328 sec]
EPOCH 22:
	batch 50 loss: 0.05581538045080379
	batch 100 loss: 0.048241230812855064
	batch 150 loss: 0.08647708055388648
	batch 200 loss: 0.05582226042170078
LOSS [train: 0.05582226042170078] [valid: 0.06347173321070537] TIME [epoch: 328 sec]
EPOCH 23:
	batch 50 loss: 0.06034034181619063
	batch 100 loss: 0.06757536789984442
	batch 150 loss: 0.06112208276521414
	batch 200 loss: 0.062836992208031
LOSS [train: 0.062836992208031] [valid: 0.05762164189499875] TIME [epoch: 329 sec]
EPOCH 24:
	batch 50 loss: 0.07077028769592289
	batch 100 loss: 0.06102316169650294
	batch 150 loss: 0.05543769676005468
	batch 200 loss: 0.06200888202060014
LOSS [train: 0.06200888202060014] [valid: 0.06630004297864313] TIME [epoch: 328 sec]
EPOCH 25:
	batch 50 loss: 0.07322645282838494
	batch 100 loss: 0.05569233848247677
	batch 150 loss: 0.04432275814455352
	batch 200 loss: 0.05977702234493336
LOSS [train: 0.05977702234493336] [valid: 0.05464419271350683] TIME [epoch: 327 sec]
EPOCH 26:
	batch 50 loss: 0.051647552840877325
	batch 100 loss: 0.07219413047190755
	batch 150 loss: 0.0559895888029132
	batch 200 loss: 0.06679437657585367
LOSS [train: 0.06679437657585367] [valid: 0.05746057953801938] TIME [epoch: 329 sec]
EPOCH 27:
	batch 50 loss: 0.06281077969702892
	batch 100 loss: 0.0642019869061187
	batch 150 loss: 0.05591304984176532
	batch 200 loss: 0.07103216850664466
LOSS [train: 0.07103216850664466] [valid: 0.0883396174487037] TIME [epoch: 327 sec]
EPOCH 28:
	batch 50 loss: 0.04969779691673466
	batch 100 loss: 0.0761417052289471
	batch 150 loss: 0.057515183865325524
	batch 200 loss: 0.04756289273034781
LOSS [train: 0.04756289273034781] [valid: 0.053047780510193356] TIME [epoch: 328 sec]
EPOCH 29:
	batch 50 loss: 0.0429144225968048
	batch 100 loss: 0.062038680664263664
	batch 150 loss: 0.06500727410544642
	batch 200 loss: 0.06823750918847508
LOSS [train: 0.06823750918847508] [valid: 0.08093881150998641] TIME [epoch: 327 sec]
EPOCH 30:
	batch 50 loss: 0.07369138545589521
	batch 100 loss: 0.05351258706359659
	batch 150 loss: 0.05796304840943776
	batch 200 loss: 0.0564964957034681
LOSS [train: 0.0564964957034681] [valid: 0.051592163313519755] TIME [epoch: 329 sec]
EPOCH 31:
	batch 50 loss: 0.06477761648595333
	batch 100 loss: 0.06854451766237617
	batch 150 loss: 0.053672245901543646
	batch 200 loss: 0.05557752545690164
LOSS [train: 0.05557752545690164] [valid: 0.060389804954562955] TIME [epoch: 327 sec]
EPOCH 32:
	batch 50 loss: 0.0637616019276902
	batch 100 loss: 0.05752434407360852
	batch 150 loss: 0.05868181929457933
	batch 200 loss: 0.07527905507478863
LOSS [train: 0.07527905507478863] [valid: 0.06597181984689086] TIME [epoch: 327 sec]
EPOCH 33:
	batch 50 loss: 0.07558739581145346
	batch 100 loss: 0.042236387333832684
	batch 150 loss: 0.05942714241798967
	batch 200 loss: 0.07692849217215553
LOSS [train: 0.07692849217215553] [valid: 0.06549486120378181] TIME [epoch: 326 sec]
EPOCH 34:
	batch 50 loss: 0.07671733235009015
	batch 100 loss: 0.06545464821858332
	batch 150 loss: 0.06318218825326767
	batch 200 loss: 0.060524539346806704
LOSS [train: 0.060524539346806704] [valid: 0.06293143139434203] TIME [epoch: 319 sec]
EPOCH 35:
	batch 50 loss: 0.06232620793860406
	batch 100 loss: 0.061364069858100265
	batch 150 loss: 0.06187873085960746
	batch 200 loss: 0.059607015474466604
LOSS [train: 0.059607015474466604] [valid: 0.057419440902594945] TIME [epoch: 296 sec]
EPOCH 36:
	batch 50 loss: 0.05824320411775261
	batch 100 loss: 0.07048599332454614
	batch 150 loss: 0.0579894956300268
	batch 200 loss: 0.06387494062306359
LOSS [train: 0.06387494062306359] [valid: 0.059560603185673244] TIME [epoch: 295 sec]
EPOCH 37:
	batch 50 loss: 0.06144490610284265
	batch 100 loss: 0.06609829718945548
	batch 150 loss: 0.07375152543187141
	batch 200 loss: 0.06170490828342736
LOSS [train: 0.06170490828342736] [valid: 0.05473251188086579] TIME [epoch: 294 sec]
EPOCH 38:
	batch 50 loss: 0.0630599312193226
	batch 100 loss: 0.07070505973068066
	batch 150 loss: 0.05505610677297227
	batch 200 loss: 0.059511301047168674
LOSS [train: 0.059511301047168674] [valid: 0.052413335541496055] TIME [epoch: 295 sec]
EPOCH 39:
	batch 50 loss: 0.06261319725075737
	batch 100 loss: 0.06316857947036623
	batch 150 loss: 0.06947496314882301
	batch 200 loss: 0.054662266750819984
LOSS [train: 0.054662266750819984] [valid: 0.053090652851066504] TIME [epoch: 295 sec]
EPOCH 40:
	batch 50 loss: 0.07028574536321684
	batch 100 loss: 0.0555029206443578
	batch 150 loss: 0.04585350238950923
	batch 200 loss: 0.07611717055086047
LOSS [train: 0.07611717055086047] [valid: 0.05045844973186225] TIME [epoch: 298 sec]
EPOCH 41:
	batch 50 loss: 0.04625542101683095
	batch 100 loss: 0.06381188474595546
	batch 150 loss: 0.06728995471901726
	batch 200 loss: 0.06263974436325953
LOSS [train: 0.06263974436325953] [valid: 0.05802064694677635] TIME [epoch: 295 sec]
EPOCH 42:
	batch 50 loss: 0.06065517859882675
	batch 100 loss: 0.06062529288232327
	batch 150 loss: 0.0884061386493886
	batch 200 loss: 0.05212373140733689
LOSS [train: 0.05212373140733689] [valid: 0.05541165669177038] TIME [epoch: 295 sec]
EPOCH 43:
	batch 50 loss: 0.06915825945208781
	batch 100 loss: 0.0654150172431946
	batch 150 loss: 0.052481144508492436
	batch 200 loss: 0.08562347166560358
LOSS [train: 0.08562347166560358] [valid: 0.16682175346262132] TIME [epoch: 294 sec]
EPOCH 44:
	batch 50 loss: 0.09326233966938162
	batch 100 loss: 0.06365504950168542
	batch 150 loss: 0.05161970236164052
	batch 200 loss: 0.0775195658278244
LOSS [train: 0.0775195658278244] [valid: 0.05363377760634951] TIME [epoch: 295 sec]
EPOCH 45:
	batch 50 loss: 0.07730946639552712
	batch 100 loss: 0.06385712856717873
	batch 150 loss: 0.0487890800461173
	batch 200 loss: 0.05969449912547134
LOSS [train: 0.05969449912547134] [valid: 0.056455844749386114] TIME [epoch: 295 sec]
EPOCH 46:
	batch 50 loss: 0.06436226362129674
	batch 100 loss: 0.05801307693356648
	batch 150 loss: 0.06023071655479725
	batch 200 loss: 0.06296456754673273
LOSS [train: 0.06296456754673273] [valid: 0.05182435445409889] TIME [epoch: 296 sec]
EPOCH 47:
	batch 50 loss: 0.06061955313431099
	batch 100 loss: 0.07521271343925036
	batch 150 loss: 0.042504407127853486
	batch 200 loss: 0.06258479750715196
LOSS [train: 0.06258479750715196] [valid: 0.05758693121169926] TIME [epoch: 296 sec]
EPOCH 48:
	batch 50 loss: 0.07888370925327763
	batch 100 loss: 0.06730967546580359
	batch 150 loss: 0.06704562379629352
	batch 200 loss: 0.043197696849238125
LOSS [train: 0.043197696849238125] [valid: 0.06621844468948741] TIME [epoch: 295 sec]
EPOCH 49:
	batch 50 loss: 0.06352664421778172
	batch 100 loss: 0.05843154714908451
	batch 150 loss: 0.06529741902457317
	batch 200 loss: 0.06405251488322392
LOSS [train: 0.06405251488322392] [valid: 0.05323559751511008] TIME [epoch: 295 sec]
EPOCH 50:
	batch 50 loss: 0.06116727072745562
	batch 100 loss: 0.05570043840445578
	batch 150 loss: 0.05821435933467001
	batch 200 loss: 0.07225757942826022
LOSS [train: 0.07225757942826022] [valid: 0.07984915886554518] TIME [epoch: 303 sec]
Finished training in 15950.714 seconds.
