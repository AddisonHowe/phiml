Slurm job ID: 7296801
args: Namespace(batch_size=25, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['softplus'], hidden_dims=[16, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model7296801', ncells=100, ndims=2, nsigs=2, nsims_training=1000, nsims_validation=200, num_epochs=50, optimizer='rms', outdir='out/model_training/model7296801', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_3', use_gpu=False, validation_data='data/model_validation_data_3', weight_decay=0.0)
Using device: cpu
Using seed: 1293866049
EPOCH 1:
	batch 25 loss: 2.4135864794254305
	batch 50 loss: 0.3801250064373016
	batch 75 loss: 0.24600803405046462
	batch 100 loss: 0.2363591343164444
	batch 125 loss: 0.1975199458003044
	batch 150 loss: 0.22378966480493545
	batch 175 loss: 0.2491539376974106
	batch 200 loss: 0.21041445910930634
LOSS [train: 0.21041445910930634] [valid: 0.22554137334227561] TIME [epoch: 1.54e+03 sec]
Saving model.
EPOCH 2:
	batch 25 loss: 0.20937247961759567
	batch 50 loss: 0.2471862840652466
	batch 75 loss: 0.2095302850008011
	batch 100 loss: 0.2219827526807785
	batch 125 loss: 0.19414810717105865
	batch 150 loss: 0.22512001395225525
	batch 175 loss: 0.21109866887331008
	batch 200 loss: 0.21497864484786988
LOSS [train: 0.21497864484786988] [valid: 0.2153824668377638] TIME [epoch: 1.53e+03 sec]
Saving model.
EPOCH 3:
	batch 25 loss: 0.18999024271965026
	batch 50 loss: 0.21868956238031387
	batch 75 loss: 0.2093629163503647
	batch 100 loss: 0.21332602202892303
	batch 125 loss: 0.22081508100032807
	batch 150 loss: 0.20213607907295228
	batch 175 loss: 0.14075080350041388
	batch 200 loss: 0.09311404563486576
LOSS [train: 0.09311404563486576] [valid: 0.05306146671064198] TIME [epoch: 1.53e+03 sec]
Saving model.
EPOCH 4:
	batch 25 loss: 0.037921536043286325
	batch 50 loss: 0.020000179465860128
	batch 75 loss: 0.03292276592925191
	batch 100 loss: 0.030408898070454597
	batch 125 loss: 0.024303811714053155
	batch 150 loss: 0.009678802229464054
	batch 175 loss: 0.015900976043194532
	batch 200 loss: 0.011557981483638286
LOSS [train: 0.011557981483638286] [valid: 0.01460741157643497] TIME [epoch: 1.52e+03 sec]
Saving model.
EPOCH 5:
	batch 25 loss: 0.010516798058524728
	batch 50 loss: 0.009507418014109134
	batch 75 loss: 0.007206508656963706
	batch 100 loss: 0.013651821613311767
	batch 125 loss: 0.013623448926955462
	batch 150 loss: 0.01091797050088644
	batch 175 loss: 0.010363933658227324
	batch 200 loss: 0.008960237009450794
LOSS [train: 0.008960237009450794] [valid: 0.008621087146457285] TIME [epoch: 1.52e+03 sec]
Saving model.
EPOCH 6:
	batch 25 loss: 0.008963359836488962
	batch 50 loss: 0.013143026530742645
	batch 75 loss: 0.0075279246363788846
	batch 100 loss: 0.009811886176466942
	batch 125 loss: 0.007643595300614834
	batch 150 loss: 0.009929699320346118
	batch 175 loss: 0.010715912794694304
	batch 200 loss: 0.0073102633655071254
LOSS [train: 0.0073102633655071254] [valid: 0.007691269513452425] TIME [epoch: 1.51e+03 sec]
Saving model.
EPOCH 7:
	batch 25 loss: 0.011794842248782515
	batch 50 loss: 0.014934487426653504
	batch 75 loss: 0.008602484092116357
	batch 100 loss: 0.007327155321836472
	batch 125 loss: 0.011990426536649466
	batch 150 loss: 0.010068948939442634
	batch 175 loss: 0.011877232752740384
	batch 200 loss: 0.008501522913575173
LOSS [train: 0.008501522913575173] [valid: 0.006050363299436867] TIME [epoch: 1.52e+03 sec]
Saving model.
EPOCH 8:
	batch 25 loss: 0.010675930362194776
	batch 50 loss: 0.010290018990635872
	batch 75 loss: 0.01000331250950694
	batch 100 loss: 0.007473183264955878
	batch 125 loss: 0.01084579411894083
	batch 150 loss: 0.008100592130795121
	batch 175 loss: 0.008049865001812578
	batch 200 loss: 0.007824138905853033
LOSS [train: 0.007824138905853033] [valid: 0.006985334402997978] TIME [epoch: 1.51e+03 sec]
EPOCH 9:
	batch 25 loss: 0.00689894862473011
	batch 50 loss: 0.006163676762953401
	batch 75 loss: 0.007076131133362651
	batch 100 loss: 0.0065859220735728745
	batch 125 loss: 0.006412956952117383
	batch 150 loss: 0.010631522871553898
	batch 175 loss: 0.008778237737715244
	batch 200 loss: 0.006714049279689788
LOSS [train: 0.006714049279689788] [valid: 0.00855329116457142] TIME [epoch: 1.51e+03 sec]
EPOCH 10:
	batch 25 loss: 0.007564826300367713
	batch 50 loss: 0.006161209503188729
	batch 75 loss: 0.006715421350672841
	batch 100 loss: 0.007225273484364152
	batch 125 loss: 0.008899428467266261
	batch 150 loss: 0.00751808425411582
	batch 175 loss: 0.009883102439343929
	batch 200 loss: 0.0073909343499690295
LOSS [train: 0.0073909343499690295] [valid: 0.006266257941024378] TIME [epoch: 1.46e+03 sec]
EPOCH 11:
	batch 25 loss: 0.006197941480204463
	batch 50 loss: 0.009413152933120728
	batch 75 loss: 0.009469343461096286
	batch 100 loss: 0.007610113173723221
	batch 125 loss: 0.007716079503297806
	batch 150 loss: 0.008261482021771372
	batch 175 loss: 0.006288847438991069
	batch 200 loss: 0.006535873431712389
LOSS [train: 0.006535873431712389] [valid: 0.006802467495435849] TIME [epoch: 1.42e+03 sec]
EPOCH 12:
	batch 25 loss: 0.0074023445136845115
	batch 50 loss: 0.006816438660025597
	batch 75 loss: 0.005900865895673632
	batch 100 loss: 0.006847367398440838
	batch 125 loss: 0.007097896630875766
	batch 150 loss: 0.007652456117793918
	batch 175 loss: 0.006748229637742043
	batch 200 loss: 0.007931416910141707
LOSS [train: 0.007931416910141707] [valid: 0.007993520301533863] TIME [epoch: 1.41e+03 sec]
EPOCH 13:
	batch 25 loss: 0.0076337525341659785
	batch 50 loss: 0.01039902163669467
	batch 75 loss: 0.00668971573933959
	batch 100 loss: 0.006523791495710612
	batch 125 loss: 0.007784930979833007
	batch 150 loss: 0.005975382006727159
	batch 175 loss: 0.006529953521676362
	batch 200 loss: 0.007406085329130292
LOSS [train: 0.007406085329130292] [valid: 0.005683359509566799] TIME [epoch: 1.41e+03 sec]
Saving model.
EPOCH 14:
	batch 25 loss: 0.0061870266869664195
	batch 50 loss: 0.0061448962520807985
	batch 75 loss: 0.007009480278939008
	batch 100 loss: 0.007995629413053394
	batch 125 loss: 0.005862852828577161
	batch 150 loss: 0.0054902805155143146
	batch 175 loss: 0.006463450351729989
	batch 200 loss: 0.007410012790933252
LOSS [train: 0.007410012790933252] [valid: 0.008570734184468165] TIME [epoch: 1.41e+03 sec]
EPOCH 15:
	batch 25 loss: 0.006196259446442127
	batch 50 loss: 0.007157269865274429
	batch 75 loss: 0.005560923162847758
	batch 100 loss: 0.009258487289771437
	batch 125 loss: 0.0065171039383858445
	batch 150 loss: 0.005990130659192801
	batch 175 loss: 0.007311525577679276
	batch 200 loss: 0.007351157339289784
LOSS [train: 0.007351157339289784] [valid: 0.007376722357003018] TIME [epoch: 1.4e+03 sec]
EPOCH 16:
	batch 25 loss: 0.006362409964203834
	batch 50 loss: 0.00619344124570489
	batch 75 loss: 0.006764691015705466
	batch 100 loss: 0.007338738581165671
	batch 125 loss: 0.006608178848400712
	batch 150 loss: 0.008168370015919208
	batch 175 loss: 0.006522644460201264
	batch 200 loss: 0.007254652851261198
LOSS [train: 0.007254652851261198] [valid: 0.009920038585551082] TIME [epoch: 1.41e+03 sec]
EPOCH 17:
	batch 25 loss: 0.006119653983041644
	batch 50 loss: 0.006323214708827436
	batch 75 loss: 0.007848972892388702
	batch 100 loss: 0.006313091954216361
	batch 125 loss: 0.00871460048481822
	batch 150 loss: 0.008858258984982968
	batch 175 loss: 0.00828975508455187
	batch 200 loss: 0.006010714750736952
LOSS [train: 0.006010714750736952] [valid: 0.00619004906329792] TIME [epoch: 1.41e+03 sec]
EPOCH 18:
	batch 25 loss: 0.0062702198233455415
	batch 50 loss: 0.006172760995104909
	batch 75 loss: 0.007271255785599351
	batch 100 loss: 0.005686784195713699
	batch 125 loss: 0.006985109951347113
	batch 150 loss: 0.00664922047406435
	batch 175 loss: 0.00863304958678782
	batch 200 loss: 0.006902738725766539
LOSS [train: 0.006902738725766539] [valid: 0.0054226961685344575] TIME [epoch: 1.31e+03 sec]
Saving model.
EPOCH 19:
	batch 25 loss: 0.006058899592608214
	batch 50 loss: 0.007150530805811286
	batch 75 loss: 0.007034707544371486
	batch 100 loss: 0.008169825337827206
	batch 125 loss: 0.006294161211699247
	batch 150 loss: 0.005953464116901159
	batch 175 loss: 0.0062372902873903515
	batch 200 loss: 0.006077593844383955
LOSS [train: 0.006077593844383955] [valid: 0.007020454719895497] TIME [epoch: 1.19e+03 sec]
EPOCH 20:
	batch 25 loss: 0.006634898306801915
	batch 50 loss: 0.00603100135922432
	batch 75 loss: 0.006262371279299259
	batch 100 loss: 0.007044836231507361
	batch 125 loss: 0.006024958249181509
	batch 150 loss: 0.0070921365823596715
	batch 175 loss: 0.007359126368537545
	batch 200 loss: 0.006447287937626243
LOSS [train: 0.006447287937626243] [valid: 0.007772396423388272] TIME [epoch: 1.19e+03 sec]
EPOCH 21:
	batch 25 loss: 0.006886023804545402
	batch 50 loss: 0.007278836467303335
	batch 75 loss: 0.006121734199114144
	batch 100 loss: 0.0062404224183410404
	batch 125 loss: 0.006617156988941133
	batch 150 loss: 0.006931415870785713
	batch 175 loss: 0.006999220848083496
	batch 200 loss: 0.006540770279243589
LOSS [train: 0.006540770279243589] [valid: 0.005806814707466401] TIME [epoch: 1.19e+03 sec]
EPOCH 22:
	batch 25 loss: 0.005815444900654256
	batch 50 loss: 0.006367691392078996
	batch 75 loss: 0.007799815172329545
	batch 100 loss: 0.006832364425063134
	batch 125 loss: 0.00797922994941473
	batch 150 loss: 0.007206787373870611
	batch 175 loss: 0.00603219386190176
	batch 200 loss: 0.007083479426801205
LOSS [train: 0.007083479426801205] [valid: 0.007183863883255981] TIME [epoch: 1.19e+03 sec]
EPOCH 23:
	batch 25 loss: 0.00523482115007937
	batch 50 loss: 0.006335875606164336
	batch 75 loss: 0.007503927228972316
	batch 100 loss: 0.005852811047807336
	batch 125 loss: 0.006834253454580903
	batch 150 loss: 0.007984575224108994
	batch 175 loss: 0.00615059950388968
	batch 200 loss: 0.006612460534088313
LOSS [train: 0.006612460534088313] [valid: 0.006139174074633047] TIME [epoch: 1.19e+03 sec]
EPOCH 24:
	batch 25 loss: 0.005070647969841957
	batch 50 loss: 0.0047013546014204625
	batch 75 loss: 0.005327971139922738
	batch 100 loss: 0.00750559956766665
	batch 125 loss: 0.008567811781540513
	batch 150 loss: 0.006409052507951856
	batch 175 loss: 0.0062162529025226836
	batch 200 loss: 0.006804793542250991
LOSS [train: 0.006804793542250991] [valid: 0.006267558294348418] TIME [epoch: 1.19e+03 sec]
EPOCH 25:
	batch 25 loss: 0.007306875120848417
	batch 50 loss: 0.00730940442532301
	batch 75 loss: 0.005670420094393194
	batch 100 loss: 0.006756128743290901
	batch 125 loss: 0.005019458825699985
	batch 150 loss: 0.007015426382422447
	batch 175 loss: 0.006129129780456424
	batch 200 loss: 0.004813823360018432
LOSS [train: 0.004813823360018432] [valid: 0.005490657308837399] TIME [epoch: 1.19e+03 sec]
EPOCH 26:
	batch 25 loss: 0.007327525825239718
	batch 50 loss: 0.006629524664022029
	batch 75 loss: 0.007877663727849721
	batch 100 loss: 0.00889736982062459
	batch 125 loss: 0.009822743805125355
	batch 150 loss: 0.006538365036249161
	batch 175 loss: 0.006153603945858776
	batch 200 loss: 0.006783582763746381
LOSS [train: 0.006783582763746381] [valid: 0.005706891373847611] TIME [epoch: 1.19e+03 sec]
EPOCH 27:
	batch 25 loss: 0.005646494096145034
	batch 50 loss: 0.006067501241341233
	batch 75 loss: 0.006398334875702858
	batch 100 loss: 0.007270128540694714
	batch 125 loss: 0.006811338434927166
	batch 150 loss: 0.004571470166556537
	batch 175 loss: 0.004938163906335831
	batch 200 loss: 0.006212781784124672
LOSS [train: 0.006212781784124672] [valid: 0.005527466975036077] TIME [epoch: 1.19e+03 sec]
EPOCH 28:
	batch 25 loss: 0.005809045173227787
	batch 50 loss: 0.005164620848372579
	batch 75 loss: 0.005582916606217622
	batch 100 loss: 0.005265511246398092
	batch 125 loss: 0.006443999214097857
	batch 150 loss: 0.00741178167052567
	batch 175 loss: 0.006942613217979669
	batch 200 loss: 0.0072256273124367
LOSS [train: 0.0072256273124367] [valid: 0.004770400389679708] TIME [epoch: 1.19e+03 sec]
Saving model.
EPOCH 29:
	batch 25 loss: 0.007053926689550281
	batch 50 loss: 0.007035639071837067
	batch 75 loss: 0.00627664924133569
	batch 100 loss: 0.005946487253531814
	batch 125 loss: 0.006618154440075159
	batch 150 loss: 0.006494163861498236
	batch 175 loss: 0.006929191322997213
	batch 200 loss: 0.006259052506648004
LOSS [train: 0.006259052506648004] [valid: 0.005286236124811694] TIME [epoch: 1.19e+03 sec]
EPOCH 30:
	batch 25 loss: 0.006848351303488016
	batch 50 loss: 0.006287609557621181
	batch 75 loss: 0.0072092959843575955
	batch 100 loss: 0.006087210727855563
	batch 125 loss: 0.0069595735380426045
	batch 150 loss: 0.005780276535078883
	batch 175 loss: 0.0079585384670645
	batch 200 loss: 0.0061948921158909796
LOSS [train: 0.0061948921158909796] [valid: 0.005953597679035738] TIME [epoch: 1.19e+03 sec]
EPOCH 31:
	batch 25 loss: 0.0054182139318436386
	batch 50 loss: 0.005311240619048477
	batch 75 loss: 0.005903557818382979
	batch 100 loss: 0.0069459282513707876
	batch 125 loss: 0.006229510391131043
	batch 150 loss: 0.004929648945108056
	batch 175 loss: 0.0064829470124095675
	batch 200 loss: 0.005794447241351008
LOSS [train: 0.005794447241351008] [valid: 0.005542923162283842] TIME [epoch: 1.2e+03 sec]
EPOCH 32:
	batch 25 loss: 0.005248314826749265
	batch 50 loss: 0.005205873809754849
	batch 75 loss: 0.0059824621211737395
	batch 100 loss: 0.0069117449410259725
	batch 125 loss: 0.006349398922175169
	batch 150 loss: 0.006716971658170224
	batch 175 loss: 0.006965272561646998
	batch 200 loss: 0.0074774052388966086
LOSS [train: 0.0074774052388966086] [valid: 0.006319025470293127] TIME [epoch: 1.21e+03 sec]
EPOCH 33:
	batch 25 loss: 0.006250148653052747
	batch 50 loss: 0.006519889254122973
	batch 75 loss: 0.005401709014549851
	batch 100 loss: 0.0052967275585979225
	batch 125 loss: 0.0071196179464459415
	batch 150 loss: 0.007486605364829302
	batch 175 loss: 0.005661637810990214
	batch 200 loss: 0.006776362834498287
LOSS [train: 0.006776362834498287] [valid: 0.005807475559413433] TIME [epoch: 1.21e+03 sec]
EPOCH 34:
	batch 25 loss: 0.00561589004471898
	batch 50 loss: 0.007033759334590286
	batch 75 loss: 0.00759163660928607
	batch 100 loss: 0.005610992107540369
	batch 125 loss: 0.0060251795314252374
	batch 150 loss: 0.006481121014803648
	batch 175 loss: 0.00555158237926662
	batch 200 loss: 0.005458035236224532
LOSS [train: 0.005458035236224532] [valid: 0.006901043199468404] TIME [epoch: 1.2e+03 sec]
EPOCH 35:
	batch 25 loss: 0.00761715360917151
	batch 50 loss: 0.00624591507948935
	batch 75 loss: 0.008037697188556195
	batch 100 loss: 0.006028436040505767
	batch 125 loss: 0.006470981249585748
	batch 150 loss: 0.0069450037088245155
	batch 175 loss: 0.0062040513008832935
	batch 200 loss: 0.006579801542684436
LOSS [train: 0.006579801542684436] [valid: 0.006400179825141095] TIME [epoch: 1.2e+03 sec]
EPOCH 36:
	batch 25 loss: 0.005821827063336968
	batch 50 loss: 0.0067560180556029085
	batch 75 loss: 0.008338068900629879
	batch 100 loss: 0.006454515727236867
	batch 125 loss: 0.0062494553253054615
	batch 150 loss: 0.005742816831916571
	batch 175 loss: 0.005423833122476935
	batch 200 loss: 0.005768273337744176
LOSS [train: 0.005768273337744176] [valid: 0.005769841725123115] TIME [epoch: 1.2e+03 sec]
EPOCH 37:
	batch 25 loss: 0.004821470035240054
	batch 50 loss: 0.005571665987372398
	batch 75 loss: 0.006765896687284112
	batch 100 loss: 0.005661319079808891
	batch 125 loss: 0.006376444343477488
	batch 150 loss: 0.006447932124137879
	batch 175 loss: 0.005332180056720972
	batch 200 loss: 0.0072424288932234045
LOSS [train: 0.0072424288932234045] [valid: 0.005068183911498636] TIME [epoch: 1.2e+03 sec]
EPOCH 38:
	batch 25 loss: 0.00645769452676177
	batch 50 loss: 0.005965964589267969
	batch 75 loss: 0.00691396052017808
	batch 100 loss: 0.005756990099325776
	batch 125 loss: 0.005809429688379168
	batch 150 loss: 0.006456241034902633
	batch 175 loss: 0.005350187234580517
	batch 200 loss: 0.0066174881160259245
LOSS [train: 0.0066174881160259245] [valid: 0.005898837064160034] TIME [epoch: 1.2e+03 sec]
EPOCH 39:
	batch 25 loss: 0.0061894900724291805
	batch 50 loss: 0.007638360215350986
	batch 75 loss: 0.006510037202388048
	batch 100 loss: 0.007038530176505447
	batch 125 loss: 0.007647923454642296
	batch 150 loss: 0.005944305155426263
	batch 175 loss: 0.00624037703499198
	batch 200 loss: 0.006603037980385125
LOSS [train: 0.006603037980385125] [valid: 0.006600208618328907] TIME [epoch: 1.2e+03 sec]
EPOCH 40:
	batch 25 loss: 0.006583938929252326
	batch 50 loss: 0.005469180857762695
	batch 75 loss: 0.0061442317115142945
	batch 100 loss: 0.00761565594933927
	batch 125 loss: 0.006246354910545051
	batch 150 loss: 0.0054458943475037816
	batch 175 loss: 0.004881255999207496
	batch 200 loss: 0.005430368147790432
LOSS [train: 0.005430368147790432] [valid: 0.0052849220781354235] TIME [epoch: 1.2e+03 sec]
EPOCH 41:
	batch 25 loss: 0.005835808347910643
	batch 50 loss: 0.007078895047307014
	batch 75 loss: 0.0065665853396058085
	batch 100 loss: 0.005695344167761505
	batch 125 loss: 0.007106628455221653
	batch 150 loss: 0.007261920031160116
	batch 175 loss: 0.005562430168502033
	batch 200 loss: 0.005543194161728025
LOSS [train: 0.005543194161728025] [valid: 0.004794326750561595] TIME [epoch: 1.2e+03 sec]
EPOCH 42:
	batch 25 loss: 0.006327657271176576
	batch 50 loss: 0.005158328982070089
	batch 75 loss: 0.007428788635879755
	batch 100 loss: 0.005955198188312352
	batch 125 loss: 0.005118803475052118
	batch 150 loss: 0.00538668789435178
	batch 175 loss: 0.00603357870131731
	batch 200 loss: 0.006896835556253791
LOSS [train: 0.006896835556253791] [valid: 0.0066636980802286415] TIME [epoch: 1.2e+03 sec]
EPOCH 43:
	batch 25 loss: 0.00637318043038249
	batch 50 loss: 0.006877681869082153
	batch 75 loss: 0.0056029596272856
	batch 100 loss: 0.006395365316420793
	batch 125 loss: 0.0068671214580535885
	batch 150 loss: 0.006145481150597334
	batch 175 loss: 0.006268535442650318
	batch 200 loss: 0.006113419672474265
LOSS [train: 0.006113419672474265] [valid: 0.006924842158332467] TIME [epoch: 1.2e+03 sec]
EPOCH 44:
	batch 25 loss: 0.005729934675619006
	batch 50 loss: 0.006477743445429951
	batch 75 loss: 0.007456638934090733
	batch 100 loss: 0.005445255697704852
	batch 125 loss: 0.004581964807584882
	batch 150 loss: 0.005786367896944284
	batch 175 loss: 0.00559053304605186
	batch 200 loss: 0.006409446783363819
LOSS [train: 0.006409446783363819] [valid: 0.006100974750006571] TIME [epoch: 1.2e+03 sec]
EPOCH 45:
	batch 25 loss: 0.006614014552906155
	batch 50 loss: 0.00674101036041975
	batch 75 loss: 0.0051175634609535335
	batch 100 loss: 0.005934079033322632
	batch 125 loss: 0.005307180713862181
	batch 150 loss: 0.005618114643730223
	batch 175 loss: 0.008060808544978499
	batch 200 loss: 0.005787071660161018
LOSS [train: 0.005787071660161018] [valid: 0.005771104226005264] TIME [epoch: 1.2e+03 sec]
EPOCH 46:
	batch 25 loss: 0.0067917334008961916
	batch 50 loss: 0.006187792895361781
	batch 75 loss: 0.006240159701555967
	batch 100 loss: 0.005144526921212673
	batch 125 loss: 0.004634897159412503
	batch 150 loss: 0.005207133893854916
	batch 175 loss: 0.007450695964507759
	batch 200 loss: 0.005963852247223258
LOSS [train: 0.005963852247223258] [valid: 0.005745475974981673] TIME [epoch: 1.2e+03 sec]
EPOCH 47:
	batch 25 loss: 0.006026000073179602
	batch 50 loss: 0.0060117283184081315
	batch 75 loss: 0.005872865281999111
	batch 100 loss: 0.00534299879334867
	batch 125 loss: 0.005424638330005109
	batch 150 loss: 0.005353185273706913
	batch 175 loss: 0.006152665549889207
	batch 200 loss: 0.007226810967549682
LOSS [train: 0.007226810967549682] [valid: 0.006035272200824693] TIME [epoch: 1.2e+03 sec]
EPOCH 48:
	batch 25 loss: 0.006616738438606262
	batch 50 loss: 0.005707965767942369
	batch 75 loss: 0.005902975853532552
	batch 100 loss: 0.007081488817930221
	batch 125 loss: 0.006406710799783468
	batch 150 loss: 0.008016427578404546
	batch 175 loss: 0.006398409912362695
	batch 200 loss: 0.0068105586338788274
LOSS [train: 0.0068105586338788274] [valid: 0.006750770751386881] TIME [epoch: 1.2e+03 sec]
EPOCH 49:
	batch 25 loss: 0.006806606585159898
	batch 50 loss: 0.00528479979839176
	batch 75 loss: 0.005679499083198607
	batch 100 loss: 0.00464537188410759
	batch 125 loss: 0.006379472995176911
	batch 150 loss: 0.007582670217379928
	batch 175 loss: 0.007285479046404361
	batch 200 loss: 0.005891634495928884
LOSS [train: 0.005891634495928884] [valid: 0.005247334719751961] TIME [epoch: 1.2e+03 sec]
EPOCH 50:
	batch 25 loss: 0.004759400025941432
	batch 50 loss: 0.006104037438053638
	batch 75 loss: 0.006556207807734609
	batch 100 loss: 0.005460139862261713
	batch 125 loss: 0.006434082211926579
	batch 150 loss: 0.006704314914532006
	batch 175 loss: 0.006041350606828928
	batch 200 loss: 0.00658903005067259
LOSS [train: 0.00658903005067259] [valid: 0.0057280723965959625] TIME [epoch: 1.2e+03 sec]
Finished training in 66335.698 seconds.
