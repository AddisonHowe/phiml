Slurm job ID: 6342713
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='softplus', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='mcd', momentum=0.9, name='model6342713', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model6342713', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 433789270
EPOCH 1:
	batch 50 loss: 0.392261911034584
	batch 100 loss: 0.6461331117153167
	batch 150 loss: 0.5337002721428871
	batch 200 loss: 0.5070616601407528
LOSS [train: 0.5070616601407528] [valid: 0.33434288871164125] TIME [epoch: 237 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.40456830605864524
	batch 100 loss: 0.44871493846178057
	batch 150 loss: 0.479777007997036
	batch 200 loss: 0.4669374853372574
LOSS [train: 0.4669374853372574] [valid: 0.34429450443324944] TIME [epoch: 240 sec]
EPOCH 3:
	batch 50 loss: 0.8578444541990757
	batch 100 loss: 1.575942051410675
	batch 150 loss: 1.3638842988014221
	batch 200 loss: 1.1644433784484862
LOSS [train: 1.1644433784484862] [valid: 1.3689084628596901] TIME [epoch: 239 sec]
EPOCH 4:
	batch 50 loss: 1.2346501088142394
	batch 100 loss: 1.0701322221755982
	batch 150 loss: 0.8940776669979096
	batch 200 loss: 0.8085889303684235
LOSS [train: 0.8085889303684235] [valid: 0.7984311106925209] TIME [epoch: 241 sec]
EPOCH 5:
	batch 50 loss: 0.7518509930372238
	batch 100 loss: 0.676926982998848
	batch 150 loss: 0.6595651406049728
	batch 200 loss: 0.585232464671135
LOSS [train: 0.585232464671135] [valid: 0.5899141209200025] TIME [epoch: 241 sec]
EPOCH 6:
	batch 50 loss: 0.5337763237953186
	batch 100 loss: 0.47709175050258634
	batch 150 loss: 0.43234399616718294
	batch 200 loss: 0.39389736354351046
LOSS [train: 0.39389736354351046] [valid: 0.4043574776810904] TIME [epoch: 240 sec]
EPOCH 7:
	batch 50 loss: 0.3599527913331986
	batch 100 loss: 0.33384288161993025
	batch 150 loss: 0.2938769379258156
	batch 200 loss: 0.24488308489322663
LOSS [train: 0.24488308489322663] [valid: 0.228435489628464] TIME [epoch: 240 sec]
Saving model.
EPOCH 8:
	batch 50 loss: 0.19096850514411925
	batch 100 loss: 0.16286168932914735
	batch 150 loss: 0.15198195800185205
	batch 200 loss: 0.13161005064845085
LOSS [train: 0.13161005064845085] [valid: 0.13252800616125265] TIME [epoch: 241 sec]
Saving model.
EPOCH 9:
	batch 50 loss: 0.11868655443191528
	batch 100 loss: 0.10934465885162353
	batch 150 loss: 0.0975388078391552
	batch 200 loss: 0.08506898067891598
LOSS [train: 0.08506898067891598] [valid: 0.08305171360261739] TIME [epoch: 241 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.07030058801174163
	batch 100 loss: 0.0592039280384779
	batch 150 loss: 0.06952380634844303
	batch 200 loss: 0.06041754744946957
LOSS [train: 0.06041754744946957] [valid: 0.1553734331081311] TIME [epoch: 240 sec]
EPOCH 11:
	batch 50 loss: 0.2067272462323308
	batch 100 loss: 0.12834894079715015
	batch 150 loss: 0.10011613145470619
	batch 200 loss: 0.08071636363863945
LOSS [train: 0.08071636363863945] [valid: 0.0850655402832975] TIME [epoch: 241 sec]
EPOCH 12:
	batch 50 loss: 0.11749041002243757
	batch 100 loss: 0.19324217427521945
	batch 150 loss: 0.7919567117467523
	batch 200 loss: 15.452895853705705
LOSS [train: 15.452895853705705] [valid: 6.0178922911329815] TIME [epoch: 241 sec]
EPOCH 13:
	batch 50 loss: 2.941168929617852
	batch 100 loss: 1.86273500174284
	batch 150 loss: 1.5011909714341163
	batch 200 loss: 1.0716964117437602
LOSS [train: 1.0716964117437602] [valid: 1.0325211987015792] TIME [epoch: 241 sec]
EPOCH 14:
	batch 50 loss: 1.7310998072475194
	batch 100 loss: 0.055686652157455684
	batch 150 loss: 0.072998418956995
	batch 200 loss: 0.0636912264302373
LOSS [train: 0.0636912264302373] [valid: 0.060425691056298095] TIME [epoch: 230 sec]
Saving model.
EPOCH 15:
	batch 50 loss: 0.05894464565441013
	batch 100 loss: 0.055271627120673655
	batch 150 loss: 0.05107165599241853
	batch 200 loss: 0.04531376166269183
LOSS [train: 0.04531376166269183] [valid: 0.038286637201478395] TIME [epoch: 229 sec]
Saving model.
EPOCH 16:
	batch 50 loss: 0.04687514074146747
	batch 100 loss: 0.03746034692972899
	batch 150 loss: 0.038628889080137015
	batch 200 loss: 0.03546331327408552
LOSS [train: 0.03546331327408552] [valid: 0.034927598499537756] TIME [epoch: 229 sec]
Saving model.
EPOCH 17:
	batch 50 loss: 0.03922930410131812
	batch 100 loss: 0.049141302071511746
	batch 150 loss: 0.07859603181481362
	batch 200 loss: 0.07082664374262095
LOSS [train: 0.07082664374262095] [valid: 0.07353248891498272] TIME [epoch: 229 sec]
EPOCH 18:
	batch 50 loss: 0.06529931653290987
	batch 100 loss: 0.04336446413770318
	batch 150 loss: 0.03816988710314036
	batch 200 loss: 0.02659277768805623
LOSS [train: 0.02659277768805623] [valid: 0.026483650165998067] TIME [epoch: 228 sec]
Saving model.
EPOCH 19:
	batch 50 loss: 0.029721992257982492
	batch 100 loss: 0.025428216587752104
	batch 150 loss: 0.031507470728829506
	batch 200 loss: 0.024354555755853654
LOSS [train: 0.024354555755853654] [valid: 0.021753501470084303] TIME [epoch: 228 sec]
Saving model.
EPOCH 20:
	batch 50 loss: 0.023587460974231363
	batch 100 loss: 0.02740404158830643
	batch 150 loss: 0.023677108641713858
	batch 200 loss: 0.022318952661007644
LOSS [train: 0.022318952661007644] [valid: 0.021515290376070577] TIME [epoch: 228 sec]
Saving model.
EPOCH 21:
	batch 50 loss: 0.02473143270239234
	batch 100 loss: 0.020481638517230748
	batch 150 loss: 0.024377066288143397
	batch 200 loss: 0.024635847918689252
LOSS [train: 0.024635847918689252] [valid: 0.020512402793974614] TIME [epoch: 227 sec]
Saving model.
EPOCH 22:
	batch 50 loss: 0.020546311028301716
	batch 100 loss: 0.024126138892024755
	batch 150 loss: 0.021446193428710104
	batch 200 loss: 0.025174429323524237
LOSS [train: 0.025174429323524237] [valid: 0.019151580322068186] TIME [epoch: 227 sec]
Saving model.
EPOCH 23:
	batch 50 loss: 0.0227349922247231
	batch 100 loss: 0.020332198608666658
	batch 150 loss: 0.023699067225679757
	batch 200 loss: 0.021925990618765354
LOSS [train: 0.021925990618765354] [valid: 0.018883033874832714] TIME [epoch: 227 sec]
Saving model.
EPOCH 24:
	batch 50 loss: 0.02504788026213646
	batch 100 loss: 0.028752014823257923
	batch 150 loss: 0.022558664064854385
	batch 200 loss: 0.02600242564454675
LOSS [train: 0.02600242564454675] [valid: 0.020796950133808424] TIME [epoch: 227 sec]
EPOCH 25:
	batch 50 loss: 0.024475673716515302
	batch 100 loss: 0.02117519523948431
	batch 150 loss: 0.022184072965756058
	batch 200 loss: 0.02817662812769413
LOSS [train: 0.02817662812769413] [valid: 0.02022524060254606] TIME [epoch: 227 sec]
EPOCH 26:
	batch 50 loss: 0.023338012183085084
	batch 100 loss: 0.024198844805359842
	batch 150 loss: 0.02235428908839822
	batch 200 loss: 0.027369062444195152
LOSS [train: 0.027369062444195152] [valid: 0.02541583032677105] TIME [epoch: 227 sec]
EPOCH 27:
	batch 50 loss: 0.027120355563238264
	batch 100 loss: 0.020062890211120246
	batch 150 loss: 0.0241393272113055
	batch 200 loss: 0.021461376091465353
LOSS [train: 0.021461376091465353] [valid: 0.019059152099362108] TIME [epoch: 227 sec]
EPOCH 28:
	batch 50 loss: 0.022818428613245487
	batch 100 loss: 0.020732690868899225
	batch 150 loss: 0.022600820986554027
	batch 200 loss: 0.022077264627441764
LOSS [train: 0.022077264627441764] [valid: 0.018639981170417742] TIME [epoch: 227 sec]
Saving model.
EPOCH 29:
	batch 50 loss: 0.01827848589979112
	batch 100 loss: 0.02402985768392682
	batch 150 loss: 0.02912661584094167
	batch 200 loss: 0.05329581525176764
LOSS [train: 0.05329581525176764] [valid: 0.040685080597177146] TIME [epoch: 228 sec]
EPOCH 30:
	batch 50 loss: 0.04024446699768305
	batch 100 loss: 0.035946520678699016
	batch 150 loss: 0.03102076916024089
	batch 200 loss: 0.02776588324457407
LOSS [train: 0.02776588324457407] [valid: 0.02146827367323567] TIME [epoch: 229 sec]
EPOCH 31:
	batch 50 loss: 0.02581195339560509
	batch 100 loss: 0.025463643465191126
	batch 150 loss: 0.031240032482892276
	batch 200 loss: 0.02851593092083931
LOSS [train: 0.02851593092083931] [valid: 0.02257997010674444] TIME [epoch: 239 sec]
EPOCH 32:
	batch 50 loss: 0.025391059555113317
	batch 100 loss: 0.022128774356096982
	batch 150 loss: 0.025840098429471256
	batch 200 loss: 0.02357379423454404
LOSS [train: 0.02357379423454404] [valid: 0.01906124371647214] TIME [epoch: 239 sec]
EPOCH 33:
	batch 50 loss: 0.020155345192179083
	batch 100 loss: 0.022019078312441705
	batch 150 loss: 0.02471255565062165
	batch 200 loss: 0.024200746230781077
LOSS [train: 0.024200746230781077] [valid: 0.018967843386781168] TIME [epoch: 238 sec]
EPOCH 34:
	batch 50 loss: 0.021582904402166606
	batch 100 loss: 0.02242596013471484
	batch 150 loss: 0.023423907253891228
	batch 200 loss: 0.022906652223318814
LOSS [train: 0.022906652223318814] [valid: 0.018837296632894625] TIME [epoch: 238 sec]
EPOCH 35:
	batch 50 loss: 0.02103556824848056
	batch 100 loss: 0.02088523456826806
	batch 150 loss: 0.023143565831705928
	batch 200 loss: 0.02196143424138427
LOSS [train: 0.02196143424138427] [valid: 0.018463330283702818] TIME [epoch: 239 sec]
Saving model.
EPOCH 36:
	batch 50 loss: 0.019765868447721005
	batch 100 loss: 0.022608046382665635
	batch 150 loss: 0.023357523325830697
	batch 200 loss: 0.028434745259582995
LOSS [train: 0.028434745259582995] [valid: 0.03992104395098674] TIME [epoch: 239 sec]
EPOCH 37:
	batch 50 loss: 0.04462397914379835
	batch 100 loss: 0.038967248611152173
	batch 150 loss: 0.03507234174758196
	batch 200 loss: 0.03127690615132451
LOSS [train: 0.03127690615132451] [valid: 0.02795571923876802] TIME [epoch: 239 sec]
EPOCH 38:
	batch 50 loss: 0.029183026868849993
	batch 100 loss: 0.02670442359521985
	batch 150 loss: 0.03104874715209007
	batch 200 loss: 0.027046509087085724
LOSS [train: 0.027046509087085724] [valid: 0.0229595906023557] TIME [epoch: 238 sec]
EPOCH 39:
	batch 50 loss: 0.02344141162931919
	batch 100 loss: 0.02553242879919708
	batch 150 loss: 0.025828994698822497
	batch 200 loss: 0.022296913750469684
LOSS [train: 0.022296913750469684] [valid: 0.02092421172807614] TIME [epoch: 238 sec]
EPOCH 40:
	batch 50 loss: 0.023417468154802917
	batch 100 loss: 0.025099444091320037
	batch 150 loss: 0.03085563026368618
	batch 200 loss: 0.07765916973352432
LOSS [train: 0.07765916973352432] [valid: 0.14075930635347808] TIME [epoch: 239 sec]
EPOCH 41:
	batch 50 loss: 0.12825182549655437
	batch 100 loss: 7.111301233023405
	batch 150 loss: 0.2484103611856699
	batch 200 loss: 0.2015762761980295
LOSS [train: 0.2015762761980295] [valid: 0.26194170201197264] TIME [epoch: 238 sec]
EPOCH 42:
	batch 50 loss: 0.15266466610133647
	batch 100 loss: 0.20802638366818427
	batch 150 loss: 0.14477558732032775
	batch 200 loss: 0.11773907586932182
LOSS [train: 0.11773907586932182] [valid: 0.1652047252903382] TIME [epoch: 238 sec]
EPOCH 43:
	batch 50 loss: 0.11773593559861183
	batch 100 loss: 0.11064111180603504
	batch 150 loss: 0.09495091803371906
	batch 200 loss: 0.08335619017481805
LOSS [train: 0.08335619017481805] [valid: 0.10194472835088769] TIME [epoch: 239 sec]
EPOCH 44:
	batch 50 loss: 0.09003279231488705
	batch 100 loss: 0.06202586203813553
	batch 150 loss: 0.063084500990808
	batch 200 loss: 0.05970356564968824
LOSS [train: 0.05970356564968824] [valid: 0.0549585091881454] TIME [epoch: 238 sec]
EPOCH 45:
	batch 50 loss: 0.04761271476745606
	batch 100 loss: 0.20786028198897838
	batch 150 loss: 0.43784143775701523
	batch 200 loss: 0.08124554805457591
LOSS [train: 0.08124554805457591] [valid: 0.09400953860022128] TIME [epoch: 239 sec]
EPOCH 46:
	batch 50 loss: 0.07893291555345058
	batch 100 loss: 0.0716106553375721
	batch 150 loss: 0.06731967709958553
	batch 200 loss: 0.06432935647666455
LOSS [train: 0.06432935647666455] [valid: 0.08907251840767762] TIME [epoch: 239 sec]
EPOCH 47:
	batch 50 loss: 0.062139941081404686
	batch 100 loss: 0.06474030420184135
	batch 150 loss: 0.06836089186370373
	batch 200 loss: 0.06334941327571869
LOSS [train: 0.06334941327571869] [valid: 0.08702738319213192] TIME [epoch: 239 sec]
EPOCH 48:
	batch 50 loss: 0.06569550544023514
	batch 100 loss: 0.062369767986238
	batch 150 loss: 0.17962169855833054
	batch 200 loss: 0.0795113780349493
LOSS [train: 0.0795113780349493] [valid: 0.07774428902970006] TIME [epoch: 238 sec]
EPOCH 49:
	batch 50 loss: 0.05807701505720615
	batch 100 loss: 0.07873120851814747
	batch 150 loss: 0.06203974954783917
	batch 200 loss: 0.05415866516530514
LOSS [train: 0.05415866516530514] [valid: 0.06551052548456937] TIME [epoch: 239 sec]
EPOCH 50:
	batch 50 loss: 0.05645115986466408
	batch 100 loss: 0.05695346940308809
	batch 150 loss: 0.05410845573991537
	batch 200 loss: 0.055701554864645
LOSS [train: 0.055701554864645] [valid: 0.06271465529377261] TIME [epoch: 238 sec]
Finished training in 11867.632 seconds.
