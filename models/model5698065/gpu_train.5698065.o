Slurm job ID: 5698065
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', infer_noise=True, learning_rate=0.001, momentum=0.9, name='model5698065', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='rms', outdir='out/model_training/model5698065', seed=0, sigma=0.01, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 1070135557
EPOCH 1:
	batch 50 loss: 0.6007821834087372
	batch 100 loss: 0.1371806514263153
	batch 150 loss: 0.08656971029937267
	batch 200 loss: 0.06727208614349366
LOSS [train: 0.06727208614349366] [valid: 0.05722266181061665] TIME [epoch: 579 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.05579057328402996
	batch 100 loss: 0.05037184961140156
	batch 150 loss: 0.0412612222507596
	batch 200 loss: 0.03544029239565134
LOSS [train: 0.03544029239565134] [valid: 0.03156030102012058] TIME [epoch: 604 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 0.036393513567745686
	batch 100 loss: 0.028821968138217927
	batch 150 loss: 0.030124889239668846
	batch 200 loss: 0.02743193319067359
LOSS [train: 0.02743193319067359] [valid: 0.023841308239692202] TIME [epoch: 607 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 0.024176225811243058
	batch 100 loss: 0.028623198922723532
	batch 150 loss: 0.02629252603277564
	batch 200 loss: 0.023310658540576696
LOSS [train: 0.023310658540576696] [valid: 0.01982781753176823] TIME [epoch: 591 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.023861448653042315
	batch 100 loss: 0.020719105526804924
	batch 150 loss: 0.023954184018075465
	batch 200 loss: 0.021782457567751407
LOSS [train: 0.021782457567751407] [valid: 0.017836076244323824] TIME [epoch: 579 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.019886901676654814
	batch 100 loss: 0.020745002925395967
	batch 150 loss: 0.019873169260099532
	batch 200 loss: 0.02322999535128474
LOSS [train: 0.02322999535128474] [valid: 0.01809913645265624] TIME [epoch: 576 sec]
EPOCH 7:
	batch 50 loss: 0.019116270784288646
	batch 100 loss: 0.022070165453478695
	batch 150 loss: 0.02123967496678233
	batch 200 loss: 0.02208092185668647
LOSS [train: 0.02208092185668647] [valid: 0.01842250501019104] TIME [epoch: 575 sec]
EPOCH 8:
	batch 50 loss: 0.022555155418813227
	batch 100 loss: 0.020136409495025872
	batch 150 loss: 0.01911776628345251
	batch 200 loss: 0.020440368792042137
LOSS [train: 0.020440368792042137] [valid: 0.017694734079608074] TIME [epoch: 574 sec]
Saving model.
EPOCH 9:
	batch 50 loss: 0.019064393360167742
	batch 100 loss: 0.021382956430315972
	batch 150 loss: 0.020369136035442353
	batch 200 loss: 0.021032191552221774
LOSS [train: 0.021032191552221774] [valid: 0.018090883269906044] TIME [epoch: 576 sec]
EPOCH 10:
	batch 50 loss: 0.018699303492903708
	batch 100 loss: 0.021526147685945032
	batch 150 loss: 0.021013896968215703
	batch 200 loss: 0.018968120673671363
LOSS [train: 0.018968120673671363] [valid: 0.017902942127451145] TIME [epoch: 575 sec]
EPOCH 11:
	batch 50 loss: 0.021754484269768
	batch 100 loss: 0.02074237085878849
	batch 150 loss: 0.019098135530948638
	batch 200 loss: 0.018894289145246147
LOSS [train: 0.018894289145246147] [valid: 0.017644909761535623] TIME [epoch: 575 sec]
Saving model.
EPOCH 12:
	batch 50 loss: 0.02069470810703933
	batch 100 loss: 0.019937444925308228
	batch 150 loss: 0.015981452269479633
	batch 200 loss: 0.021637921705842018
LOSS [train: 0.021637921705842018] [valid: 0.01714815045852447] TIME [epoch: 576 sec]
Saving model.
EPOCH 13:
	batch 50 loss: 0.018465539813041686
	batch 100 loss: 0.01962649683468044
	batch 150 loss: 0.01819218526594341
	batch 200 loss: 0.02002263119444251
LOSS [train: 0.02002263119444251] [valid: 0.01645359562583811] TIME [epoch: 575 sec]
Saving model.
EPOCH 14:
	batch 50 loss: 0.017096319412812592
	batch 100 loss: 0.022067061569541694
	batch 150 loss: 0.019061348708346484
	batch 200 loss: 0.018902790481224657
LOSS [train: 0.018902790481224657] [valid: 0.01739508215978276] TIME [epoch: 574 sec]
EPOCH 15:
	batch 50 loss: 0.020154981464147567
	batch 100 loss: 0.01988374289125204
	batch 150 loss: 0.01931323948316276
	batch 200 loss: 0.017796323215588928
LOSS [train: 0.017796323215588928] [valid: 0.01623041671603763] TIME [epoch: 575 sec]
Saving model.
EPOCH 16:
	batch 50 loss: 0.019198774974793196
	batch 100 loss: 0.01949146925471723
	batch 150 loss: 0.01632425113581121
	batch 200 loss: 0.019735068092122676
LOSS [train: 0.019735068092122676] [valid: 0.016434419034339953] TIME [epoch: 575 sec]
EPOCH 17:
	batch 50 loss: 0.018648946210741996
	batch 100 loss: 0.01804535706527531
	batch 150 loss: 0.01756882222369313
	batch 200 loss: 0.018575595300644634
LOSS [train: 0.018575595300644634] [valid: 0.016518965714688725] TIME [epoch: 574 sec]
EPOCH 18:
	batch 50 loss: 0.01934616661630571
	batch 100 loss: 0.01942716583609581
	batch 150 loss: 0.017023277934640646
	batch 200 loss: 0.01649682566523552
LOSS [train: 0.01649682566523552] [valid: 0.016271301410112453] TIME [epoch: 574 sec]
EPOCH 19:
	batch 50 loss: 0.01839827257208526
	batch 100 loss: 0.017764070564880968
	batch 150 loss: 0.017522878758609296
	batch 200 loss: 0.024849972520023585
LOSS [train: 0.024849972520023585] [valid: 0.016735789470840246] TIME [epoch: 573 sec]
EPOCH 20:
	batch 50 loss: 0.018853382132947447
	batch 100 loss: 0.014964812034741044
	batch 150 loss: 0.01874929467216134
	batch 200 loss: 0.019003267316147685
LOSS [train: 0.019003267316147685] [valid: 0.017090758160338736] TIME [epoch: 573 sec]
EPOCH 21:
	batch 50 loss: 0.01819100394845009
	batch 100 loss: 0.01802133842371404
	batch 150 loss: 0.017760774632915855
	batch 200 loss: 0.020288808792829512
LOSS [train: 0.020288808792829512] [valid: 0.014755600438608478] TIME [epoch: 573 sec]
Saving model.
EPOCH 22:
	batch 50 loss: 0.018635516203939915
	batch 100 loss: 0.01578665280714631
	batch 150 loss: 0.018343044137582185
	batch 200 loss: 0.018078643763437867
LOSS [train: 0.018078643763437867] [valid: 0.01522284457181134] TIME [epoch: 574 sec]
EPOCH 23:
	batch 50 loss: 0.01857904967851937
	batch 100 loss: 0.01632661757990718
	batch 150 loss: 0.01814409830607474
	batch 200 loss: 0.018380639972165227
LOSS [train: 0.018380639972165227] [valid: 0.015368613078074607] TIME [epoch: 573 sec]
EPOCH 24:
	batch 50 loss: 0.016257149893790485
	batch 100 loss: 0.01569748977199197
	batch 150 loss: 0.016966014942154287
	batch 200 loss: 0.020121782533824444
LOSS [train: 0.020121782533824444] [valid: 0.01622612163919257] TIME [epoch: 574 sec]
EPOCH 25:
	batch 50 loss: 0.019893052279949187
	batch 100 loss: 0.016639178935438394
	batch 150 loss: 0.01759525427594781
	batch 200 loss: 0.014535238165408373
LOSS [train: 0.014535238165408373] [valid: 0.014795299358471918] TIME [epoch: 574 sec]
EPOCH 26:
	batch 50 loss: 0.018734579272568226
	batch 100 loss: 0.01530619009397924
	batch 150 loss: 0.016406081896275283
	batch 200 loss: 0.01523519616574049
LOSS [train: 0.01523519616574049] [valid: 0.015570318485454966] TIME [epoch: 575 sec]
EPOCH 27:
	batch 50 loss: 0.0165559327788651
	batch 100 loss: 0.01591956652700901
	batch 150 loss: 0.01449239975772798
	batch 200 loss: 0.014276405498385429
LOSS [train: 0.014276405498385429] [valid: 0.013421097042252465] TIME [epoch: 574 sec]
Saving model.
EPOCH 28:
	batch 50 loss: 0.01501112937927246
	batch 100 loss: 0.01457739963196218
	batch 150 loss: 0.013729996094480158
	batch 200 loss: 0.01405388804152608
LOSS [train: 0.01405388804152608] [valid: 0.011150937160709872] TIME [epoch: 574 sec]
Saving model.
EPOCH 29:
	batch 50 loss: 0.013164067640900612
	batch 100 loss: 0.01416948426514864
	batch 150 loss: 0.056918162768706676
	batch 200 loss: 0.03101329866796732
LOSS [train: 0.03101329866796732] [valid: 0.019807532790582626] TIME [epoch: 574 sec]
EPOCH 30:
	batch 50 loss: 0.021966553293168545
	batch 100 loss: 0.019079104140400885
	batch 150 loss: 0.020928884223103524
	batch 200 loss: 0.020969177251681684
LOSS [train: 0.020969177251681684] [valid: 0.017815519501649155] TIME [epoch: 574 sec]
EPOCH 31:
	batch 50 loss: 0.019294389318674804
	batch 100 loss: 0.019063339373096823
	batch 150 loss: 0.019746368159539996
	batch 200 loss: 0.017933466546237467
LOSS [train: 0.017933466546237467] [valid: 0.016152458012948047] TIME [epoch: 575 sec]
EPOCH 32:
	batch 50 loss: 0.01827064136043191
	batch 100 loss: 0.018058964051306248
	batch 150 loss: 0.014737115632742643
	batch 200 loss: 0.015564576648175716
LOSS [train: 0.015564576648175716] [valid: 0.014039268031046959] TIME [epoch: 574 sec]
EPOCH 33:
	batch 50 loss: 0.01527993880212307
	batch 100 loss: 0.01638927451334894
	batch 150 loss: 0.014837545379996299
	batch 200 loss: 0.013874513180926442
LOSS [train: 0.013874513180926442] [valid: 0.01301291446822385] TIME [epoch: 574 sec]
EPOCH 34:
	batch 50 loss: 0.015411348957568407
	batch 100 loss: 0.014100698167458176
	batch 150 loss: 0.016649851705878974
	batch 200 loss: 0.01392465685494244
LOSS [train: 0.01392465685494244] [valid: 0.011658290305058472] TIME [epoch: 574 sec]
EPOCH 35:
	batch 50 loss: 0.013220069613307715
	batch 100 loss: 0.012687343992292882
	batch 150 loss: 0.012941574836149812
	batch 200 loss: 0.011543608540669083
LOSS [train: 0.011543608540669083] [valid: 0.010018544040212874] TIME [epoch: 575 sec]
Saving model.
EPOCH 36:
	batch 50 loss: 0.013342249626293778
	batch 100 loss: 0.012596063949167728
	batch 150 loss: 0.011633372707292438
	batch 200 loss: 0.010652414653450251
LOSS [train: 0.010652414653450251] [valid: 0.01095168717923419] TIME [epoch: 574 sec]
EPOCH 37:
	batch 50 loss: 0.012064014319330454
	batch 100 loss: 0.18895849598571657
	batch 150 loss: 0.11362612016499042
	batch 200 loss: 0.042579145208001136
LOSS [train: 0.042579145208001136] [valid: 0.02938864240422845] TIME [epoch: 573 sec]
EPOCH 38:
	batch 50 loss: 0.0270748408511281
	batch 100 loss: 0.02641490824520588
	batch 150 loss: 0.01994020549580455
	batch 200 loss: 0.022987994085997342
LOSS [train: 0.022987994085997342] [valid: 0.019088762416504323] TIME [epoch: 574 sec]
EPOCH 39:
	batch 50 loss: 0.02107956426218152
	batch 100 loss: 0.022805284019559623
	batch 150 loss: 0.01974495718255639
	batch 200 loss: 0.019142025168985128
LOSS [train: 0.019142025168985128] [valid: 0.018322853232287648] TIME [epoch: 574 sec]
EPOCH 40:
	batch 50 loss: 0.01862477251328528
	batch 100 loss: 0.019780193781480195
	batch 150 loss: 0.019645673939958214
	batch 200 loss: 0.019899845821782947
LOSS [train: 0.019899845821782947] [valid: 0.01731433782891448] TIME [epoch: 575 sec]
EPOCH 41:
	batch 50 loss: 0.019902522768825294
	batch 100 loss: 0.02045257625170052
	batch 150 loss: 0.021089690905064344
	batch 200 loss: 0.018135575354099275
LOSS [train: 0.018135575354099275] [valid: 0.016656069609113425] TIME [epoch: 572 sec]
EPOCH 42:
	batch 50 loss: 0.021238837912678717
	batch 100 loss: 0.019886875338852406
	batch 150 loss: 0.01778235281817615
	batch 200 loss: 0.017579603157937527
LOSS [train: 0.017579603157937527] [valid: 0.01619175900341361] TIME [epoch: 573 sec]
EPOCH 43:
	batch 50 loss: 0.02033448662608862
	batch 100 loss: 0.0166269407607615
	batch 150 loss: 0.020905730165541172
	batch 200 loss: 0.01771847138181329
LOSS [train: 0.01771847138181329] [valid: 0.018049581121401086] TIME [epoch: 573 sec]
EPOCH 44:
	batch 50 loss: 0.01741138828918338
	batch 100 loss: 0.0193324223626405
	batch 150 loss: 0.020104178600013255
	batch 200 loss: 0.019126849817112088
LOSS [train: 0.019126849817112088] [valid: 0.016235288790388342] TIME [epoch: 573 sec]
EPOCH 45:
	batch 50 loss: 0.02007841248065233
	batch 100 loss: 0.01751006561331451
	batch 150 loss: 0.021474247090518473
	batch 200 loss: 0.016294840537011624
LOSS [train: 0.016294840537011624] [valid: 0.015066002951061818] TIME [epoch: 573 sec]
EPOCH 46:
	batch 50 loss: 0.019306652471423148
	batch 100 loss: 0.01811051094904542
	batch 150 loss: 0.017915056897327305
	batch 200 loss: 0.015138439247384667
LOSS [train: 0.015138439247384667] [valid: 0.015146001520042774] TIME [epoch: 572 sec]
EPOCH 47:
	batch 50 loss: 0.018013777900487183
	batch 100 loss: 0.01595493189059198
	batch 150 loss: 0.01703619796782732
	batch 200 loss: 0.015838548792526125
LOSS [train: 0.015838548792526125] [valid: 0.014931619684405935] TIME [epoch: 572 sec]
EPOCH 48:
	batch 50 loss: 0.01823557320982218
	batch 100 loss: 0.016536254482343794
	batch 150 loss: 0.01546804410405457
	batch 200 loss: 0.015215284526348113
LOSS [train: 0.015215284526348113] [valid: 0.015178293499775463] TIME [epoch: 572 sec]
EPOCH 49:
	batch 50 loss: 0.014617288131266832
	batch 100 loss: 0.017226675432175396
	batch 150 loss: 0.014208301678299904
	batch 200 loss: 0.016460934970527887
LOSS [train: 0.016460934970527887] [valid: 0.014335354796882409] TIME [epoch: 573 sec]
EPOCH 50:
	batch 50 loss: 0.015001239031553268
	batch 100 loss: 0.016502255368977786
	batch 150 loss: 0.013928943574428558
	batch 200 loss: 0.015301331840455533
LOSS [train: 0.015301331840455533] [valid: 0.013264677409461] TIME [epoch: 573 sec]
Finished training in 28788.015 seconds.
