Slurm job ID: 6347976
Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['tanh'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='kl', momentum=0.9, name='model6347976', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='sgd', outdir='out/model_training/model6347976', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2')
Using device: cuda
Using seed: 2278838257
EPOCH 1:
	batch 50 loss: 5.232950096130371
	batch 100 loss: 0.8167229966819286
	batch 150 loss: 0.11921848942700308
	batch 200 loss: 0.09805076731368899
LOSS [train: 0.09805076731368899] [valid: 0.07318181385223094] TIME [epoch: 266 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 0.08034531069220975
	batch 100 loss: 0.08129896414931864
	batch 150 loss: 0.09973198119201697
	batch 200 loss: 0.07092381526279495
LOSS [train: 0.07092381526279495] [valid: 0.09016228437346095] TIME [epoch: 264 sec]
EPOCH 3:
	batch 50 loss: 0.09344234612770379
	batch 100 loss: 0.07508476211412926
	batch 150 loss: 0.05900140784215182
	batch 200 loss: 0.07111851886231307
LOSS [train: 0.07111851886231307] [valid: 0.08680048841245783] TIME [epoch: 266 sec]
EPOCH 4:
	batch 50 loss: 0.08215190948685631
	batch 100 loss: 0.07020808567060158
	batch 150 loss: 0.08479594840668142
	batch 200 loss: 0.08796825209166854
LOSS [train: 0.08796825209166854] [valid: 0.08077537718345411] TIME [epoch: 266 sec]
EPOCH 5:
	batch 50 loss: 0.11440400425344706
	batch 100 loss: 0.07811013038270176
	batch 150 loss: 0.06210429536367883
	batch 200 loss: 0.12411323128617369
LOSS [train: 0.12411323128617369] [valid: 0.1072544077100853] TIME [epoch: 266 sec]
EPOCH 6:
	batch 50 loss: 0.09942960232961923
	batch 100 loss: 0.06423244894947856
	batch 150 loss: 0.08427304999437184
	batch 200 loss: 0.09624558992218227
LOSS [train: 0.09624558992218227] [valid: 0.07583953499949227] TIME [epoch: 267 sec]
EPOCH 7:
	batch 50 loss: 0.09607828854117542
	batch 100 loss: 0.10314240954699926
	batch 150 loss: 0.05925440499559045
	batch 200 loss: 0.06779408208356472
LOSS [train: 0.06779408208356472] [valid: 0.07845878815666461] TIME [epoch: 279 sec]
EPOCH 8:
	batch 50 loss: 0.06230315766530112
	batch 100 loss: 0.09315912023652345
	batch 150 loss: 0.09004816758446395
	batch 200 loss: 0.10146320431493222
LOSS [train: 0.10146320431493222] [valid: 0.08784864590076419] TIME [epoch: 279 sec]
EPOCH 9:
	batch 50 loss: 0.08197868546471
	batch 100 loss: 0.07332145091379061
	batch 150 loss: 0.07642538502579554
	batch 200 loss: 0.07445434645283967
LOSS [train: 0.07445434645283967] [valid: 0.07340051012967402] TIME [epoch: 279 sec]
EPOCH 10:
	batch 50 loss: 0.08237505804747343
	batch 100 loss: 0.05533951487392187
	batch 150 loss: 0.08741692600073293
	batch 200 loss: 0.09735994269605726
LOSS [train: 0.09735994269605726] [valid: 0.059225451469925854] TIME [epoch: 278 sec]
Saving model.
EPOCH 11:
	batch 50 loss: 0.078548120619962
	batch 100 loss: 0.06822171663865446
	batch 150 loss: 0.07305282191373408
	batch 200 loss: 0.06744156032975297
LOSS [train: 0.06744156032975297] [valid: 0.08809502869844436] TIME [epoch: 278 sec]
EPOCH 12:
	batch 50 loss: 0.08397533757612109
	batch 100 loss: 0.05347765458747745
	batch 150 loss: 0.08149373712483793
	batch 200 loss: 0.061732824106002226
LOSS [train: 0.061732824106002226] [valid: 0.10159397766692564] TIME [epoch: 276 sec]
EPOCH 13:
	batch 50 loss: 0.09162667728029192
	batch 100 loss: 0.1089241522172233
	batch 150 loss: 0.07288465858844574
	batch 200 loss: 0.05351069792173803
LOSS [train: 0.05351069792173803] [valid: 0.05711527567279215] TIME [epoch: 263 sec]
Saving model.
EPOCH 14:
	batch 50 loss: 0.05752046134148259
	batch 100 loss: 0.08176164630334824
	batch 150 loss: 0.06846169862721581
	batch 200 loss: 0.06992333102272824
LOSS [train: 0.06992333102272824] [valid: 0.05389527583126134] TIME [epoch: 262 sec]
Saving model.
EPOCH 15:
	batch 50 loss: 0.05818487282609567
	batch 100 loss: 0.09224138454534113
	batch 150 loss: 0.08876989961368963
	batch 200 loss: 0.05618548688478768
LOSS [train: 0.05618548688478768] [valid: 0.06531154380063527] TIME [epoch: 267 sec]
EPOCH 16:
	batch 50 loss: 0.07175760750920744
	batch 100 loss: 0.09652635940816254
	batch 150 loss: 0.10627177248472436
	batch 200 loss: 0.06893503302475437
LOSS [train: 0.06893503302475437] [valid: 0.07882839075076238] TIME [epoch: 267 sec]
EPOCH 17:
	batch 50 loss: 0.0663690100144595
	batch 100 loss: 0.07213679572567344
	batch 150 loss: 0.10646759919356555
	batch 200 loss: 0.06566710954532028
LOSS [train: 0.06566710954532028] [valid: 0.05852228030125843] TIME [epoch: 266 sec]
EPOCH 18:
	batch 50 loss: 0.04943439525552094
	batch 100 loss: 0.07189369253697805
	batch 150 loss: 0.08687895913724787
	batch 200 loss: 0.05863417660701089
LOSS [train: 0.05863417660701089] [valid: 0.060075501265237106] TIME [epoch: 266 sec]
EPOCH 19:
	batch 50 loss: 0.0655843520606868
	batch 100 loss: 0.07433229409856722
	batch 150 loss: 0.1201101259328425
	batch 200 loss: 0.07371663386176806
LOSS [train: 0.07371663386176806] [valid: 0.05881703509561097] TIME [epoch: 254 sec]
EPOCH 20:
	batch 50 loss: 0.06757647258229554
	batch 100 loss: 0.07690049638040364
	batch 150 loss: 0.07229865465284092
	batch 200 loss: 0.06228830924839713
LOSS [train: 0.06228830924839713] [valid: 0.06038994053960778] TIME [epoch: 253 sec]
EPOCH 21:
	batch 50 loss: 0.07535734398057685
	batch 100 loss: 0.070098486879142
	batch 150 loss: 0.06956304954597727
	batch 200 loss: 0.056755603463097944
LOSS [train: 0.056755603463097944] [valid: 0.05257987000901873] TIME [epoch: 254 sec]
Saving model.
EPOCH 22:
	batch 50 loss: 0.07342198602389544
	batch 100 loss: 0.08139730813913047
	batch 150 loss: 0.051906688087037765
	batch 200 loss: 0.06146486789453775
LOSS [train: 0.06146486789453775] [valid: 0.05593902677840864] TIME [epoch: 254 sec]
EPOCH 23:
	batch 50 loss: 0.07915440984070302
	batch 100 loss: 0.05995050898520276
	batch 150 loss: 0.07054091882193461
	batch 200 loss: 0.061836038576439024
LOSS [train: 0.061836038576439024] [valid: 0.055653778012492695] TIME [epoch: 253 sec]
EPOCH 24:
	batch 50 loss: 0.0653078423376428
	batch 100 loss: 0.06244133417494595
	batch 150 loss: 0.09107152001466602
	batch 200 loss: 0.0745333800977096
LOSS [train: 0.0745333800977096] [valid: 0.08363386496203021] TIME [epoch: 255 sec]
EPOCH 25:
	batch 50 loss: 0.08402063424699008
	batch 100 loss: 0.08399798948084936
	batch 150 loss: 0.05936530886217952
	batch 200 loss: 0.04792990285030101
LOSS [train: 0.04792990285030101] [valid: 0.0607527823895604] TIME [epoch: 254 sec]
EPOCH 26:
	batch 50 loss: 0.07515192374470643
	batch 100 loss: 0.0768173451628536
	batch 150 loss: 0.07389508376130834
	batch 200 loss: 0.06484310844447463
LOSS [train: 0.06484310844447463] [valid: 0.0596623162428538] TIME [epoch: 254 sec]
EPOCH 27:
	batch 50 loss: 0.08568215146660804
	batch 100 loss: 0.06666743321253307
	batch 150 loss: 0.06487473031971604
	batch 200 loss: 0.05405187564902007
LOSS [train: 0.05405187564902007] [valid: 0.055777207210970424] TIME [epoch: 254 sec]
EPOCH 28:
	batch 50 loss: 0.07438911208184436
	batch 100 loss: 0.06847104615764693
	batch 150 loss: 0.060634364052966704
	batch 200 loss: 0.06815269286045805
LOSS [train: 0.06815269286045805] [valid: 0.06712758276844397] TIME [epoch: 255 sec]
EPOCH 29:
	batch 50 loss: 0.07344829115638277
	batch 100 loss: 0.07072374826297163
	batch 150 loss: 0.05515834214107599
	batch 200 loss: 0.07588080933317543
LOSS [train: 0.07588080933317543] [valid: 0.08075652025387778] TIME [epoch: 254 sec]
EPOCH 30:
	batch 50 loss: 0.08732159269740805
	batch 100 loss: 0.061268373888451605
	batch 150 loss: 0.04247495503630489
	batch 200 loss: 0.06433236111275618
LOSS [train: 0.06433236111275618] [valid: 0.07606947560367795] TIME [epoch: 254 sec]
EPOCH 31:
	batch 50 loss: 0.06539650665130466
	batch 100 loss: 0.09261285285465419
	batch 150 loss: 0.06412644541589543
	batch 200 loss: 0.09152956497739069
LOSS [train: 0.09152956497739069] [valid: 0.19595323990409572] TIME [epoch: 266 sec]
EPOCH 32:
	batch 50 loss: 0.08849146512633524
	batch 100 loss: 0.0686634995535951
	batch 150 loss: 0.058262399048544466
	batch 200 loss: 0.06941366224069498
LOSS [train: 0.06941366224069498] [valid: 0.05852744837593491] TIME [epoch: 266 sec]
EPOCH 33:
	batch 50 loss: 0.05762498924159445
	batch 100 loss: 0.07706536122655962
	batch 150 loss: 0.0664348416775465
	batch 200 loss: 0.05239425843115896
LOSS [train: 0.05239425843115896] [valid: 0.04613147931134639] TIME [epoch: 266 sec]
Saving model.
EPOCH 34:
	batch 50 loss: 0.08134152251761406
	batch 100 loss: 0.06644801592919976
	batch 150 loss: 0.04351925894035958
	batch 200 loss: 0.06267623486462981
LOSS [train: 0.06267623486462981] [valid: 0.10222957709144491] TIME [epoch: 266 sec]
EPOCH 35:
	batch 50 loss: 0.07824425947619602
	batch 100 loss: 0.04683411190169864
	batch 150 loss: 0.06703905728179961
	batch 200 loss: 0.10191190834913869
LOSS [train: 0.10191190834913869] [valid: 0.07697479763495114] TIME [epoch: 266 sec]
EPOCH 36:
	batch 50 loss: 0.05056741450825939
	batch 100 loss: 0.05016154479701072
	batch 150 loss: 0.07402265042765066
	batch 200 loss: 0.06943163527408615
LOSS [train: 0.06943163527408615] [valid: 0.04752431442029774] TIME [epoch: 265 sec]
EPOCH 37:
	batch 50 loss: 0.05183295054244809
	batch 100 loss: 0.061201366742607205
	batch 150 loss: 0.1894303686497733
	batch 200 loss: 0.1332738359074574
LOSS [train: 0.1332738359074574] [valid: 0.05887878293481966] TIME [epoch: 265 sec]
EPOCH 38:
	batch 50 loss: 0.0658183831430506
	batch 100 loss: 0.05241905598784797
	batch 150 loss: 0.06761365720303729
	batch 200 loss: 0.061156122926622626
LOSS [train: 0.061156122926622626] [valid: 0.04119890971220836] TIME [epoch: 266 sec]
Saving model.
EPOCH 39:
	batch 50 loss: 0.05118664641864598
	batch 100 loss: 0.07864849020959809
	batch 150 loss: 0.05445953203714453
	batch 200 loss: 0.047261557830497625
LOSS [train: 0.047261557830497625] [valid: 0.06172686014712478] TIME [epoch: 265 sec]
EPOCH 40:
	batch 50 loss: 0.06462878191610798
	batch 100 loss: 0.07104204186005518
	batch 150 loss: 0.7922361390199513
	batch 200 loss: 0.14593333374010398
LOSS [train: 0.14593333374010398] [valid: 0.04825602103859031] TIME [epoch: 265 sec]
EPOCH 41:
	batch 50 loss: 0.05864773045701441
	batch 100 loss: 0.06689727058212157
	batch 150 loss: 0.07866685166489333
	batch 200 loss: 0.05889917180407792
LOSS [train: 0.05889917180407792] [valid: 0.05071707380702719] TIME [epoch: 265 sec]
EPOCH 42:
	batch 50 loss: 0.054262008058140056
	batch 100 loss: 0.05876844624988735
	batch 150 loss: 0.4066583019308746
	batch 200 loss: 0.21443915169686079
LOSS [train: 0.21443915169686079] [valid: 0.15313980473559544] TIME [epoch: 265 sec]
EPOCH 43:
	batch 50 loss: 0.13365878437645734
	batch 100 loss: 0.14530394065892324
	batch 150 loss: 0.08889028016477823
	batch 200 loss: 0.08469029797706754
LOSS [train: 0.08469029797706754] [valid: 0.0736484135404074] TIME [epoch: 266 sec]
EPOCH 44:
	batch 50 loss: 0.11674157415371156
	batch 100 loss: 0.09494683675467969
	batch 150 loss: 0.06472055713878945
	batch 200 loss: 0.058602225980721415
LOSS [train: 0.058602225980721415] [valid: 0.07675733929354464] TIME [epoch: 266 sec]
EPOCH 45:
	batch 50 loss: 0.09898022815119475
	batch 100 loss: 0.053347613082733
	batch 150 loss: 0.06489715951960534
	batch 200 loss: 0.06691164746880532
LOSS [train: 0.06691164746880532] [valid: 0.0454582840349758] TIME [epoch: 265 sec]
EPOCH 46:
	batch 50 loss: 0.06991183193866163
	batch 100 loss: 0.0674888804834336
	batch 150 loss: 0.058084292258135974
	batch 200 loss: 0.06169800650328398
LOSS [train: 0.06169800650328398] [valid: 0.06436614060755043] TIME [epoch: 268 sec]
EPOCH 47:
	batch 50 loss: 0.0685205391375348
	batch 100 loss: 0.0704128148034215
	batch 150 loss: 0.05428402425954118
	batch 200 loss: 0.0675581431388855
LOSS [train: 0.0675581431388855] [valid: 0.05870170096265307] TIME [epoch: 268 sec]
EPOCH 48:
	batch 50 loss: 0.06124038578160253
	batch 100 loss: 0.06915613705088618
	batch 150 loss: 0.06587582962820307
	batch 200 loss: 0.04990992250677664
LOSS [train: 0.04990992250677664] [valid: 0.05357300055523714] TIME [epoch: 268 sec]
EPOCH 49:
	batch 50 loss: 0.06509911486209603
	batch 100 loss: 0.06654426068998873
	batch 150 loss: 0.0588789717759937
	batch 200 loss: 0.0651417815964669
LOSS [train: 0.0651417815964669] [valid: 0.05963616866283701] TIME [epoch: 268 sec]
EPOCH 50:
	batch 50 loss: 0.07618444232037291
	batch 100 loss: 0.06721551583148538
	batch 150 loss: 0.06692803738173098
	batch 200 loss: 0.06438280174596002
LOSS [train: 0.06438280174596002] [valid: 0.05264560447540134] TIME [epoch: 268 sec]
Finished training in 13330.639 seconds.
