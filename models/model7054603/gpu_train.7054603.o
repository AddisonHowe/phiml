Slurm job ID: 7054603
args: Namespace(batch_size=50, continuation=None, dt=0.1, dtype='float32', final_act='None', hidden_acts=['softplus'], hidden_dims=[16, 32, 32, 32, 16], infer_noise=True, layer_normalize=False, learning_rate=0.001, loss='kl', momentum=0.9, name='model7054603', ncells=100, ndims=2, nsigs=2, nsims_training=100, nsims_validation=30, num_epochs=50, optimizer='adam', outdir='out/model_training/model7054603', plot=True, seed=0, sigma=0.01, signal_function='jump', timestamp=False, training_data='data/model_training_data_2', use_gpu=True, validation_data='data/model_validation_data_2', weight_decay=0.0001)
Using device: cuda
Using seed: 4086843585
EPOCH 1:
	batch 50 loss: 5.466041975021362
	batch 100 loss: 5.354152307510376
	batch 150 loss: 5.146531496047974
	batch 200 loss: 5.0172667312622075
LOSS [train: 5.0172667312622075] [valid: 5.286329830437898] TIME [epoch: 298 sec]
Saving model.
EPOCH 2:
	batch 50 loss: 4.704107441902161
	batch 100 loss: 4.513944716453552
	batch 150 loss: 4.245405259132386
	batch 200 loss: 3.89234929561615
LOSS [train: 3.89234929561615] [valid: 4.283181674033403] TIME [epoch: 297 sec]
Saving model.
EPOCH 3:
	batch 50 loss: 3.5083713817596434
	batch 100 loss: 3.0987690258026124
	batch 150 loss: 2.5959076619148256
	batch 200 loss: 1.9307792830467223
LOSS [train: 1.9307792830467223] [valid: 2.0083353854715824] TIME [epoch: 298 sec]
Saving model.
EPOCH 4:
	batch 50 loss: 1.3387797749042512
	batch 100 loss: 0.6742316907644272
	batch 150 loss: 0.30259117811918257
	batch 200 loss: 0.1371472813934088
LOSS [train: 0.1371472813934088] [valid: 0.11166856616425018] TIME [epoch: 298 sec]
Saving model.
EPOCH 5:
	batch 50 loss: 0.0956614082865417
	batch 100 loss: 0.08606832880526781
	batch 150 loss: 0.06916061046533287
	batch 200 loss: 0.07636556488942006
LOSS [train: 0.07636556488942006] [valid: 0.06605922997647819] TIME [epoch: 298 sec]
Saving model.
EPOCH 6:
	batch 50 loss: 0.06514640509616583
	batch 100 loss: 0.056269983985694126
	batch 150 loss: 0.06946599815972149
	batch 200 loss: 0.0780345863639377
LOSS [train: 0.0780345863639377] [valid: 0.057862940973912674] TIME [epoch: 298 sec]
Saving model.
EPOCH 7:
	batch 50 loss: 0.0679075389256468
	batch 100 loss: 0.0620995497208969
	batch 150 loss: 0.06607361526257591
	batch 200 loss: 0.07087253501947999
LOSS [train: 0.07087253501947999] [valid: 0.06030822563625406] TIME [epoch: 298 sec]
EPOCH 8:
	batch 50 loss: 0.07896265030838549
	batch 100 loss: 0.0706484775734134
	batch 150 loss: 0.04425060113950167
	batch 200 loss: 0.0544412859575823
LOSS [train: 0.0544412859575823] [valid: 0.057605677742200594] TIME [epoch: 299 sec]
Saving model.
EPOCH 9:
	batch 50 loss: 0.06668757505947724
	batch 100 loss: 0.049737115362659096
	batch 150 loss: 0.07253017862793058
	batch 200 loss: 0.06175160386454081
LOSS [train: 0.06175160386454081] [valid: 0.056550574335778946] TIME [epoch: 299 sec]
Saving model.
EPOCH 10:
	batch 50 loss: 0.05371175432199379
	batch 100 loss: 0.0714777173474431
	batch 150 loss: 0.06616999167366884
	batch 200 loss: 0.04476425739470869
LOSS [train: 0.04476425739470869] [valid: 0.05790011452239317] TIME [epoch: 298 sec]
EPOCH 11:
	batch 50 loss: 0.04944760115147801
	batch 100 loss: 0.048038298799656334
	batch 150 loss: 0.07039603269309737
	batch 200 loss: 0.06988355288449384
LOSS [train: 0.06988355288449384] [valid: 0.055588604723258564] TIME [epoch: 299 sec]
Saving model.
EPOCH 12:
	batch 50 loss: 0.06316762391012162
	batch 100 loss: 0.06196173513773829
	batch 150 loss: 0.052016350170597435
	batch 200 loss: 0.057524956571869554
LOSS [train: 0.057524956571869554] [valid: 0.05460854349269842] TIME [epoch: 298 sec]
Saving model.
EPOCH 13:
	batch 50 loss: 0.06396456903312356
	batch 100 loss: 0.07338694493751972
	batch 150 loss: 0.043594917320588136
	batch 200 loss: 0.046653073950292306
LOSS [train: 0.046653073950292306] [valid: 0.05189230557928871] TIME [epoch: 298 sec]
Saving model.
EPOCH 14:
	batch 50 loss: 0.0563633309607394
	batch 100 loss: 0.059501362395276376
	batch 150 loss: 0.054510464819613844
	batch 200 loss: 0.0506646228581667
LOSS [train: 0.0506646228581667] [valid: 0.05324150417824664] TIME [epoch: 297 sec]
EPOCH 15:
	batch 50 loss: 0.05484401500783861
	batch 100 loss: 0.04518168744747527
	batch 150 loss: 0.055123592159943655
	batch 200 loss: 0.07871860056126025
LOSS [train: 0.07871860056126025] [valid: 0.05189300728961825] TIME [epoch: 299 sec]
EPOCH 16:
	batch 50 loss: 0.05808878893381916
	batch 100 loss: 0.05491229875362478
	batch 150 loss: 0.06424783894093707
	batch 200 loss: 0.04632008212000074
LOSS [train: 0.04632008212000074] [valid: 0.05086708337573024] TIME [epoch: 299 sec]
Saving model.
EPOCH 17:
	batch 50 loss: 0.06878799002384767
	batch 100 loss: 0.04139700207160786
	batch 150 loss: 0.0534061670882511
	batch 200 loss: 0.054446953560691326
LOSS [train: 0.054446953560691326] [valid: 0.05149951485003233] TIME [epoch: 299 sec]
EPOCH 18:
	batch 50 loss: 0.04915187584236264
	batch 100 loss: 0.05500398004427552
	batch 150 loss: 0.06179637557594106
	batch 200 loss: 0.055063655225094404
LOSS [train: 0.055063655225094404] [valid: 0.050104587643484896] TIME [epoch: 299 sec]
Saving model.
EPOCH 19:
	batch 50 loss: 0.07461750254500657
	batch 100 loss: 0.059320499447640035
	batch 150 loss: 0.04743607741896994
	batch 200 loss: 0.03576639533712296
LOSS [train: 0.03576639533712296] [valid: 0.05403313690330833] TIME [epoch: 299 sec]
EPOCH 20:
	batch 50 loss: 0.06579461854882539
	batch 100 loss: 0.044548819574993105
	batch 150 loss: 0.047785750738112254
	batch 200 loss: 0.055417785272002223
LOSS [train: 0.055417785272002223] [valid: 0.04806891037733294] TIME [epoch: 299 sec]
Saving model.
EPOCH 21:
	batch 50 loss: 0.04924385585822165
	batch 100 loss: 0.04939430654863827
	batch 150 loss: 0.056848910379703736
	batch 200 loss: 0.0534256843329058
LOSS [train: 0.0534256843329058] [valid: 0.04864525726491896] TIME [epoch: 294 sec]
EPOCH 22:
	batch 50 loss: 0.04819848025974352
	batch 100 loss: 0.053114189058542254
	batch 150 loss: 0.04011377951945178
	batch 200 loss: 0.06900603087648051
LOSS [train: 0.06900603087648051] [valid: 0.05014726503904967] TIME [epoch: 294 sec]
EPOCH 23:
	batch 50 loss: 0.04421039991895668
	batch 100 loss: 0.04280577870784327
	batch 150 loss: 0.06550095477927244
	batch 200 loss: 0.05458314874704229
LOSS [train: 0.05458314874704229] [valid: 0.05245240886773293] TIME [epoch: 294 sec]
EPOCH 24:
	batch 50 loss: 0.049510894359555094
	batch 100 loss: 0.05825132074183784
	batch 150 loss: 0.05232446287176572
	batch 200 loss: 0.03888212417601608
LOSS [train: 0.03888212417601608] [valid: 0.04715564019752492] TIME [epoch: 294 sec]
Saving model.
EPOCH 25:
	batch 50 loss: 0.03893824271042831
	batch 100 loss: 0.04803780950722285
	batch 150 loss: 0.061550444366584994
	batch 200 loss: 0.05526906891725957
LOSS [train: 0.05526906891725957] [valid: 0.04740682862175163] TIME [epoch: 294 sec]
EPOCH 26:
	batch 50 loss: 0.04820590770803392
	batch 100 loss: 0.04038444344885647
	batch 150 loss: 0.05805439818650484
	batch 200 loss: 0.05641955621773377
LOSS [train: 0.05641955621773377] [valid: 0.04886290551124451] TIME [epoch: 294 sec]
EPOCH 27:
	batch 50 loss: 0.0545362118788762
	batch 100 loss: 0.055282070185639895
	batch 150 loss: 0.031514305257587694
	batch 200 loss: 0.058103287674020974
LOSS [train: 0.058103287674020974] [valid: 0.050442298154424255] TIME [epoch: 295 sec]
EPOCH 28:
	batch 50 loss: 0.0517854640679434
	batch 100 loss: 0.04906766671338118
	batch 150 loss: 0.05537214195937849
	batch 200 loss: 0.049216741900891064
LOSS [train: 0.049216741900891064] [valid: 0.04987703041406348] TIME [epoch: 294 sec]
EPOCH 29:
	batch 50 loss: 0.05657188427925575
	batch 100 loss: 0.04203722283593379
	batch 150 loss: 0.05820029420239734
	batch 200 loss: 0.043716867964249104
LOSS [train: 0.043716867964249104] [valid: 0.045621317790088746] TIME [epoch: 295 sec]
Saving model.
EPOCH 30:
	batch 50 loss: 0.05643213175004348
	batch 100 loss: 0.050647691742778986
	batch 150 loss: 0.039088443531654776
	batch 200 loss: 0.0571175442187814
LOSS [train: 0.0571175442187814] [valid: 0.04717341420764569] TIME [epoch: 294 sec]
EPOCH 31:
